{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_SVM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMznZZ1k+cYuzrxAmu68del",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Richish/hands_on_ml/blob/master/5_SVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHWeYAGzogAc",
        "colab_type": "text"
      },
      "source": [
        "# SVC for Iris dataset. To classify verginica flowers.(data has 3 varieties of flowers)\n",
        "\n",
        "SVC is trying to fit the largest possible\n",
        "street between two classes while limiting margin violations,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMXK4vHBmXB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import LinearSVC"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kV5yFLLwq4Ng",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "209286a8-9205-4958-bbcb-3ecebb50118a"
      },
      "source": [
        "iris_data = datasets.load_iris()\n",
        "iris_data.keys()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d27dLFdorIgb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "c19bb9e3-2201-4750-8d8b-bc3d0f8994b0"
      },
      "source": [
        "iris_data[\"DESCR\"]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "of3WcxFLsq9l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "4f5236fa-c19e-4c95-89b4-cb9d561579aa"
      },
      "source": [
        "iris_data[\"target\"], iris_data[\"target_names\"]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
              " array(['setosa', 'versicolor', 'virginica'], dtype='<U10'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeEdrgiirt5P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = iris_data[\"data\"][:,(2,3)] # petal length and petal width\n",
        "y = (iris_data[\"target\"] == 2).astype(np.float64)\n",
        "#X, y"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlQmShkJrvK6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "svm_clf = Pipeline([\n",
        "                    (\"scaler\", StandardScaler()),\n",
        "                    (\"linear_svc\", LinearSVC(C=1.0, loss=\"hinge\"))\n",
        "                    ])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrpFDOwcrwOU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "cb71e457-46e8-4b1d-a7c5-518dbb99db64"
      },
      "source": [
        "svm_clf.fit(X=X, y=y)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('scaler',\n",
              "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
              "                ('linear_svc',\n",
              "                 LinearSVC(C=1.0, class_weight=None, dual=True,\n",
              "                           fit_intercept=True, intercept_scaling=1,\n",
              "                           loss='hinge', max_iter=1000, multi_class='ovr',\n",
              "                           penalty='l2', random_state=None, tol=0.0001,\n",
              "                           verbose=0))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFKNFbODrwaU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9eed96dd-a327-4660-bdc2-228770ada2f0"
      },
      "source": [
        "svm_clf.predict(X=[[5.5, 1.7]])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFB7vDsWu1Nh",
        "colab_type": "text"
      },
      "source": [
        "## Options other than LinearSVM:\n",
        "\n",
        "\n",
        "1.   SVM: using same but it is much slower than linearSVM and is not recommended.\n",
        "\n",
        "2.   SGDClassifier: with SGDClassifier(loss=\"hinge\",\n",
        "alpha=1/(m*C)). Applies regular Stochastic Gradient Descent  to train a linear SVM classifier. It does not converge as fast as the LinearSVC class, but it\n",
        "can be useful to handle huge datasets that do not fit in memory (out-of-core training), or to handle online classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUAretnGxG22",
        "colab_type": "text"
      },
      "source": [
        "# Non-linear SVM classification.\n",
        "In case of datasets that are not separable by linear svm.\n",
        "\n",
        "Creat a pipeline where we transform existing features using PolynomialFeatures transformer(Thus creating new features taht are linearly seperable). And then apply our normal standard scaler and LinearSVC.\n",
        "\n",
        "Let’s test this on the moons\n",
        "dataset: this is a toy dataset for binary classification in which the data points are shaped as two interleaving half circles. You can generate this dataset using the make_moons() function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJ6MONXPrwdc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import make_moons\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.svm import LinearSVC"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYw-nwbpyVtD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X, y = make_moons(n_samples=100)\n",
        "#X,y"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LheiCVUwyWCO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "polynomial_svm_clf = Pipeline([\n",
        "                               (\"poly_features\", PolynomialFeatures(degree=3)),\n",
        "                               (\"scaler\", StandardScaler()),\n",
        "                               (\"linear_svm\", LinearSVC(C=10, loss=\"hinge\")) # loss must be set to hinge in linear svcs\n",
        "])"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esxkXBAxyWFV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "817b2582-bfc1-47ff-ac5c-7356668b19db"
      },
      "source": [
        "polynomial_svm_clf.fit(X, y)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('poly_features',\n",
              "                 PolynomialFeatures(degree=3, include_bias=True,\n",
              "                                    interaction_only=False, order='C')),\n",
              "                ('scaler',\n",
              "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
              "                ('linear_svm',\n",
              "                 LinearSVC(C=10, class_weight=None, dual=True,\n",
              "                           fit_intercept=True, intercept_scaling=1,\n",
              "                           loss='hinge', max_iter=1000, multi_class='ovr',\n",
              "                           penalty='l2', random_state=None, tol=0.0001,\n",
              "                           verbose=0))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhhZl4dwyWII",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "029ef7be-ec07-4d0d-f577-c1c29619b27c"
      },
      "source": [
        "polynomial_svm_clf.predict([[1,1]])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rXfMaSA0rTV",
        "colab_type": "text"
      },
      "source": [
        "# Polynomial Kernel\n",
        "\n",
        "Problem with polynomial features:\n",
        "1. At a low polynomial degree it can't deal with very complex datasets. And increasing degree leads to too many features that makes the model very slow.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZDK96zUyWK_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "27ceb56b-c3c5-4ef4-9a12-e8ab86930a5a"
      },
      "source": [
        "# SMC(kernel=\"poly\") makes it applies a technique called kernel trick. Called a trick because actually applying polynomial features, it bahves as if polynomial features have been added. \n",
        "from sklearn.svm import SVC\n",
        "poly_kernel_svm_clf = Pipeline([\n",
        "                                (\"scaler\", StandardScaler()),\n",
        "                                (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5)) # coef0, describes we want to focus on lower order terms or higher order terms\n",
        "])\n",
        "poly_kernel_svm_clf.fit(X, y)\n",
        "poly_kernel_svm_clf.predict([[1,1]])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XakffvO89nY",
        "colab_type": "text"
      },
      "source": [
        "# Adding similarity features\n",
        "\n",
        "to convert a 1d dataset to 2d datset, choose 2 points(p1 and p2) on x1 axis.\n",
        "For any instance(x) the features x2 and x3 for that point will be:\n",
        "x2 = e^(gamma * (x-p1))\n",
        "x3 = e^(gamma * (x-p2))\n",
        "\n",
        "Basically this defines how close a instance is to a these 2 points(called landmarks).Or in other words how smilar are they.\n",
        "\n",
        "Here gamma is gamma of Gaussian Radial Basis Functio( bell curve with range [0,1] )\n",
        "\n",
        "## how to choose landmarks:\n",
        "The simplest approach is to create a\n",
        "landmark at the location of each and every instance in the dataset. This creates many\n",
        "dimensions and thus increases the chances that the transformed training set will be\n",
        "linearly separable. The downside is that a training set with m instances and n features\n",
        "gets transformed into a training set with m instances and m features (assuming you\n",
        "drop the original features). If your training set is very large, you end up with an\n",
        "equally large number of features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-VJdrWu--Dj",
        "colab_type": "text"
      },
      "source": [
        "## Gaussian RBF Kernel\n",
        "\n",
        "Since, it may be computationally expensive to\n",
        "compute all the additional features, especially on large training sets. However, once\n",
        "again the kernel trick does its SVM magic: it makes it possible to obtain a similar\n",
        "result as if you had added many similarity features, without actually having to add\n",
        "them. Let’s try the Gaussian RBF kernel using the SVC class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-gIsc139u_t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4e5d9164-68ec-4c9e-9c91-028a28f7b662"
      },
      "source": [
        "rbf_kernel_svm_clf = Pipeline([\n",
        "                               (\"scaler\", StandardScaler()),\n",
        "                               (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))\n",
        "])\n",
        "rbf_kernel_svm_clf.fit(X, y)\n",
        "rbf_kernel_svm_clf.predict([[0,1]])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdTW0y-L_4Op",
        "colab_type": "text"
      },
      "source": [
        "### hyperparameter tuning:\n",
        "Increasing\n",
        "gamma makes the bell-shape curve narrower, and as a\n",
        "result each instance’s range of influence is smaller: the decision boundary ends up\n",
        "being more irregular, wiggling around individual instances. Conversely, a small gamma\n",
        "value makes the bell-shaped curve wider, so instances have a larger range of influence,\n",
        "and the decision boundary ends up smoother. So γ acts like a regularization\n",
        "hyperparameter: if your model is overfitting, you should reduce it, and if it is underfitting,\n",
        "you should increase it (similar to the C hyperparameter)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tWC5PBrApGP",
        "colab_type": "text"
      },
      "source": [
        "# Other kernels:\n",
        "Used but for very specific use cases:\n",
        "## String kernel: used for text documents or dna sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmnVF7BpA_1p",
        "colab_type": "text"
      },
      "source": [
        "# Which kernel to use:\n",
        "As a rule of thumb, you should always try the linear\n",
        "kernel first (remember that LinearSVC is much faster than SVC(ker\n",
        "nel=\"linear\")), especially if the training set is very large or if it\n",
        "has plenty of features. If the training set is not too large, you should\n",
        "try the Gaussian RBF kernel as well; it works well in most cases.\n",
        "Then if you have spare time and computing power, you can also\n",
        "experiment with a few other kernels using cross-validation and grid\n",
        "search, especially if there are kernels specialized for your training\n",
        "set’s data structure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LG9YvgNuD0HA",
        "colab_type": "text"
      },
      "source": [
        "# Computational complexity:\n",
        "\n",
        "\n",
        "1.   Linear SVC: O(m * n). The algorithm takes longer if you require a very high precision. This is controlled by the tolerance hyperparameter ϵ (called tol in Scikit-Learn). \n",
        "2.   SVC: Between O(m^2 * n) and O(m^3 * n). This\n",
        "algorithm is perfect for complex but small or medium training sets. However, it scales well with the number of features, especially with sparse features (i.e., when each instance has few nonzero features). In this case, the algorithm scales roughly with the average number of nonzero features per instance.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuvIbgkYyWN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuGmMDdn_8D0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}