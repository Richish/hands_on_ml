{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7_1_ensemble_learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNd9Z60VLeGODypKJcwxcis",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Richish/hands_on_ml/blob/master/7_1_ensemble_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYdEpClsg-gx",
        "colab_type": "text"
      },
      "source": [
        "# Hard voting classifier\n",
        "\n",
        "Suppose you have trained a few classifiers, each one achieving about x% accuracy\n",
        "A very simple way to create an even better classifier is to aggregate the predictions of each classifier and predict the class that gets the most votes. This majority-vote classifier is called a hard voting classifier.\n",
        "\n",
        "even if each classifier is a weak learner (meaning\n",
        "it does only slightly better than random guessing), the ensemble can still be a\n",
        "strong learner (achieving high accuracy), provided there are a sufficient number of\n",
        "weak learners and they are sufficiently diverse.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drM0c9lfhNDU",
        "colab_type": "text"
      },
      "source": [
        "## Analogy from game theory:\n",
        "Suppose you have a slightly biased coin that has a 51% chance of coming up heads,\n",
        "and 49% chance of coming up tails. If you toss it 1,000 times, you will generally get\n",
        "more or less 510 heads and 490 tails, and hence a majority of heads. If you do the\n",
        "math, you will find that the probability of obtaining a majority of heads after 1,000\n",
        "tosses is close to 75%. The more you toss the coin, the higher the probability (e.g.,\n",
        "with 10,000 tosses, the probability climbs over 97%). This is due to the law of large\n",
        "numbers: as you keep tossing the coin, the ratio of heads gets closer and closer to the\n",
        "probability of heads (51%)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtM429nThNF9",
        "colab_type": "text"
      },
      "source": [
        "## Example of voting classifier on moons dataset\n",
        "\n",
        "we will use ensemble of svc, random forest and logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrJ0ctbYh_c6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "7da19dee-1ab5-496e-81f3-e35e27ba09df"
      },
      "source": [
        "# data prep\n",
        "from sklearn.datasets import make_moons\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "X, y = make_moons(n_samples=10_000, noise=0.4, random_state=42)\n",
        "X.shape, y.shape\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)\n",
        "X_train, X_test, y_train, y_test"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[-0.56413534,  0.29283681],\n",
              "        [-1.16033479,  0.96512577],\n",
              "        [-0.06598769, -0.15191052],\n",
              "        ...,\n",
              "        [ 0.38876425, -0.78662881],\n",
              "        [ 2.50492832,  0.21133631],\n",
              "        [ 0.35428745,  0.74582457]]), array([[ 0.69945888, -0.8734481 ],\n",
              "        [ 1.7764418 ,  0.13222334],\n",
              "        [-1.14450821,  0.24446319],\n",
              "        ...,\n",
              "        [ 0.66336269,  0.79833307],\n",
              "        [-0.6493245 ,  1.19920859],\n",
              "        [-0.09883144,  0.40961263]]), array([0, 0, 1, ..., 1, 1, 0]), array([1, 1, 0, ..., 0, 0, 0]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0M83O9_6gyH-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "7bd2c126-819f-4a40-e756-3e3ecb49f642"
      },
      "source": [
        "# training each classifier with no hyperparameter tuning\n",
        "lr_clf = LogisticRegression()\n",
        "rf_clf = RandomForestClassifier()\n",
        "svc_clf = SVC()\n",
        "\n",
        "lr_clf.fit(X_train, y_train)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "svc_clf.fit(X_train, y_train)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "# seeing the individual performance for each classifier\n",
        "for clf in (lr_clf, rf_clf, svc_clf):\n",
        "    y_pred = clf.predict(X_test)\n",
        "    acc_score = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
        "    print(\"{}: {}\".format(clf.__class__.__name__, acc_score))\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LogisticRegression: 0.8415\n",
            "RandomForestClassifier: 0.857\n",
            "SVC: 0.874\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_HL0o28nWC9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b21e41f0-419a-4f1f-f024-56cbee5a55db"
      },
      "source": [
        "# checking performance of a voting classifier based on exact same models\n",
        "\n",
        "voting_clf = VotingClassifier(estimators=[('lr', lr_clf), ('rf', rf_clf), ('svc', svc_clf)], voting='hard')\n",
        "voting_clf.fit(X_train, y_train)\n",
        "y_pred = voting_clf.predict(X_test)\n",
        "acc_score = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
        "print(\"voting clf: {}\".format(acc_score)) \n",
        "# in most of the cases will be higher that all of the constituents thogh did not happen in this particular example.\n",
        "# in this particular case though looks like svc is smiply too good for this data pattern\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "voting clf: 0.8695\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQGX7DKphNIt",
        "colab_type": "text"
      },
      "source": [
        "## Soft voting classifieer.\n",
        "Instead of counting votes from each classifier as in \"hard voting classifier\", here we ask each classifier what the predict_proba for that class and classify based on which class that has highest probability(as calculated from predict_prob given by individual classifiers). In essence, more weight is given to high confidence votes.\n",
        "\n",
        "Often, soft voting classifiers perform better than hard voting ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7-A3yqqhO0U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "754a24ef-54e3-4bcf-c15b-98b34f4cf0eb"
      },
      "source": [
        "# checking performance of a soft voting classifier\n",
        "svc_clf_with_prob = SVC(probability=True) # turning on the predict_prob method of SVC which is off by default\n",
        "soft_voting_clf = VotingClassifier(estimators=[('lr', lr_clf), ('rf', rf_clf), ('svc', svc_clf_with_prob)], voting='soft')\n",
        "soft_voting_clf.fit(X_train, y_train)\n",
        "y_pred_soft = soft_voting_clf.predict(X_test)\n",
        "acc_score_soft = accuracy_score(y_true=y_test, y_pred=y_pred_soft)\n",
        "print(\"voting clf: {}\".format(acc_score_soft)) "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "voting clf: 0.871\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWJiTuflhNLX",
        "colab_type": "text"
      },
      "source": [
        "# Bagging and pasting \n",
        "oh.. these scale so well.\n",
        "\n",
        "These are same as hard voting/soft voting classsifier. But the difference is that instead of having different algorithms for each predictor, here each predictor is based on same algorithm(svc or random forest or some other). That is- the type of classifier for all the predictors is same. What is different is that, for each of the predictors a differnt sample of training data is picked up. That is the training data fed to predictor1 is different from training data fed to predictor2 for training.\n",
        "All these predictors are trained in parallel and after they have been trained we make them do predictions and then do final ensemble of results using the same method as before(hard or soft voting).\n",
        "\n",
        "Bagging is short for bootstrap aggregating.\n",
        "\n",
        "## Difference between bagging and pasting:\n",
        "Bagging - sampling performed with replacement.\n",
        "Pasting- sampling does not involve replacement.\n",
        "Both bagging and pasting allow training instances to be sampled several\n",
        "times across multiple predictors, but only bagging allows training instances to be sampled several times for the same predictor.\n",
        "\n",
        "## How to combine the results after individual predictions:\n",
        "Once all predictors are trained, the ensemble can make a prediction for a new\n",
        "instance by simply aggregating the predictions of all predictors. The aggregation function is typically the statistical mode (i.e., the most frequent prediction, just like a hard voting classifier) for classification, or the average for regression. Each individual predictor has a higher bias than if it were trained on the original training set, but aggregation reduces both bias and variance. Generally, the net result is that the ensemble has a similar bias but a lower variance than a single predictor trained on the original training set.\n",
        "\n",
        "## Why is bagging/pasting so scalable:\n",
        "predictors can all be trained in parallel, via different\n",
        "CPU cores or even different servers. Similarly, predictions can be made in parallel.\n",
        "This is one of the reasons why bagging and pasting are such popular methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvW1oN-HTer1",
        "colab_type": "text"
      },
      "source": [
        "## Bagging using sklearn\n",
        "The following code trains an\n",
        "ensemble of 500 Decision Tree classifiers,5 each trained on 100 training instances randomly\n",
        "sampled from the training set with replacement (this is an example of bagging,\n",
        "but if you want to use pasting instead, just set bootstrap=False). The n_jobs parameter\n",
        "tells Scikit-Learn the number of CPU cores to use for training and predictions\n",
        "(–1 tells Scikit-Learn to use all available cores)\n",
        "\n",
        "The BaggingClassifier automatically performs soft voting\n",
        "instead of hard voting if the base classifier can estimate class probabilities\n",
        "(i.e., if it has a predict_proba() method), which is the case\n",
        "with Decision Trees classifiers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfdR3Py0hO29",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "40ff9df3-70e8-4f01-dfaa-97978fdf1af6"
      },
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500, max_samples=100, bootstrap=True, n_jobs=-1)\n",
        "bag_clf.fit(X_train, y_train)\n",
        "y_pred = bag_clf.predict(X_test)\n",
        "\n",
        "acc_score = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
        "acc_score\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.877"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_k5JBlaVLut",
        "colab_type": "text"
      },
      "source": [
        "## Which one to prefer- Bagging or pasting:\n",
        "Bootstrapping introduces a bit more diversity in the subsets that each predictor is\n",
        "trained on, so bagging ends up with a slightly higher bias than pasting, but this also\n",
        "means that predictors end up being less correlated so the ensemble’s variance is\n",
        "reduced. Overall, bagging often results in better models, which explains why it is generally\n",
        "preferred."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rmr96MtoWzFU",
        "colab_type": "text"
      },
      "source": [
        "## Out of bag evaluation\n",
        "\n",
        "In case of bagging classifer since samples are picked at random each time with replacement, almost always there are some samples that are never part of the training samples picked for a predictor in any of the attempts(Infact if sampling size is same as training size-which is default for Bagging classifier- by probility theory, 63% of samples are ever picked for training). \n",
        "So there always are some samples that have not beem seen by the predictor during training. We can treat these samples as validation set and evaluate our model on these samples.(Before we predict/evaluate on test set).\n",
        "This process is called out of bag evaluation.\n",
        "(Evaluation on samples that were left out of training bag)\n",
        "\n",
        "In Scikit-Learn, you can set oob_score=True when creating a BaggingClassifier to\n",
        "request an automatic oob evaluation after training.\n",
        "The resulting evaluation score is available through the oob_score_ variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoPAbJE3hO6H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "87236fbf-ddf3-4c50-e1f0-75bbc8d9e2bc"
      },
      "source": [
        "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500, bootstrap=True, n_jobs=-1, oob_score=True)\n",
        "bag_clf.fit(X_train, y_train)\n",
        "bag_clf.oob_score_ \n",
        "# generally this oob_score is quite a good indicator of what the accuracy on test set will be. Since this is a score on samples that have never been seen by predictor."
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.836125"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxuolXC3hO9A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c8792157-2356-4ef1-d542-66213a9f9685"
      },
      "source": [
        "y_pred = bag_clf.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.851"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b0RIxLnZRsL",
        "colab_type": "text"
      },
      "source": [
        "The oob decision function for each training instance is also available through the\n",
        "oob_decision_function_ variable. In this case (since the base estimator has a pre\n",
        "dict_proba() method) the decision function returns the class probabilities for each\n",
        "training instance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M54V9U_DZO4y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "c203d503-aa25-4118-c945-50fff4bfac3b"
      },
      "source": [
        "bag_clf.oob_decision_function_"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.9076087 , 0.0923913 ],\n",
              "       [1.        , 0.        ],\n",
              "       [0.29120879, 0.70879121],\n",
              "       ...,\n",
              "       [0.        , 1.        ],\n",
              "       [0.        , 1.        ],\n",
              "       [0.6827957 , 0.3172043 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZD-hNOwbUfz",
        "colab_type": "text"
      },
      "source": [
        "## Random Patches and Random Subspaces\n",
        "All about feature sampling. Specially useful if number of features are large, ex: image classification.\n",
        "\n",
        "In sklearn's BaggingClassifier, Feature sampling is controlled by max_features and bootstrap_features (these work\n",
        "the same way as max_samples and bootstrap but for features instead of samples)\n",
        "\n",
        "### Random Patches: \n",
        "Sampling both training instances and features is called the Random\n",
        "Patches method.\n",
        "\n",
        "### Random Subspaces\n",
        "Keeping all training instances (i.e., bootstrap=False and max_samples=1.0) but sampling features (i.e., bootstrap_features=True and/or max_features smaller than 1.0) is called the Random Subspaces method.\n",
        "\n",
        "### Advantage:\n",
        "Sampling features results in even more predictor diversity, trading a bit more bias for a lower variance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUHVXrGvdAo0",
        "colab_type": "text"
      },
      "source": [
        "# Random Forest\n",
        "\n",
        "It is an ensemble of Decision Trees, generally trained via the bagging method (or sometimes pasting), typically with max_samples set to the size of the training set. \n",
        "\n",
        "Instead of building a BaggingClassifier and passing it a DecisionTreeClassifier, you can instead use the RandomForestClassifier class, which is more convenient and optimized for Decision Trees10 (similarly, there is\n",
        "a RandomForestRegressor class for regression tasks)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FU0kdfSbdArf",
        "colab_type": "text"
      },
      "source": [
        "Training a\n",
        "Random Forest classifier with 500 trees (each limited to maximum 16 nodes), using\n",
        "all available CPU cores:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7qH0VajdBL_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "43048117-4a82-4f16-a3b7-fa901527d79a"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
        "rnd_clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_rf = rnd_clf.predict(X_test)\n",
        "accuracy_score(y_true=y_test, y_pred=y_pred_rf)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.872"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3La0Hf_OdAuU",
        "colab_type": "text"
      },
      "source": [
        "## Advantage of random forest over a bagging classifier of decision trees-\n",
        "\n",
        "The Random Forest algorithm introduces extra randomness when growing trees; instead of searching for the very best feature when splitting a node, it searches for the best feature among a random subset of features. This results in a greater tree diversity, which (once again) trades a higher bias for a lower variance, generally yielding an overall better model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUIaLf1wdAxO",
        "colab_type": "text"
      },
      "source": [
        "## Equivalence of random forest classifier with BaggingClassifier of decison trees:\n",
        "\n",
        "The following BaggingClassifier is roughly equivalent to the previous RandomForestClassifier:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_aftMNAdBPD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bag_clf = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16),\n",
        "    n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1\n",
        ")\n",
        "# notice splitter=\"random\" to achive random forest (instead of default - \"best\")\n",
        "# also always max_samples=1.0 in random forest.\n",
        "# and ofcource base_estimator is always DecisionTreeClassifier"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zti3KwSXbvuS",
        "colab_type": "text"
      },
      "source": [
        "With a few exceptions, a RandomForestClassifier has all the hyperparameters of a\n",
        "DecisionTreeClassifier (to control how trees are grown), plus all the hyperparameters\n",
        "of a BaggingClassifier to control the ensemble itself.\n",
        "\n",
        "Exceptions- splitter is absent (forced to \"random\"), presort is absent (forced to\n",
        "False), max_samples is absent (forced to 1.0), and base_estimator is absent (forced to DecisionTreeClassifier with the provided hyperparameters)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTn3YNHxnYEm",
        "colab_type": "text"
      },
      "source": [
        "# Extra Trees (Extremely randomized trees)\n",
        "Extra trees is shortfor extremely randomized trees.\n",
        "\n",
        "In this randaom forest when we are growing a decision tree we introduce more randomness by: \"by using random thresholds for each feature rather than searching for the best possible thresholds as in case of normal decision tress.\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LN1K9K9InYIp",
        "colab_type": "text"
      },
      "source": [
        "## Advantage of extra trees(Extremely randomized trees)\n",
        "More randomnoss results in trading more bias for a lower variance. \n",
        "\n",
        "It also makes Extra-Trees much faster to train than regular Random\n",
        "Forests since finding the best possible threshold for each feature at every node is one of the most time-consuming tasks of growing a tree."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFXOe6x7pr7u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7d4f7b03-be15-40a5-9ed0-696ac81f45f3"
      },
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "extra_tree_clf = ExtraTreesClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
        "extra_tree_clf.fit(X_train, y_train)\n",
        "y_pred_extra_tree_clf = extra_tree_clf.predict(X_test)\n",
        "accuracy_score(y_true=y_test, y_pred=y_pred_extra_tree_clf)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8645"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ixo_YBiYnYLd",
        "colab_type": "text"
      },
      "source": [
        "# Which one to use- random forest or extra trees\n",
        "It is hard to tell in advance whether a RandomForestClassifier\n",
        "will perform better or worse than an ExtraTreesClassifier. Generally,\n",
        "the only way to know is to try both and compare them using\n",
        "cross-validation (and tuning the hyperparameters using grid\n",
        "search)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VafR33NnYOL",
        "colab_type": "text"
      },
      "source": [
        "# Feature Importance\n",
        "Random Forests make it easy to measure the relative importance of each feature. Scikit-Learn measures a feature’s importance by looking at how much the tree nodes that use that feature reduce impurity on average\n",
        "(across all trees in the forest). More precisely, it is a weighted average, where each node’s weight is equal to the number of training samples that are associated with it.\n",
        "\n",
        "Scikit-Learn computes this score automatically for each feature after training, then it\n",
        "scales the results so that the sum of all importances is equal to 1. You can access the\n",
        "result using the feature_importances_ variable.(higher value means more importance)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvcS_rWVdBR7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "c693ffc4-215e-4bd6-b466-36a2e8188383"
      },
      "source": [
        "# example of feature importance in iris dataset\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
        "rnd_clf.fit(X=iris[\"data\"], y=iris[\"target\"])\n",
        "for fname, f_importance in zip(iris.feature_names, rnd_clf.feature_importances_):\n",
        "    print(fname, f_importance)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sepal length (cm) 0.100894363455805\n",
            "sepal width (cm) 0.025151824514962354\n",
            "petal length (cm) 0.42738227771562665\n",
            "petal width (cm) 0.446571534313606\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}