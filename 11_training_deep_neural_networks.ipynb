{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "11_training_deep_neural_networks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO80KqtowJs6MurYZbBaDp5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Richish/hands_on_ml/blob/master/11_training_deep_neural_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZld5fT5Ci-W"
      },
      "source": [
        "# Challenges\n",
        "when traiining a much deeper DNN, perhaps with 10 layers or much more, each containing hundreds of neurons, connected by hundreds of thousands of connections.\n",
        "\n",
        "1. Vanishing gradients problem (or the related exploding gradients problem) that affects deep neural networks and makes lower layers very hard to train. \n",
        "2. You might not have enough training data for such a large network, or it might be too costly to label - Solved by transfer learning.\n",
        "3. Training may be extremely slow - solved by various optimizers.\n",
        "4. A model with millions of parameters would severely risk overfitting the training set, especially if there are not enough training instances, or they are too noisy. - solved by regularization techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMVvrhnCCjKg"
      },
      "source": [
        "# Vanishing/Exploding Gradients Problems\n",
        "\n",
        "During backpropagation: gradients often get smaller and smaller as the algorithm progresses down to the lower layers. As a result, the Gradient Descent update leaves the lower layer connection weights virtually unchanged, and training never converges to a good\n",
        "solution. This is called the vanishing gradients problem. In some cases, the opposite\n",
        "can happen: the gradients can grow bigger and bigger, so many layers get insanely\n",
        "large weight updates and the algorithm diverges. This is the exploding gradients problem,\n",
        "which is mostly encountered in recurrent neural networks. More generally,\n",
        "deep neural networks suffer from unstable gradients; different layers may learn at\n",
        "widely different speeds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8Oy4C1BCjNR"
      },
      "source": [
        "This behavior was one of the reasons why deep neural networks were mostly abandoned for a\n",
        "long time, it is only around 2010 that significant progress was made in understanding\n",
        "it. \n",
        "\n",
        "A paper titled “Understanding the Difficulty of Training Deep Feedforward\n",
        "Neural Networks” by Xavier Glorot and Yoshua Bengio found a few suspects, including\n",
        "the combination of the popular logistic sigmoid activation function and the\n",
        "weight initialization technique that was most popular at the time, namely random initialization\n",
        "using a normal distribution with a mean of 0 and a standard deviation of 1.\n",
        "In short, they showed that with this activation function and this initialization scheme,\n",
        "the variance of the outputs of each layer is much greater than the variance of its\n",
        "inputs. Going forward in the network, the variance keeps increasing after each layer\n",
        "until the activation function saturates at the top layers. This is actually made worse by\n",
        "the fact that the logistic function has a mean of 0.5, not 0 (the hyperbolic tangent\n",
        "function has a mean of 0 and behaves slightly better than the logistic function in deep\n",
        "networks).\n",
        "\n",
        "When\n",
        "inputs become large (negative or positive), the function saturates at 0 or 1, with a\n",
        "derivative extremely close to 0. Thus when backpropagation kicks in, it has virtually\n",
        "no gradient to propagate back through the network, and what little gradient exists\n",
        "keeps getting diluted as backpropagation progresses down through the top layers, so\n",
        "there is really nothing left for the lower layers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVacXQYvCjSd"
      },
      "source": [
        "## Initializers- Glorot, LeCunn and He Initializations\n",
        "\n",
        "### Glorot\n",
        "In their paper, Glorot and Bengio propose a way to significantly alleviate this problem.\n",
        "We need the signal to flow properly in both directions: in the forward direction\n",
        "when making predictions, and in the reverse direction when backpropagating gradients.\n",
        "We don’t want the signal to die out, nor do we want it to explode and saturate.\n",
        "For the signal to flow properly, the authors argue that we need the variance of the\n",
        "outputs of each layer to be equal to the variance of its inputs,2 and we also need the\n",
        "gradients to have equal variance before and after flowing through a layer in the\n",
        "reverse direction. \n",
        "\n",
        "It is actually not possible to guarantee both unless the layer has an equal\n",
        "number of inputs and neurons (these numbers are called the fan-in and fan-out of the\n",
        "layer), but they proposed a good compromise that has proven to work very well in\n",
        "practice: the connection weights of each layer must be initialized randomly as described in Equation below, where fan{avg} = (fan{in} + fan{out})/2. This initialization strategy is called Xavier initialization (after the author’s first name) or Glorot initialization (after his last name).\n",
        "\n",
        "Normal distribution with mean 0 and variance: σ^2 = 1/fan{avg}\n",
        "\n",
        "Or a uniform distribution between −r and + r, with r = root(3/fan{avg})\n",
        "\n",
        "### LeCunn\n",
        "If you just replace fan{avg} with fan{in} in above eqn, you get an initialization strategy\n",
        "that was actually already proposed by Yann LeCun in the 1990s, called LeCun initialization. It is equivalent to Glorot initialization when fan{in} = fan{out}. It took over a decade for researchers to realize\n",
        "just how important this trick really is. Using Glorot initialization can speed up training considerably, and it is one of the tricks that led to the current success of Deep Learning.\n",
        "\n",
        "### He\n",
        "Some papers have provided similar strategies for different activation functions.\n",
        "These strategies differ only by the scale of the variance and whether they use fan{avg} or fan{in}, as shown in Table below - for the uniform distribution, just compute r = root(3.σ^2). \n",
        "\n",
        "The initialization strategy for the ReLU activation function (and its variants, including the ELU activation described shortly) is sometimes called He initialization (after the last name of its author). \n",
        "\n",
        "The SELU activation function will be explained . It should be used with LeCun initialization (preferably with a normal distribution, as we will see).\n",
        "\n",
        "### Table of initializers:\n",
        "| Initialization      | Activation functions | σ^2 (Normal)    |\n",
        "| :---        |    :----   |          :--- |\n",
        "| Glorot      | None, Logistic, tanh, Softmax      | 1/fan{avg}  |\n",
        "| LeCunn   | SELU        | 1/fan{in}      |\n",
        "| He   | RELU        | 2/fan{in}      |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQ7d6CgwCjVI"
      },
      "source": [
        "By default, Keras uses Glorot initialization with a uniform distribution. You can\n",
        "change this to He initialization by setting kernel_initializer=\"he_uniform\" or kernel_initializer=\"he_normal\" when creating a layer, like this:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rM8OkWOS_zF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "37c52392-433c-46ca-dc87-86bf9e609b31"
      },
      "source": [
        "import keras\n",
        "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.layers.core.Dense at 0x7ff6d47a33c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9R0_ZR-TXbN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0492833d-52d9-4f3f-d273-45a2381d5e9d"
      },
      "source": [
        "# If you want He initialization with a uniform distribution, but based on fanavg rather\n",
        "# than fanin, you can use the VarianceScaling initializer like this:\n",
        "he_avg_init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg', distribution='uniform') # basically a custom initializer\n",
        "keras.layers.Dense(10, activation=\"sigmoid\", kernel_initializer=he_avg_init)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.layers.core.Dense at 0x7ff6d5128358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Y4kaf_BXnUN"
      },
      "source": [
        "## Activation Functions- Nonsaturating Activation Functions\n",
        "\n",
        "### Relu\n",
        "One of the insights in the 2010 paper by Glorot and Bengio was that the vanishing/\n",
        "exploding gradients problems were in part due to a poor choice of activation function.\n",
        "Until then most people had assumed that if Mother Nature had chosen to use\n",
        "roughly sigmoid activation functions in biological neurons, they must be an excellent\n",
        "choice. But it turns out that other activation functions behave much better in deep\n",
        "neural networks, in particular the ReLU activation function, mostly because it does\n",
        "not saturate for positive values (and also because it is quite fast to compute).\n",
        "\n",
        "#### Problem of dying relus:\n",
        "During training in relu, some neurons effectively die, meaning\n",
        "they stop outputting anything other than 0. In some cases, you may find that half of your network’s neurons are dead, especially if you used a large learning rate. A neuron dies when its weights get tweaked in such a way that the weighted sum of its inputs are negative for all instances in the training set. When this happens, it just keeps outputting 0s, and gradient descent does not affect it anymore since the gradient\n",
        "of the ReLU function is 0 when its input is negative.\n",
        "\n",
        "### Leaky relu(solves the problem of dying relu):\n",
        "This function is defined as LeakyReLUα(z) = max(αz, z). The hyperparameter α defines how much the function “leaks”: it is the\n",
        "slope of the function for z < 0, and is typically set to 0.01. This small slope ensures that leaky ReLUs never die; they can go into a long coma, but they have a chance to eventually wake up. \n",
        "\n",
        "A 2015 paper compared several variants of the ReLU activation function and one of its conclusions was that the leaky variants always outperformed the strict ReLU activation function. In fact, setting α = 0.2 (huge leak) seemed to result in better performance than α = 0.01 (small leak). \n",
        "\n",
        "They also evaluated the **randomized leaky ReLU (RReLU)***, where α is picked randomly in a given range during training, and it is fixed to an average value during testing. It also performed fairly well and seemed to act as a regularizer (reducing the risk of overfitting the training set).\n",
        "\n",
        "Finally, they also evaluated the **parametric leaky ReLU (PReLU)**, where α is authorized to be learned during training (instead of being a hyperparameter, it becomes a parameter that can be modified by backpropagation like any other parameter). This was reported to strongly outperform ReLU on large image datasets, but on smaller\n",
        "datasets it runs the risk of overfitting the training set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E12NRlMFCjX5"
      },
      "source": [
        "### ELU\n",
        "A 2015 paper by Djork-Arné Clevert et al.6 proposed a new activation\n",
        "function called the exponential linear unit (ELU) that outperformed all the ReLU variants in their experiments: training time was reduced and the neural network performed better on the test set. \n",
        "ELU activation function:\n",
        "ELU{α} (z) = α (exp(z) − 1) if z < 0 else z\n",
        "\n",
        "It looks like relu for +ve values of z.\n",
        "\n",
        " 3 differences from relu:\n",
        "1. It takes on negative values when z < 0, which allows the unit to have an average output closer to 0. This helps alleviate the vanishing gradients problem, as discussed earlier. The hyperparameter α defines the value that the ELU function approaches when z is a large negative number. It is usually set to 1, but you can tweak it like any other hyperparameter if you want.\n",
        "2. It has a nonzero gradient for z < 0, which avoids the dead neurons problem.\n",
        "3. If α is equal to 1 then the function is smooth everywhere, including\n",
        "around z = 0, which helps speed up Gradient Descent, since it does not bounce as much left and right of z = 0.\n",
        "\n",
        "Drawbacks of ELU:\n",
        "The main drawback of the ELU activation function is that it is slower to compute than the ReLU and its variants (due to the use of the exponential function), but during training this is compensated by the faster convergence rate. However, at test time an ELU network will be slower than a ReLU network.\n",
        "\n",
        "#### SELU (Scaled ELU)\n",
        "In a 2017 paper7 by Günter Klambauer et al., called “Self-Normalizing\n",
        "Neural Networks”, the authors showed that if you build a neural network composed exclusively of a stack of dense layers, and if all hidden layers use the SELU activation function (which is just a scaled version of the ELU activation function, as its name\n",
        "suggests), then the network will self-normalize: the output of each layer will tend to preserve mean 0 and standard deviation 1 during training, which solves the vanishing/ exploding gradients problem. As a result, this activation function often outperforms other activation functions very significantly for such neural nets (especially\n",
        "deep ones). **However, there are a few conditions for self-normalization to happen:**\n",
        "\n",
        "1. The input features must be standardized (mean 0 and standard deviation 1).\n",
        "2. Every hidden layer’s weights must also be initialized using LeCun normal initialization. In Keras, this means setting kernel_initializer=\"lecun_normal\".\n",
        "3. **The network’s architecture must be sequential.** Unfortunately, if you try to use SELU in non-sequential architectures, such as recurrent networks or networks with skip connections (i.e., connections that skip layers, such as in wide & deep nets), self-normalization will not be guaranteed, so SELU will not necessarily outperform other activation functions.\n",
        "4. The paper only guarantees self-normalization if all layers are dense. However, in practice the SELU activation function seems to work great with convolutional neural nets as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNBkW6G1ouQ_"
      },
      "source": [
        "### Which optimization function to use:\n",
        "\n",
        "For the hidden layers of your deep neural networks- Although your mileage will vary, in\n",
        "general:\n",
        "\n",
        "SELU > ELU > leaky ReLU (and its variants) > ReLU > tanh\n",
        "> logistic. \n",
        "\n",
        "If the network’s architecture prevents it from self-normalizing,\n",
        "then ELU may perform better than SELU (since SELU\n",
        "is not smooth at z = 0). \n",
        "\n",
        "If you care a lot about runtime latency, then you may prefer leaky ReLU. \n",
        "\n",
        "If you don’t want to tweak yet another hyperparameter, you may just use the default α values used by Keras (e.g., 0.3 for the leaky ReLU). \n",
        "If you have spare time and computing power, you can use cross-validation to evaluate other activation functions, in particular RReLU if your network is overfitting, or PReLU if you have a huge training set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xiv6Wn46pk06"
      },
      "source": [
        "# To use the leaky ReLU activation function, you must create a LeakyReLU instance like this:\n",
        "from tensorflow import keras\n",
        "leaky_relu = keras.layers.LeakyReLU(alpha=0.2)\n",
        "layer = keras.layers.Dense(10, activation=leaky_relu, kernel_initializer=\"he_normal\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNS4Yx8Cpk38"
      },
      "source": [
        "# For PReLU, just replace LeakyRelu(alpha=0.2) with PReLU(). There is currently no\n",
        "# official implementation of RReLU in Keras, but you can fairly easily implement your own.\n",
        "p_relu = keras.layers.PReLU()\n",
        "layer = keras.layers.Dense(10, activation=p_relu, kernel_initializer=\"he_normal\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTwy3PFspk68"
      },
      "source": [
        "# For SELU activation, just set activation=\"selu\" and kernel_initializer=\"lecun_normal\" when creating a layer:\n",
        "layer = keras.layers.Dense(10, activation=\"selu\", kernel_initializer=\"lecun_normal\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlsgdFahpf_b"
      },
      "source": [
        "## Batch Normalization:\n",
        "\n",
        "Although using He initialization along with ELU (or any variant of ReLU) can significantly\n",
        "reduce the vanishing/exploding gradients problems at the beginning of training,\n",
        "it doesn’t guarantee that they won’t come back during training.\n",
        "\n",
        "Batch Normalization consists of adding an operation in the model just before or after the\n",
        "activation function of each hidden layer, simply zero-centering and normalizing each\n",
        "input, then scaling and shifting the result using two new parameter vectors per layer:\n",
        "one for scaling, the other for shifting. This operation lets the model\n",
        "learn the optimal scale and mean of each of the layer’s inputs. \n",
        "\n",
        "In many cases, if you\n",
        "add a BN layer as the very first layer of your neural network, you do not need to\n",
        "standardize your training set (e.g., using a StandardScaler): the BN layer will do it\n",
        "for you (well, approximately, since it only looks at one batch at a time, and it can also\n",
        "rescale and shift each input feature).\n",
        "\n",
        "In order to zero-center and normalize the inputs, the algorithm needs to estimate\n",
        "each input’s mean and standard deviation. It does so by evaluating the mean and standard\n",
        "deviation of each input over the current mini-batch (hence the name “Batch\n",
        "Normalization”).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiYIP93Wvms0"
      },
      "source": [
        "So during training, BN just standardizes its inputs then rescales and offsets them.\n",
        "What about at test time?\n",
        "\n",
        "It is often preferred to estimate these final statistics\n",
        "during training using a moving average of the layer’s input means and standard\n",
        "deviations. To sum up, four parameter vectors are learned in each batch-normalized\n",
        "layer: γ (the ouput scale vector) and β (the output offset vector) are learned through\n",
        "regular backpropagation, and μ (the final input mean vector), and σ (the final input\n",
        "standard deviation vector) are estimated using an exponential moving average. Note\n",
        "that μ and σ are estimated during training, but they are not used at all during training,\n",
        "only after training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C332W2YWvmyn"
      },
      "source": [
        "Batch Normalization\n",
        "also acts like a regularizer, reducing the need for other regularization techniques\n",
        "(such as dropout, described later in this chapter).\n",
        "Batch Normalization does, however, add some complexity to the model (although it\n",
        "can remove the need for normalizing the input data, as we discussed earlier). Moreover,\n",
        "there is a runtime penalty: the neural network makes slower predictions due to\n",
        "the extra computations required at each layer. So if you need predictions to be\n",
        "lightning-fast, you may want to check how well plain ELU + He initialization perform\n",
        "before playing with Batch Normalization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6HmG3k5vm9F"
      },
      "source": [
        "You may find that training is rather slow, because each epoch takes\n",
        "much more time when you use batch normalization. However, this\n",
        "is usually counterbalanced by the fact that convergence is much\n",
        "faster with BN, so it will take fewer epochs to reach the same performance.\n",
        "All in all, wall time will usually be smaller (this is the\n",
        "time measured by the clock on your wall)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIyh93WXvnC5"
      },
      "source": [
        "### Implementing Batch Normalization with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hca-FrZcvnI5"
      },
      "source": [
        "Just add a BatchNormalization layer before or after each hidden layer’s activation\n",
        "function, and optionally add a BN layer as well as the first layer in your model. For\n",
        "example, this model applies BN after every hidden layer and as the first layer in the\n",
        "model (after flattening the input images):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zk5rMNcawjAm"
      },
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxFY-LR-wy5G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eeb8e426-4697-40fe-91b3-13a8a75702e6"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 784)               3136      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 300)               235500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 300)               1200      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100)               30100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 271,346\n",
            "Trainable params: 268,978\n",
            "Non-trainable params: 2,368\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpKPcjwKwyIn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c5035d8-aa1b-4e4d-86c3-035bf43a09c0"
      },
      "source": [
        "[(var.name, var.trainable) for var in model.layers[1].variables]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('batch_normalization/gamma:0', True),\n",
              " ('batch_normalization/beta:0', True),\n",
              " ('batch_normalization/moving_mean:0', False),\n",
              " ('batch_normalization/moving_variance:0', False)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jOOXoFvnFv"
      },
      "source": [
        "The authors of the BN paper argued in favor of adding the BN layers before the activation\n",
        "functions, rather than after (as we just did). There is some debate about this, as\n",
        "it seems to depend on the task. So that’s one more thing you can experiment with to\n",
        "see which option works best on your dataset. To add the BN layers before the activation\n",
        "functions, we must remove the activation function from the hidden layers, and\n",
        "add them as separate layers after the BN layers. Moreover, since a Batch Normalization\n",
        "layer includes one offset parameter per input, you can remove the bias term from\n",
        "the previous layer (just pass use_bias=False when creating it):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDixSTNwx6Yv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8a6053a-2283-43a4-966f-a5973d534896"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(300, kernel_initializer=\"he_normal\", use_bias=False),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Activation(\"elu\"),\n",
        "    keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
        "    keras.layers.Activation(\"elu\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_2 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 784)               3136      \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 300)               235200    \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 300)               1200      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 100)               30000     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 270,946\n",
            "Trainable params: 268,578\n",
            "Non-trainable params: 2,368\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJs51wqzvnAK"
      },
      "source": [
        "#### Hyperparameters of Batch Normalization layer:\n",
        "\n",
        "1. Momentum:  This hyperparameter is used when updating the exponential moving averages: given a\n",
        "new value v.\n",
        "Running average, V{avg} is calculated using eqn:\n",
        "V{avg} = V{avg}*momentum + v{new_batch}*(1-momentum)\n",
        "\n",
        "A good momentum value is typically close to 1—for example, 0.9, 0.99, or 0.999 (you\n",
        "want more 9s for larger datasets and smaller mini-batches).\n",
        "\n",
        "2. Axis: it determines which axis should be normalized.\n",
        "It defaults to –1, meaning that by default it will normalize the last axis (using\n",
        "the means and standard deviations computed across the other axes). For example,\n",
        "when the input batch is 2D (i.e., the batch shape is [batch size, features]), this means\n",
        "that each input feature will be normalized based on the mean and standard deviation\n",
        "computed across all the instances in the batch. For example, the first BN layer in the\n",
        "previous code example will independently normalize (and rescale and shift) each of\n",
        "the 784 input features. However, if we move the first BN layer before the Flatten\n",
        "layer, then the input batches will be 3D, with shape [batch size, height, width], therefore\n",
        "the BN layer will compute 28 means and 28 standard deviations.\n",
        "\n",
        "\n",
        "it will normalize all pixels in a given column using the same mean and standard deviation.\n",
        "There will also be just 28 scale parameters and 28 shift parameters. If instead\n",
        "you still want to treat each of the 784 pixels independently, then you should set\n",
        "axis=[1, 2]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1i7v4jxvm5y"
      },
      "source": [
        "Notice that the BN layer does not perform the same computation during training and\n",
        "after training: it uses batch statistics during training, and the “final” statistics after\n",
        "training (i.e., the final value of the moving averages)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVGd1wwVvmvm"
      },
      "source": [
        "Batch Normalization has become one of the most used layers in deep neural networks,\n",
        "to the point that it is often omitted in the diagrams, as it is assumed that BN is\n",
        "added after every layer. \n",
        "\n",
        "However, a very recent paper10 by Hongyi Zhang et al. may\n",
        "well change this: the authors show that by using a novel fixed-update (fixup) weight\n",
        "initialization technique, they manage to train a very deep neural network (10,000 layers!)\n",
        "without BN, achieving state-of-the-art performance on complex image classification\n",
        "tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2_2csjD3E8S"
      },
      "source": [
        "## Gradient Clipping\n",
        "\n",
        "Another popular technique to lessen the exploding gradients problem is to simply\n",
        "clip the gradients during backpropagation so that they never exceed some threshold.\n",
        "This is called Gradient Clipping.\n",
        "\n",
        "This technique is most often used in recurrent neural networks, as Batch Normalization is tricky to use in RNNs.\n",
        "For other types of networks, BN is usually sufficient.\n",
        "\n",
        "In Keras, implementing Gradient Clipping is just a matter of setting the clipvalue or\n",
        "clipnorm argument when creating an optimizer. For example:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysviAWPi4I93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "846ed05a-0a10-4575-8ab8-962abdd2bf03"
      },
      "source": [
        "optimizer = keras.optimizers.SGD(clipvalue=1.0)\n",
        "model.compile(loss=\"mse\", optimizer=optimizer)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_2 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 784)               3136      \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 300)               235200    \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 300)               1200      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 100)               30000     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 270,946\n",
            "Trainable params: 268,578\n",
            "Non-trainable params: 2,368\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RCd2pDe3FDT"
      },
      "source": [
        "This will clip every component of the gradient vector to a value between –1.0 and 1.0.\n",
        "This means that all the partial derivatives of the loss (with regards to each and every\n",
        "trainable parameter) will be clipped between –1.0 and 1.0. The threshold is a hyperparameter\n",
        "you can tune. Note that it may change the orientation of the gradient vector:\n",
        "for example, if the original gradient vector is [0.9, 100.0], it points mostly in the\n",
        "direction of the second axis, but once you clip it by value, you get [0.9, 1.0], which\n",
        "points roughly in the diagonal between the two axes. In practice however, this\n",
        "approach works well. \n",
        "\n",
        "If you want to ensure that Gradient Clipping does not change\n",
        "the direction of the gradient vector, you should clip by norm by setting clipnorm\n",
        "instead of clipvalue. This will clip the whole gradient if its ℓ2 norm is greater than\n",
        "the threshold you picked. For example, if you set clipnorm=1.0, then the vector [0.9,\n",
        "100.0] will be clipped to [0.00899964, 0.9999595], preserving its orientation, but\n",
        "almost eliminating the first component. If you observe that the gradients explode\n",
        "during training (you can track the size of the gradients using TensorBoard), you may\n",
        "want to try both clipping by value and clipping by norm, with different threshold,\n",
        "and see which option performs best on the validation set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYvxV4273FMw"
      },
      "source": [
        "# Transfer Learning\n",
        "\n",
        "## Reusing pretrained layers\n",
        "\n",
        "It is generally not a good idea to train a very large DNN from scratch: instead, you\n",
        "should always try to find an existing neural network that accomplishes a similar task\n",
        "to the one you are trying to tackle, then just reuse the lower layers of this network: this is called transfer learning. It will\n",
        "not only **speed up training considerably, but will also require much less training data.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyBd9My23FS7"
      },
      "source": [
        "For example, suppose that you have access to a DNN that was trained to classify pictures\n",
        "into 100 different categories, including animals, plants, vehicles, and everyday\n",
        "objects. You now want to train a DNN to classify specific types of vehicles. These\n",
        "tasks are very similar, even partly overlapping, so you should try to reuse parts of the\n",
        "first network.\n",
        "\n",
        "You\n",
        "want to find the right number of layers to reuse.\n",
        "The more similar the tasks are, the more layers you want to reuse\n",
        "(starting with the lower layers). For very similar tasks, you can try\n",
        "keeping all the hidden layers and just replace the output layer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZ5dwmQ53FZJ"
      },
      "source": [
        "Try freezing all the reused layers first (i.e., make their weights non-trainable, so gradient\n",
        "descent won’t modify them), then train your model and see how it performs.\n",
        "Then try unfreezing one or two of the top hidden layers to let backpropagation tweak\n",
        "them and see if performance improves. The more training data you have, the more layers you can unfreeze. It is also useful to reduce the learning rate when you unfreeze\n",
        "reused layers: this will avoid wrecking their fine-tuned weights.\n",
        "If you still cannot get good performance, and you have little training data, try dropping\n",
        "the top hidden layer(s) and freeze all remaining hidden layers again. You can\n",
        "iterate until you find the right number of layers to reuse. If you have plenty of training\n",
        "data, you may try replacing the top hidden layers instead of dropping them, and\n",
        "even add more hidden layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tsam77x3Ffi"
      },
      "source": [
        "## Transfer Learning With Keras\n",
        "\n",
        "Let’s look at an example. Suppose the fashion MNIST dataset only contained 8 classes,\n",
        "for example all classes except for sandals and shirts. Someone built and trained a\n",
        "Keras model on that set and got reasonably good performance (>90% accuracy). Let’s\n",
        "call this model A. You now want to tackle a different task: you have images of sandals\n",
        "and shirts, and you want to train a binary classifier (positive=shirts, negative=sandals).\n",
        "However, your dataset is quite small, you only have 200 labeled images. When\n",
        "you train a new model for this task (let’s call it model B), with the same architecture\n",
        "as model A, it performs reasonably well (97.2% accuracy), but since it’s a much easier\n",
        "task (there are just 2 classes), you were hoping for more. You realize that your task is quite similar to task A, so perhaps transfer\n",
        "learning can help? \n",
        "\n",
        "Let’s find out!\n",
        "First, you need to load model A, and create a new model based on the model A’s layers.\n",
        "Let’s reuse all layers except for the output layer:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKMv0A9NBI1p"
      },
      "source": [
        "model_A = keras.models.load_model(\"my_model_A.h5\")\n",
        "\n",
        "model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
        "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2bDBgKrBdV6"
      },
      "source": [
        "model_A and model_B_on_A now share some layers. When you train\n",
        "model_B_on_A, it will also affect model_A. If you want to avoid that, you need to clone\n",
        "model_A before you reuse its layers. To do this, you must clone model A’s architecture,\n",
        "then copy its weights (since clone_model() does not clone the weights):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bQ6WYNtBcP9"
      },
      "source": [
        "model_A_clone = keras.models.clone_model(model_A)\n",
        "model_A_clone.set_weights(model_A.get_weights())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsmhlswI3Fi8"
      },
      "source": [
        "Now we could just train model_B_on_A for task B, but since the new output layer was\n",
        "initialized randomly, it will make large errors, at least during the first few epochs, so\n",
        "there will be large error gradients that may wreck the reused weights. To avoid this,\n",
        "one approach is to freeze the reused layers during the first few epochs, giving the new\n",
        "layer some time to learn reasonable weights. To do this, simply set every layer’s train\n",
        "able attribute to False and compile the model:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJ5Dr3wfBwWb"
      },
      "source": [
        "for layer in model_B_on_A.layers[:-1]:\n",
        "    layer.trainable = False\n",
        "\n",
        "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\",metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHwtkLJn3FcY"
      },
      "source": [
        "**You must always compile your model after you freeze or unfreeze\n",
        "layers.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puOmWafZ3FV7"
      },
      "source": [
        "Next, we can train the model for a few epochs, then unfreeze the reused layers (which\n",
        "requires compiling the model again) and continue training to fine-tune the reused\n",
        "layers for task B. After unfreezing the reused layers, it is usually a good idea to reduce\n",
        "the learning rate, once again to avoid damaging the reused weights:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYMEA9z3CEBj"
      },
      "source": [
        "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4, validation_data=(X_valid_B, y_valid_B))\n",
        "\n",
        "for layer in model_B_on_A.layers[:-1]:\n",
        "    layer.trainable = True\n",
        "optimizer = keras.optimizers.SGD(lr=1e-4) # the default lr is 1e-3\n",
        "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16, validation_data=(X_valid_B, y_valid_B))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewsGHbCK3FP6"
      },
      "source": [
        "**It turns out that transfer learning does not work very well\n",
        "with small dense networks: it works best with deep convolutional neural networks**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4he4GH33FJ2"
      },
      "source": [
        "## Unsupervised Pretraining\n",
        "\n",
        "**good option\n",
        "when you have a complex task to solve, no similar model you can reuse, and little\n",
        "labeled training data but plenty of unlabeled training data.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ucz0ftV3E_0"
      },
      "source": [
        "Suppose you want to tackle a complex task for which you don’t have much labeled\n",
        "training data, but unfortunately you cannot find a model trained on a similar task.\n",
        "Don’t lose all hope! First, you should of course try to gather more labeled training\n",
        "data, but if this is too hard or too expensive, you may still be able to perform unsupervised\n",
        "pretraining. It is often rather cheap to gather unlabeled training\n",
        "examples, but quite expensive to label them. If you can gather plenty of unlabeled\n",
        "training data, you can try to train the layers one by one, starting with the lowest layer\n",
        "and then going up, using an unsupervised feature detector algorithm such as Restricted\n",
        "Boltzmann Machines or autoencoders. Each layer is\n",
        "trained on the output of the previously trained layers (all layers except the one being\n",
        "trained are frozen). Once all layers have been trained this way, you can add the output\n",
        "layer for your task, and fine-tune the final network using supervised learning (i.e.,\n",
        "with the labeled training examples). At this point, you can unfreeze all the pretrained\n",
        "layers, or just some of the upper ones.\n",
        "\n",
        "This is a rather long and tedious process, but it often works well; in fact, it is this\n",
        "technique that Geoffrey Hinton and his team used in 2006 and which led to the\n",
        "revival of neural networks and the success of Deep Learning. Until 2010, unsupervised\n",
        "pretraining (typically using RBMs) was the norm for deep nets, and it was only\n",
        "after the vanishing gradients problem was alleviated that it became much more com‐mon to train DNNs purely using supervised learning. However, unsupervised pretraining\n",
        "(today typically using autoencoders rather than RBMs) is still a good option\n",
        "when you have a complex task to solve, no similar model you can reuse, and little\n",
        "labeled training data but plenty of unlabeled training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flcNh7rd3EyS"
      },
      "source": [
        "## Pretraining on an Auxiliary Task\n",
        "\n",
        "If you do not have much labeled training data, one last option is to train a first neural\n",
        "network on an auxiliary task for which you can easily obtain or generate labeled\n",
        "training data, then reuse the lower layers of that network for your actual task. The\n",
        "first neural network’s lower layers will learn feature detectors that will likely be reusable\n",
        "by the second neural network.\n",
        "\n",
        "\n",
        "For example, if you want to build a system to recognize faces, you may only have a\n",
        "few pictures of each individual—clearly not enough to train a good classifier. Gathering\n",
        "hundreds of pictures of each person would not be practical. However, you could\n",
        "gather a lot of pictures of random people on the web and train a first neural network\n",
        "to detect whether or not two different pictures feature the same person. Such a network\n",
        "would learn good feature detectors for faces, so reusing its lower layers would\n",
        "allow you to train a good face classifier using little training data.\n",
        "\n",
        "\n",
        "For natural language processing (NLP) applications, you can easily download millions\n",
        "of text documents and automatically generate labeled data from it. For example, you\n",
        "could randomly mask out some words and train a model to predict what the missing\n",
        "words are (e.g., it should predict that the missing word in the sentence “What ___\n",
        "you saying?” is probably “are” or “were”). If you can train a model to reach good performance\n",
        "on this task, then it will already know quite a lot about language, and you\n",
        "can certainly reuse it for your actual task, and fine-tune it on your labeled data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDjb2-eaXEvd"
      },
      "source": [
        "## Self-supervised learning \n",
        "Is when you automatically generate the\n",
        "labels from the data itself, then you train a model on the resulting\n",
        "“labeled” dataset using supervised learning techniques. Since this\n",
        "approach requires no human labeling whatsoever, it is best classified\n",
        "as a form of unsupervised learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNFcb09NXE2J"
      },
      "source": [
        "# Faster Optimizers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIwlwkeLXFEz"
      },
      "source": [
        "Another huge speed boost comes from\n",
        "using a faster optimizer than the regular Gradient Descent optimizer.\n",
        "\n",
        "The most popular optimizers are: \n",
        "1. Momentum optimization\n",
        "2. Nesterov Accelerated Gradient\n",
        "3. AdaGrad\n",
        "4. RMSProp\n",
        "5. Adam and Nadam optimization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9SDvlQPXFUZ"
      },
      "source": [
        "## Momentum Optimization\n",
        "Imagine a bowling ball rolling down a gentle slope on a smooth surface: it will start\n",
        "out slowly, but it will quickly pick up momentum until it eventually reaches terminal\n",
        "velocity (if there is some friction or air resistance). This is the very simple idea behind\n",
        "Momentum optimization. In contrast, regular\n",
        "Gradient Descent will simply take small regular steps down the slope, so it will take\n",
        "much more time to reach the bottom.\n",
        "\n",
        "Simple Gradient Descent simply updates the weights θ by directly subtracting the\n",
        "gradient of the cost function J(θ) with regards to the weights (∇θJ(θ)) multiplied by\n",
        "the learning rate η. The equation is: θ ← θ – η∇θJ(θ). It does not care about what the\n",
        "earlier gradients were. If the local gradient is tiny, it goes very slowly.\n",
        "Momentum optimization cares a great deal about what previous gradients were: at\n",
        "each iteration, it subtracts the local gradient from the momentum vector m (multiplied\n",
        "by the learning rate η), and it updates the weights by simply adding this\n",
        "momentum vector. In other words, the gradient is used for acceleration,\n",
        "not for speed. To simulate some sort of friction mechanism and prevent the\n",
        "momentum from growing too large, the algorithm introduces a new hyperparameter\n",
        "β, simply called the momentum, which must be set between 0 (high friction) and 1\n",
        "(no friction). A typical momentum value is 0.9.\n",
        "\n",
        "Equation for Momentum algorithm\n",
        "1. m-> βm − η∇{θ}J(θ)\n",
        "2. θ-> θ + m\n",
        "\n",
        "You can easily verify that if the gradient remains constant, the terminal velocity (i.e.,\n",
        "the maximum size of the weight updates) is equal to that gradient multiplied by the\n",
        "learning rate η multiplied by 1/(1 − β) (ignoring the sign). For example, if β = 0.9, then the\n",
        "terminal velocity is equal to 10 times the gradient times the learning rate, so Momentum\n",
        "optimization ends up going 10 times faster than Gradient Descent! This allows\n",
        "Momentum optimization to escape from plateaus much faster than Gradient Descent.\n",
        "\n",
        "In particular, since when the inputs have very different scales the\n",
        "cost function will look like an elongated bowl. Gradient Descent goes\n",
        "down the steep slope quite fast, but then it takes a very long time to go down the valley. In contrast, Momentum optimization will roll down the valley faster and faster\n",
        "until it reaches the bottom (the optimum). In deep neural networks that don’t use\n",
        "Batch Normalization, the upper layers will often end up having inputs with very different\n",
        "scales, so using Momentum optimization helps a lot. It can also help roll past\n",
        "local optima.\n",
        "\n",
        "\n",
        "Due to the momentum, the optimizer may overshoot a bit, then\n",
        "come back, overshoot again, and oscillate like this many times\n",
        "before stabilizing at the minimum. This is one of the reasons why it\n",
        "is good to have a bit of friction in the system: it gets rid of these\n",
        "oscillations and thus speeds up convergence.\n",
        "\n",
        "Implementing Momentum optimization in Keras: just use the SGD\n",
        "optimizer and set its momentum hyperparameter, then lie back and profit!\n",
        "\n",
        "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)\n",
        "The one drawback of Momentum optimization is that it adds yet another hyperparameter\n",
        "to tune. However, the momentum value of 0.9 usually works well in practice\n",
        "and almost always goes faster than regular Gradient Descent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uqT90Ju-Ch1"
      },
      "source": [
        "import keras\n",
        "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLfsOWGgXFLq"
      },
      "source": [
        "## Nesterov Accelerated Gradient\n",
        "One small variant to Momentum optimization, proposed by Yurii Nesterov in 1983\n",
        "is almost always faster than vanilla Momentum optimization. The idea of Nesterov\n",
        "Momentum optimization, or Nesterov Accelerated Gradient (NAG), is to measure the\n",
        "gradient of the cost function not at the local position but slightly ahead in the direction\n",
        "of the momentum. The only difference from vanilla\n",
        "Momentum optimization is that the gradient is measured at θ + βm rather than at θ.\n",
        "\n",
        "Nesterov Accelerated Gradient algorithm\n",
        "1. m-> βm − η∇{θ}J(θ + βm)\n",
        "2. θ-> θ + m\n",
        "\n",
        "This small tweak works because in general the momentum vector will be pointing in\n",
        "the right direction (i.e., toward the optimum), so it will be slightly more accurate to\n",
        "use the gradient measured a bit farther in that direction rather than using the gradient\n",
        "at the original position. As you can see, the Nesterov update ends up\n",
        "slightly closer to the optimum. After a while, these small improvements add up and\n",
        "NAG ends up being significantly faster than regular Momentum optimization. \n",
        "\n",
        "Moreover,\n",
        "note that when the momentum pushes the weights across a valley, ∇1 continues\n",
        "to push further across the valley, while ∇2 pushes back toward the bottom of the valley.\n",
        "This helps reduce oscillations and thus converges faster.\n",
        "NAG will almost always speed up training compared to regular Momentum optimization.\n",
        "To use it, simply set nesterov=True when creating the SGD optimizer:\n",
        "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)\n",
        "\n",
        "My own analogy-> Think of car with headlights knows when to accelerate/deccelerate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PmOJtGVAK0e"
      },
      "source": [
        "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTHmiXmtXE5A"
      },
      "source": [
        "## AdaGrad\n",
        "Consider the elongated bowl problem again: Gradient Descent starts by quickly going\n",
        "down the steepest slope, then slowly goes down the bottom of the valley. It would be\n",
        "nice if the algorithm could detect this early on and correct its direction to point a bit\n",
        "more toward the global optimum.\n",
        "The AdaGrad algorithm1 achieves this by scaling down the gradient vector along the\n",
        "steepest dimensions.\n",
        "\n",
        "1. s-> s + ∇{θ}J(θ) ⊗ ∇{θ}J(θ)\n",
        "2. θ-> θ − η ∇{θ}J(θ) ⊘(s+é)^0.5\n",
        "\n",
        "The first step accumulates the square of the gradients into the vector s (recall that the\n",
        "⊗ symbol represents the element-wise multiplication). This vectorized form is equivalent\n",
        "to computing si ← si + (∂ J(θ) / ∂ θi)2 for each element si of the vector s; in other\n",
        "words, each si accumulates the squares of the partial derivative of the cost function\n",
        "with regards to parameter θi. If the cost function is steep along the ith dimension, then\n",
        "si will get larger and larger at each iteration.\n",
        "\n",
        "The second step is almost identical to Gradient Descent, but with one big difference:\n",
        "the gradient vector is scaled down by a factor of sqrt(s+è). (the ⊘ symbol represents the\n",
        "element-wise division, and ϵ is a smoothing term to avoid division by zero, typically\n",
        "set to 10^–10). This vectorized form is equivalent to computing\n",
        "θi->θi − η ∂J(θ)/ ∂(θi)/sqrt(s+è) for all parameters θi (simultaneously).\n",
        "\n",
        "In short, this algorithm decays the learning rate, but it does so faster for steep dimensions\n",
        "than for dimensions with gentler slopes. This is called an adaptive learning rate.\n",
        "It helps point the resulting updates more directly toward the global optimum (see\n",
        "Figure 11-7). One additional benefit is that it requires much less tuning of the learning\n",
        "rate hyperparameter η.\n",
        "\n",
        "AdaGrad often performs well for simple quadratic problems, but unfortunately it\n",
        "often stops too early when training neural networks. The learning rate gets scaled\n",
        "down so much that the algorithm ends up stopping entirely before reaching the\n",
        "global optimum. So even though Keras has an Adagrad optimizer, **you should not use\n",
        "it to train deep neural networks** (it may be efficient for simpler tasks such as Linear\n",
        "Regression, though). However, understanding Adagrad is helpful to grasp the other\n",
        "adaptive learning rate optimizers.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuaA5X4aXEy5"
      },
      "source": [
        "## RMSProp\n",
        "Although AdaGrad slows down a bit too fast and ends up never converging to the\n",
        "global optimum, the RMSProp algorithm fixes this by accumulating only the gradients\n",
        "from the most recent iterations (as opposed to all the gradients since the beginning\n",
        "of training). \n",
        "\n",
        "It does so by using exponential decay in the first step:\n",
        "RMSProp algorithm:\n",
        "1. s-> βs + (1 − β) ∇{θ}J(θ) ⊗ ∇{θ}J(θ)\n",
        "2. θ-> θ − η ∇{θ}J(θ) ⊘/sqrt(s+è)\n",
        "\n",
        "\n",
        "The decay rate β is typically set to 0.9. Yes, it is once again a new hyperparameter, but\n",
        "this default value often works well, so you may not need to tune it at all.\n",
        "\n",
        "Keras has an RMSProp optimizer:\n",
        "optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)\n",
        "\n",
        "Except on very simple problems, this optimizer almost always performs much better\n",
        "than AdaGrad. In fact, **it was the preferred optimization algorithm of many researchers\n",
        "until Adam optimization came around.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Je9x0sUqGkat"
      },
      "source": [
        "optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ozEsp0MXElh"
      },
      "source": [
        "## Adam and Nadam Optimization\n",
        "Adam which stands for adaptive moment estimation, combines the ideas of Momentum\n",
        "optimization and RMSProp: just like Momentum optimization it keeps track of\n",
        "an exponentially decaying average of past gradients, and just like RMSProp it keeps\n",
        "track of an exponentially decaying average of past squared gradients.\n",
        "\n",
        "1. m-> β{1}m − (1 − β1). ∇{θ}J(θ)\n",
        "2. s-> β{2}s + (1 − β2) ∇{θ}J(θ) ⊗ ∇{θ}J(θ)\n",
        "3. m-> m/(1 − β{1}^t)\n",
        "4. s-> s(1 − β{2}^t)\n",
        "5. θ-> θ + ηm ⊘ sqrt(s+è)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-F3DLC_fj8i"
      },
      "source": [
        "If you just look at steps 1, 2, and 5, you will notice Adam’s close similarity to both\n",
        "Momentum optimization and RMSProp. The only difference is that step 1 computes\n",
        "an exponentially decaying average rather than an exponentially decaying sum, but\n",
        "these are actually equivalent except for a constant factor (the decaying average is just\n",
        "1 – β1 times the decaying sum). \n",
        "\n",
        "Steps 3 and 4 are somewhat of a technical detail: since\n",
        "m and s are initialized at 0, they will be biased toward 0 at the beginning of training,\n",
        "so these two steps will help boost m and s at the beginning of training.\n",
        "The momentum decay hyperparameter β1 is typically initialized to 0.9, while the scaling\n",
        "decay hyperparameter β2 is often initialized to 0.999. As earlier, the smoothing\n",
        "term ϵ is usually initialized to a tiny number such as 10–7. These are the default values\n",
        "for the Adam class (to be precise, epsilon defaults to None, which tells Keras to use\n",
        "keras.backend.epsilon(), which defaults to 10–7; you can change it using\n",
        "keras.backend.set_epsilon()).\n",
        "optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
        "Since Adam is an adaptive learning rate algorithm (like AdaGrad and RMSProp), it\n",
        "requires less tuning of the learning rate hyperparameter η. You can often use the\n",
        "default value η = 0.001, making Adam even easier to use than Gradient Descent.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Amd46IMyfj5j"
      },
      "source": [
        "## Adamax\n",
        "**Not very useful in general** introduced in the same paper as Adam: notice that in step 2 of Equation for adamm, Adam accumulates the squares of the gradients in s (with a greater weight\n",
        "for more recent weights). In step 5, if we ignore ϵ and steps 3 and 4 (which are\n",
        "technical details anyway), Adam just scales down the parameter updates by the\n",
        "square root of s. In short, Adam scales down the parameter updates by the ℓ2\n",
        "norm of the time-decayed gradients (recall that the ℓ2 norm is the square root of\n",
        "the sum of squares). Adamax just replaces the ℓ2 norm with the ℓ∞ norm (a fancy\n",
        "way of saying the max).\n",
        "\n",
        "In\n",
        "practice, this can make Adamax more stable than Adam, but this really depends\n",
        "on the dataset, and in general Adam actually performs better. So it’s just one\n",
        "more optimizer you can try if you experience problems with Adam on some task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPVGY5Jafj3g"
      },
      "source": [
        "## Nadam optimization \n",
        "**is more important it is simply Adam optimization plus\n",
        "the Nesterov trick, so it will often converge slightly faster than Adam** In his\n",
        "report, Timothy Dozat compares many different optimizers on various tasks, and\n",
        "finds that Nadam generally outperforms Adam, but is sometimes outperformed\n",
        "by RMSProp.\n",
        "\n",
        "Adaptive optimization methods (including RMSProp, Adam and\n",
        "Nadam optimization) are often great, converging fast to a good solution.\n",
        "However, a 2017 paper19 by Ashia C. Wilson et al. showed\n",
        "that they can lead to solutions that generalize poorly on some datasets.\n",
        "So when you are disappointed by your model’s performance,\n",
        "try using plain Nesterov Accelerated Gradient instead: your dataset\n",
        "may just be allergic to adaptive gradients. Also check out the latest\n",
        "research, it is moving fast (e.g., AdaBound).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de3lWNSgfj0R"
      },
      "source": [
        "## Second order optimization algorithms(Using hessions instead of Jacobians)\n",
        "All the optimization techniques discussed so far only rely on the first-order partial\n",
        "derivatives (Jacobians). The optimization literature contains amazing algorithms\n",
        "based on the second-order partial derivatives (the Hessians, which are the partial\n",
        "derivatives of the Jacobians). Unfortunately, these algorithms are very hard to apply\n",
        "to deep neural networks because there are n2 Hessians per output (where n is the\n",
        "number of parameters), as opposed to just n Jacobians per output. Since DNNs typically\n",
        "have tens of thousands of parameters, **the second-order optimization algorithms often don’t even fit in memory, and even when they do, computing the Hessians is\n",
        "just too slow.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EJk6Blufjx-"
      },
      "source": [
        "## Optimzers to train sparse models\n",
        "\n",
        "All the optimization algorithms just presented produce dense models, meaning that\n",
        "most parameters will be nonzero. If you need a blazingly fast model at runtime, or if\n",
        "you need it to take up less memory, you may prefer to end up with a sparse model\n",
        "instead.\n",
        "\n",
        "One trivial way to achieve this is to train the model as usual, then get rid of the tiny\n",
        "weights (set them to 0). However, this will typically not lead to a very sparse model,\n",
        "and it may degrade the model’s performance.\n",
        "\n",
        "A better option is to apply strong ℓ1 regularization during training, as it pushes the\n",
        "optimizer to zero out as many weights as it can (as in Lasso\n",
        "Regression).\n",
        "\n",
        "However, in some cases these techniques may remain insufficient. \n",
        "One last option is\n",
        "to apply **Dual Averaging, often called Follow The Regularized Leader (FTRL)**, a technique\n",
        "proposed by Yurii Nesterov. When used with ℓ1 regularization, this technique\n",
        "often leads to very sparse models. Keras implements a variant of FTRL called FTRLProximal\n",
        "in the FTRL optimizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvHfjdPxfjvE"
      },
      "source": [
        "## Summarizing all the optimizers:\n",
        "1. SGD: simply looks at the local gradient at a point.\n",
        "2. Momentum optimizer: adds the momentum upto current position and adds it to theta. But to avoid overshooting also friction is introduced, by setting beta=0.9. (basically saying momentum is 0.9 and friction would be like 0.1).\n",
        "3. Nesterov accelerated: same as momentum but looks ahead some distance and accounts for cost gradient at that point ahead instead of cost gradient at current point.\n",
        "3. AdaGrad: Adaptive grad: decays the learning rate by a factor that is root of square of slope at a point. That is more decay in learning rate towards steeper slope dimension. But this algo not useful in deep lerning as learning rate decays too soon without converging.\n",
        "4. RMSProp: same as adagrad but to avoid too much decay of learning rate, s is accumulating only the gradients from the most recent iterations this is achieved by following a exponential decay.\n",
        "5. Adam: Basically momentum + rmsprop\n",
        "\n",
        "    i. Adamax: Not very useful. Adamax just replaces the ℓ2 norm with the ℓ∞ norm (a fancy way of saying the max).\n",
        "\n",
        "    ii. Nadam: Adam + nesterov(looking a short distance ahead)\n",
        "6. Second order: use second order derivatives instead of first but not useful since n^2 terms and hence ofte3n goes out of memory and also are very slow to compute.\n",
        "7. Optimizers for sparse models: \n",
        "\n",
        "    i. Get rid of tiny weights(make them 0)\n",
        "\n",
        "    ii. l1 regularization(pushes weights to 0)\n",
        "\n",
        "    iii. Dual averaging- FLTR(follow the regularized leader). Used alongside l1 normalization this leads to very sparse models.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htASuHgyfjr4"
      },
      "source": [
        "# Learning Schedules- Scheduling the learning rate:\n",
        "one approach is to start with a large learning rate, and\n",
        "divide it by 3 until the training algorithm stops diverging. You will not be too far\n",
        "from the optimal learning rate, which will learn quickly and converge to good solution.\n",
        "\n",
        "There are many different\n",
        "strategies to reduce the learning rate during training. These strategies are called\n",
        "learning schedules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "924DCTQ_fjpa"
      },
      "source": [
        "## Power scheduling\n",
        "Set the learning rate to a function of the iteration number t: η(t) = η0 / (1 + t/k)^c.\n",
        "The initial learning rate η0, the power c (typically set to 1) and the steps s are\n",
        "hyperparameters. The learning rate drops at each step, and after s steps it is down\n",
        "to η0 / 2. After s more steps, it is down to η0 / 3. Then down to η0 / 4, then η0 / 5,\n",
        "and so on. As you can see, this schedule first drops quickly, then more and more\n",
        "slowly. Of course, this requires tuning η0, s (and possibly c)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUBxVbMBnOyQ"
      },
      "source": [
        "\"\"\"Implementing power scheduling in Keras is the easiest option: just set the decay\n",
        "hyperparameter when creating an optimizer. The decay is the inverse of s (the number\n",
        "of steps it takes to divide the learning rate by one more unit), and Keras assumes\n",
        "that c is equal to 1. For example:\"\"\"\n",
        "import keras\n",
        "optimizer = keras.optimizers.SGD(lr=0.01, decay=1e-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpbKt3G3fjm0"
      },
      "source": [
        "## Exponential scheduling\n",
        "Set the learning rate to: η(t) = η0 *((0.1)^(t/s)). The learning rate will gradually drop by a\n",
        "factor of 10 every s steps. While power scheduling reduces the learning rate more\n",
        "and more slowly, exponential scheduling keeps slashing it by a factor of 10 every\n",
        "s steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68H_1UtXnspj"
      },
      "source": [
        "def exponential_decay_fn(epoch):\n",
        "    return 0.01 * 0.1**(epoch / 20)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlO7Br_4nwJV"
      },
      "source": [
        "# If you do not want to hard-code η0 and s, you can create a function that returns a configured function:\n",
        "def exponential_decay(lr0, s):\n",
        "    def exponential_decay_fn(epoch):\n",
        "        return lr0 * 0.1**(epoch / s)\n",
        "    return exponential_decay_fn\n",
        "exponential_decay_fn = exponential_decay(lr0=0.01, s=20)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpDQyt9roCkM"
      },
      "source": [
        "# Next, just create a LearningRateScheduler callback, giving it the schedule function,\n",
        "# and pass this callback to the fit() method:\n",
        "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
        "history = model.fit(X_train_scaled, y_train, [...], callbacks=[lr_scheduler])\n",
        "\n",
        "\"\"\"\n",
        "The LearningRateScheduler will update the optimizer’s learning_rate attribute at\n",
        "the beginning of each epoch. Updating the learning rate just once per epoch is usually\n",
        "enough, but if you want it to be updated more often, for example at every step, you\n",
        "need to write your own callback (see the notebook for an example). This can make\n",
        "sense if there are many steps per epoch.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgQ4o9t5fjjA"
      },
      "source": [
        "## Piecewise constant scheduling\n",
        "Use a constant learning rate for a number of epochs (e.g., η0 = 0.1 for 5 epochs),\n",
        "then a smaller learning rate for another number of epochs (e.g., η1 = 0.001 for 50\n",
        "epochs), and so on. Although this solution can work very well, it requires fiddling around to figure out the right sequence of learning rates, and how long to\n",
        "use each of them.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UgulBgGo9Iz"
      },
      "source": [
        "def piecewise_constant_fn(epoch):\n",
        "    if epoch < 5:\n",
        "        return 0.01\n",
        "    elif epoch < 15:\n",
        "        return 0.005\n",
        "    else:\n",
        "        return 0.001"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRdgKSWRjdob"
      },
      "source": [
        "## Performance scheduling\n",
        "Measure the validation error every N steps (just like for early stopping) and\n",
        "reduce the learning rate by a factor of λ when the error stops dropping."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTOVefVgpGjS"
      },
      "source": [
        "\"\"\"\n",
        "For performance scheduling, simply use the ReduceLROnPlateau callback. For example,\n",
        "if you pass the following callback to the fit() method, it will multiply the learning\n",
        "rate by 0.5 whenever the best validation loss does not improve for 5 consecutive\n",
        "epochs\"\"\" \n",
        "\n",
        "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFQCSASJjdlm"
      },
      "source": [
        "## simpler implementation via tf.keras:\n",
        "tf.keras offers an alternative way to implement learning rate scheduling: just\n",
        "define the learning rate using one of the schedules available in keras.optimizers.schedules, then pass this learning rate to any optimizer. This approach updates\n",
        "the learning rate at each step rather than at each epoch. For example, here is how to\n",
        "implement the same exponential schedule as earlier:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAK6CIqarXOw"
      },
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "s = 20 * len(X_train) // 32 \n",
        "# number of steps in 20 epochs (batch size = 32)\n",
        "\n",
        "learning_rate = keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1)\n",
        "\n",
        "optimizer = keras.optimizers.SGD(learning_rate)\n",
        "\n",
        "\"\"\"\n",
        "This is nice and simple, plus when you save the model, the learning rate and its\n",
        "schedule (including its state) get saved as well. However, this approach is not part of\n",
        "the Keras API, it is specific to tf.keras.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbej-KWpjdio"
      },
      "source": [
        "**Exponential decay or performance scheduling can considerably speed up\n",
        "convergence.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94pZwpFLjdf3"
      },
      "source": [
        "# Avoiding Overfitting Through Regularization\n",
        "With four parameters I can fit an elephant and with five I can make him wiggle his\n",
        "trunk.\n",
        "—John von Neumann, cited by Enrico Fermi in Nature 427\n",
        "\n",
        "With thousands of parameters you can fit the whole zoo. Deep neural networks typically\n",
        "have tens of thousands of parameters, sometimes even millions. With so many\n",
        "parameters, the network has an incredible amount of freedom and can fit a huge variety\n",
        "of complex datasets. But this great flexibility also means that it is prone to overfitting\n",
        "the training set. \n",
        "\n",
        "We need regularization.\n",
        "We already implemented one of the best regularization techniques earler:\n",
        "early stopping. Moreover, even though Batch Normalization was designed to solve\n",
        "the vanishing/exploding gradients problems, is also acts like a pretty good regularizer.\n",
        "\n",
        "Other popular regularization techniques for neural networks:\n",
        "ℓ1 and ℓ2 regularization, dropout and max-norm regularization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkKMMYO7jdc7"
      },
      "source": [
        "## ℓ1 and ℓ2 Regularization\n",
        "Just like for simple linear models, you can use ℓ1 and ℓ2 regularization\n",
        "to constrain a neural network’s connection weights (but typically not its biases).\n",
        "Here is how to apply ℓ2 regularization to a Keras layer’s connection weights,\n",
        "using a regularization factor of 0.01:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_J3X97Bs5vJ"
      },
      "source": [
        "layer = keras.layers.Dense(100, activation=\"elu\",kernel_initializer=\"he_normal\", kernel_regularizer=keras.regularizers.l2(0.01))\n",
        "\n",
        "\"\"\"\n",
        "The l2() function returns a regularizer that will be called to compute the regularization\n",
        "loss, at each step during training. This regularization loss is then added to the\n",
        "final loss. As you might expect, you can just use keras.regularizers.l1() if you\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sf2o5uBjhKWQ"
      },
      "source": [
        "\"\"\"\n",
        "Since you will typically want to apply the same regularizer to all layers in your network,\n",
        "as well as the same activation function and the same initialization strategy in all\n",
        "hidden layers, you may find yourself repeating the same arguments over and over.\n",
        "This makes it ugly and error-prone. To avoid this, you can try refactoring your code\n",
        "to use loops. Another option is to use Python’s functools.partial() function: it lets\n",
        "you create a thin wrapper for any callable, with some default argument values. For\n",
        "example:\"\"\"\n",
        "from functools import partial\n",
        "RegularizedDense = partial(keras.layers.Dense, activation=\"elu\",kernel_initializer=\"he_normal\", kernel_regularizer=keras.regularizers.l2(0.01))\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "keras.layers.Flatten(input_shape=[28, 28]),\n",
        "RegularizedDense(300),\n",
        "RegularizedDense(100),\n",
        "RegularizedDense(10, activation=\"softmax\",\n",
        "kernel_initializer=\"glorot_uniform\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASfOnErEjdaI"
      },
      "source": [
        "## Dropout\n",
        "Dropout is one of the most popular regularization techniques for deep neural networks.\n",
        "Neural networks got a 1–2% accuracy boost simply by adding dropout. \n",
        "\n",
        "It is a fairly simple algorithm: at every training step, every neuron (including the\n",
        "input  and hidden neurons, but always excluding the output neurons) has a probability p of being\n",
        "temporarily “dropped out,” meaning it will be entirely ignored during this training\n",
        "step, but it may be active during the next step (see Figure 11-9). The hyperparameter\n",
        "p is called the dropout rate, and it is typically set to 50%. After training, neurons don’t\n",
        "get dropped anymore. And that’s all.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36_2PaOvjdW3"
      },
      "source": [
        "### Intution for dropout:\n",
        "Would a\n",
        "company perform better if its employees were told to toss a coin every morning to\n",
        "decide whether or not to go to work? Well, who knows; perhaps it would! The company\n",
        "would obviously be forced to adapt its organization; it could not rely on any single\n",
        "person to fill in the coffee machine or perform any other critical tasks, so this\n",
        "expertise would have to be spread across several people. Employees would have to\n",
        "learn to cooperate with many of their coworkers, not just a handful of them. The\n",
        "company would become much more resilient. If one person quit, it wouldn’t make\n",
        "much of a difference. It’s unclear whether this idea would actually work for companies,\n",
        "but it certainly does for neural networks. \n",
        "\n",
        "Neurons trained with dropout cannot\n",
        "co-adapt with their neighboring neurons; they have to be as useful as possible on\n",
        "their own. They also cannot rely excessively on just a few input neurons; they must\n",
        "pay attention to each of their input neurons. They end up being less sensitive to slight\n",
        "changes in the inputs. In the end you get a more robust network that generalizes better.\n",
        "\n",
        "Another way to understand the power of dropout is to realize that a unique neural\n",
        "network is generated at each training step. Since each neuron can be either present or\n",
        "absent, there is a total of 2N possible networks (where N is the total number of droppable\n",
        "neurons). This is such a huge number that it is virtually impossible for the same\n",
        "neural network to be sampled twice. Once you have run a 10,000 training steps, you\n",
        "have essentially trained 10,000 different neural networks (each with just one training\n",
        "instance). These neural networks are obviously not independent since they share\n",
        "many of their weights, but they are nevertheless all different. The resulting neural\n",
        "network can be seen as an averaging ensemble of all these smaller neural networks.\n",
        "\n",
        "There is one small but important technical detail. Suppose p = 50%, in which case\n",
        "during testing a neuron will be connected to twice as many input neurons as it was\n",
        "(on average) during training. To compensate for this fact, we need to multiply each neuron’s input connection weights by 0.5 after training. If we don’t, each neuron will\n",
        "get a total input signal roughly twice as large as what the network was trained on, and\n",
        "it is unlikely to perform well. More generally, we need to multiply each input connection\n",
        "weight by the keep probability (1 – p) after training. \n",
        "\n",
        "Alternatively, we can divide\n",
        "each neuron’s output by the keep probability during training (these alternatives are\n",
        "not perfectly equivalent, but they work equally well).\n",
        "To implement dropout using Keras, you can use the keras.layers.Dropout layer.\n",
        "During training, it randomly drops some inputs (setting them to 0) and divides the\n",
        "remaining inputs by the keep probability. After training, it does nothing at all, it just\n",
        "passes the inputs to the next layer. For example, the following code applies dropout\n",
        "regularization before every Dense layer, using a dropout rate of 0.2:\n",
        "model = keras.models.Sequential([\n",
        "keras.layers.Flatten(input_shape=[28, 28]),\n",
        "keras.layers.Dropout(rate=0.2),\n",
        "keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "keras.layers.Dropout(rate=0.2),\n",
        "keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "keras.layers.Dropout(rate=0.2),\n",
        "keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "Since dropout is only active during training, the training loss is\n",
        "penalized compared to the validation loss, so comparing the two\n",
        "can be misleading. In particular, a model may be overfitting the\n",
        "training set and yet have similar training and validation losses. So\n",
        "make sure to evaluate the training loss without dropout (e.g., after\n",
        "training). Alternatively, you can call the fit() method inside a\n",
        "with keras.backend.learning_phase_scope(1) block: this will\n",
        "force dropout to be active during both training and validation.25\n",
        "If you observe that the model is overfitting, you can increase the dropout rate. Conversely,\n",
        "you should try decreasing the dropout rate if the model underfits the training\n",
        "set. It can also help to increase the dropout rate for large layers, and reduce it for\n",
        "small ones. Moreover, many state-of-the-art architectures only use dropout after the\n",
        "last hidden layer, so you may want to try this if full dropout is too strong.\n",
        "Dropout does tend to significantly slow down convergence, but it usually results in a\n",
        "much better model when tuned properly. So, it is generally well worth the extra time\n",
        "and effort."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkaAhRpijdT3"
      },
      "source": [
        "If you want to regularize a self-normalizing network based on the\n",
        "SELU activation function (as discussed earlier), you should use\n",
        "AlphaDropout: this is a variant of dropout that preserves the mean\n",
        "and standard deviation of its inputs (it was introduced in the same\n",
        "paper as SELU, as regular dropout would break self-normalization).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1ADFNYWjdQ0"
      },
      "source": [
        "## Monte-Carlo (MC) Dropout\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xxwp9pQjdN5"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrevw8CyjdKL"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3eCPzBtjdHJ"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMbLdMHnjdCt"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGXw0oFkfjft"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5Ol18M6feyu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1YkQLI3ffmF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcER1Ldrfg53"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXPdxlQXcDis"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_JVrTocvljq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XF-clykvvlmi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbXo3XYBvlug"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDPgOqLCvlxU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW4jP1qkvlzq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ked0JFQpvl2H"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJEuHkMevl4i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpVNpW32vl69"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ha3z5KPvl9f"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyTyR1n5vl_x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBzl3-tlvmC3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}