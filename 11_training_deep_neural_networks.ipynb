{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "11_training_deep_neural_networks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPxAemjern0PZLfmgerWF7A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Richish/hands_on_ml/blob/master/11_training_deep_neural_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZld5fT5Ci-W"
      },
      "source": [
        "# Challenges\n",
        "when traiining a much deeper DNN, perhaps with 10 layers or much more, each containing hundreds of neurons, connected by hundreds of thousands of connections.\n",
        "\n",
        "1. Vanishing gradients problem (or the related exploding gradients problem) that affects deep neural networks and makes lower layers very hard to train. \n",
        "2. You might not have enough training data for such a large network, or it might be too costly to label - Solved by transfer learning.\n",
        "3. Training may be extremely slow - solved by various optimizers.\n",
        "4. A model with millions of parameters would severely risk overfitting the training set, especially if there are not enough training instances, or they are too noisy. - solved by regularization techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMVvrhnCCjKg"
      },
      "source": [
        "# Vanishing/Exploding Gradients Problems\n",
        "\n",
        "During backpropagation: gradients often get smaller and smaller as the algorithm progresses down to the lower layers. As a result, the Gradient Descent update leaves the lower layer connection weights virtually unchanged, and training never converges to a good\n",
        "solution. This is called the vanishing gradients problem. In some cases, the opposite\n",
        "can happen: the gradients can grow bigger and bigger, so many layers get insanely\n",
        "large weight updates and the algorithm diverges. This is the exploding gradients problem,\n",
        "which is mostly encountered in recurrent neural networks. More generally,\n",
        "deep neural networks suffer from unstable gradients; different layers may learn at\n",
        "widely different speeds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8Oy4C1BCjNR"
      },
      "source": [
        "This behavior was one of the reasons why deep neural networks were mostly abandoned for a\n",
        "long time, it is only around 2010 that significant progress was made in understanding\n",
        "it. \n",
        "\n",
        "A paper titled “Understanding the Difficulty of Training Deep Feedforward\n",
        "Neural Networks” by Xavier Glorot and Yoshua Bengio found a few suspects, including\n",
        "the combination of the popular logistic sigmoid activation function and the\n",
        "weight initialization technique that was most popular at the time, namely random initialization\n",
        "using a normal distribution with a mean of 0 and a standard deviation of 1.\n",
        "In short, they showed that with this activation function and this initialization scheme,\n",
        "the variance of the outputs of each layer is much greater than the variance of its\n",
        "inputs. Going forward in the network, the variance keeps increasing after each layer\n",
        "until the activation function saturates at the top layers. This is actually made worse by\n",
        "the fact that the logistic function has a mean of 0.5, not 0 (the hyperbolic tangent\n",
        "function has a mean of 0 and behaves slightly better than the logistic function in deep\n",
        "networks).\n",
        "\n",
        "When\n",
        "inputs become large (negative or positive), the function saturates at 0 or 1, with a\n",
        "derivative extremely close to 0. Thus when backpropagation kicks in, it has virtually\n",
        "no gradient to propagate back through the network, and what little gradient exists\n",
        "keeps getting diluted as backpropagation progresses down through the top layers, so\n",
        "there is really nothing left for the lower layers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVacXQYvCjSd"
      },
      "source": [
        "## Initializers- Glorot, LeCunn and He Initializations\n",
        "\n",
        "### Glorot\n",
        "In their paper, Glorot and Bengio propose a way to significantly alleviate this problem.\n",
        "We need the signal to flow properly in both directions: in the forward direction\n",
        "when making predictions, and in the reverse direction when backpropagating gradients.\n",
        "We don’t want the signal to die out, nor do we want it to explode and saturate.\n",
        "For the signal to flow properly, the authors argue that we need the variance of the\n",
        "outputs of each layer to be equal to the variance of its inputs,2 and we also need the\n",
        "gradients to have equal variance before and after flowing through a layer in the\n",
        "reverse direction. \n",
        "\n",
        "It is actually not possible to guarantee both unless the layer has an equal\n",
        "number of inputs and neurons (these numbers are called the fan-in and fan-out of the\n",
        "layer), but they proposed a good compromise that has proven to work very well in\n",
        "practice: the connection weights of each layer must be initialized randomly as described in Equation below, where fan{avg} = (fan{in} + fan{out})/2. This initialization strategy is called Xavier initialization (after the author’s first name) or Glorot initialization (after his last name).\n",
        "\n",
        "Normal distribution with mean 0 and variance: σ^2 = 1/fan{avg}\n",
        "\n",
        "Or a uniform distribution between −r and + r, with r = root(3/fan{avg})\n",
        "\n",
        "### LeCunn\n",
        "If you just replace fan{avg} with fan{in} in above eqn, you get an initialization strategy\n",
        "that was actually already proposed by Yann LeCun in the 1990s, called LeCun initialization. It is equivalent to Glorot initialization when fan{in} = fan{out}. It took over a decade for researchers to realize\n",
        "just how important this trick really is. Using Glorot initialization can speed up training considerably, and it is one of the tricks that led to the current success of Deep Learning.\n",
        "\n",
        "### He\n",
        "Some papers have provided similar strategies for different activation functions.\n",
        "These strategies differ only by the scale of the variance and whether they use fan{avg} or fan{in}, as shown in Table below - for the uniform distribution, just compute r = root(3.σ^2). \n",
        "\n",
        "The initialization strategy for the ReLU activation function (and its variants, including the ELU activation described shortly) is sometimes called He initialization (after the last name of its author). \n",
        "\n",
        "The SELU activation function will be explained . It should be used with LeCun initialization (preferably with a normal distribution, as we will see).\n",
        "\n",
        "### Table of initializers:\n",
        "| Initialization      | Activation functions | σ^2 (Normal)    |\n",
        "| :---        |    :----   |          :--- |\n",
        "| Glorot      | None, Logistic, tanh, Softmax      | 1/fan{avg}  |\n",
        "| LeCunn   | SELU        | 1/fan{in}      |\n",
        "| He   | RELU        | 2/fan{in}      |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQ7d6CgwCjVI"
      },
      "source": [
        "By default, Keras uses Glorot initialization with a uniform distribution. You can\n",
        "change this to He initialization by setting kernel_initializer=\"he_uniform\" or kernel_initializer=\"he_normal\" when creating a layer, like this:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rM8OkWOS_zF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "37c52392-433c-46ca-dc87-86bf9e609b31"
      },
      "source": [
        "import keras\n",
        "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.layers.core.Dense at 0x7ff6d47a33c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9R0_ZR-TXbN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0492833d-52d9-4f3f-d273-45a2381d5e9d"
      },
      "source": [
        "# If you want He initialization with a uniform distribution, but based on fanavg rather\n",
        "# than fanin, you can use the VarianceScaling initializer like this:\n",
        "he_avg_init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg', distribution='uniform') # basically a custom initializer\n",
        "keras.layers.Dense(10, activation=\"sigmoid\", kernel_initializer=he_avg_init)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.layers.core.Dense at 0x7ff6d5128358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Y4kaf_BXnUN"
      },
      "source": [
        "## Activation Functions- Nonsaturating Activation Functions\n",
        "\n",
        "### Relu\n",
        "One of the insights in the 2010 paper by Glorot and Bengio was that the vanishing/\n",
        "exploding gradients problems were in part due to a poor choice of activation function.\n",
        "Until then most people had assumed that if Mother Nature had chosen to use\n",
        "roughly sigmoid activation functions in biological neurons, they must be an excellent\n",
        "choice. But it turns out that other activation functions behave much better in deep\n",
        "neural networks, in particular the ReLU activation function, mostly because it does\n",
        "not saturate for positive values (and also because it is quite fast to compute).\n",
        "\n",
        "#### Problem of dying relus:\n",
        "During training in relu, some neurons effectively die, meaning\n",
        "they stop outputting anything other than 0. In some cases, you may find that half of your network’s neurons are dead, especially if you used a large learning rate. A neuron dies when its weights get tweaked in such a way that the weighted sum of its inputs are negative for all instances in the training set. When this happens, it just keeps outputting 0s, and gradient descent does not affect it anymore since the gradient\n",
        "of the ReLU function is 0 when its input is negative.\n",
        "\n",
        "### Leaky relu(solves the problem of dying relu):\n",
        "This function is defined as LeakyReLUα(z) = max(αz, z). The hyperparameter α defines how much the function “leaks”: it is the\n",
        "slope of the function for z < 0, and is typically set to 0.01. This small slope ensures that leaky ReLUs never die; they can go into a long coma, but they have a chance to eventually wake up. \n",
        "\n",
        "A 2015 paper compared several variants of the ReLU activation function and one of its conclusions was that the leaky variants always outperformed the strict ReLU activation function. In fact, setting α = 0.2 (huge leak) seemed to result in better performance than α = 0.01 (small leak). \n",
        "\n",
        "They also evaluated the **randomized leaky ReLU (RReLU)***, where α is picked randomly in a given range during training, and it is fixed to an average value during testing. It also performed fairly well and seemed to act as a regularizer (reducing the risk of overfitting the training set).\n",
        "\n",
        "Finally, they also evaluated the **parametric leaky ReLU (PReLU)**, where α is authorized to be learned during training (instead of being a hyperparameter, it becomes a parameter that can be modified by backpropagation like any other parameter). This was reported to strongly outperform ReLU on large image datasets, but on smaller\n",
        "datasets it runs the risk of overfitting the training set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E12NRlMFCjX5"
      },
      "source": [
        "### ELU\n",
        "A 2015 paper by Djork-Arné Clevert et al.6 proposed a new activation\n",
        "function called the exponential linear unit (ELU) that outperformed all the ReLU variants in their experiments: training time was reduced and the neural network performed better on the test set. \n",
        "ELU activation function:\n",
        "ELU{α} (z) = α (exp(z) − 1) if z < 0 else z\n",
        "\n",
        "It looks like relu for +ve values of z.\n",
        "\n",
        " 3 differences from relu:\n",
        "1. It takes on negative values when z < 0, which allows the unit to have an average output closer to 0. This helps alleviate the vanishing gradients problem, as discussed earlier. The hyperparameter α defines the value that the ELU function approaches when z is a large negative number. It is usually set to 1, but you can tweak it like any other hyperparameter if you want.\n",
        "2. It has a nonzero gradient for z < 0, which avoids the dead neurons problem.\n",
        "3. If α is equal to 1 then the function is smooth everywhere, including\n",
        "around z = 0, which helps speed up Gradient Descent, since it does not bounce as much left and right of z = 0.\n",
        "\n",
        "Drawbacks of ELU:\n",
        "The main drawback of the ELU activation function is that it is slower to compute than the ReLU and its variants (due to the use of the exponential function), but during training this is compensated by the faster convergence rate. However, at test time an ELU network will be slower than a ReLU network.\n",
        "\n",
        "#### SELU (Scaled ELU)\n",
        "In a 2017 paper7 by Günter Klambauer et al., called “Self-Normalizing\n",
        "Neural Networks”, the authors showed that if you build a neural network composed exclusively of a stack of dense layers, and if all hidden layers use the SELU activation function (which is just a scaled version of the ELU activation function, as its name\n",
        "suggests), then the network will self-normalize: the output of each layer will tend to preserve mean 0 and standard deviation 1 during training, which solves the vanishing/ exploding gradients problem. As a result, this activation function often outperforms other activation functions very significantly for such neural nets (especially\n",
        "deep ones). **However, there are a few conditions for self-normalization to happen:**\n",
        "\n",
        "1. The input features must be standardized (mean 0 and standard deviation 1).\n",
        "2. Every hidden layer’s weights must also be initialized using LeCun normal initialization. In Keras, this means setting kernel_initializer=\"lecun_normal\".\n",
        "3. **The network’s architecture must be sequential.** Unfortunately, if you try to use SELU in non-sequential architectures, such as recurrent networks or networks with skip connections (i.e., connections that skip layers, such as in wide & deep nets), self-normalization will not be guaranteed, so SELU will not necessarily outperform other activation functions.\n",
        "4. The paper only guarantees self-normalization if all layers are dense. However, in practice the SELU activation function seems to work great with convolutional neural nets as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNBkW6G1ouQ_"
      },
      "source": [
        "### Which optimization function to use:\n",
        "\n",
        "For the hidden layers of your deep neural networks- Although your mileage will vary, in\n",
        "general:\n",
        "\n",
        "SELU > ELU > leaky ReLU (and its variants) > ReLU > tanh\n",
        "> logistic. \n",
        "\n",
        "If the network’s architecture prevents it from self-normalizing,\n",
        "then ELU may perform better than SELU (since SELU\n",
        "is not smooth at z = 0). \n",
        "\n",
        "If you care a lot about runtime latency, then you may prefer leaky ReLU. \n",
        "\n",
        "If you don’t want to tweak yet another hyperparameter, you may just use the default α values used by Keras (e.g., 0.3 for the leaky ReLU). \n",
        "If you have spare time and computing power, you can use cross-validation to evaluate other activation functions, in particular RReLU if your network is overfitting, or PReLU if you have a huge training set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xiv6Wn46pk06"
      },
      "source": [
        "# To use the leaky ReLU activation function, you must create a LeakyReLU instance like this:\n",
        "from tensorflow import keras\n",
        "leaky_relu = keras.layers.LeakyReLU(alpha=0.2)\n",
        "layer = keras.layers.Dense(10, activation=leaky_relu, kernel_initializer=\"he_normal\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNS4Yx8Cpk38"
      },
      "source": [
        "# For PReLU, just replace LeakyRelu(alpha=0.2) with PReLU(). There is currently no\n",
        "# official implementation of RReLU in Keras, but you can fairly easily implement your own.\n",
        "p_relu = keras.layers.PReLU()\n",
        "layer = keras.layers.Dense(10, activation=p_relu, kernel_initializer=\"he_normal\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTwy3PFspk68"
      },
      "source": [
        "# For SELU activation, just set activation=\"selu\" and kernel_initializer=\"lecun_normal\" when creating a layer:\n",
        "layer = keras.layers.Dense(10, activation=\"selu\", kernel_initializer=\"lecun_normal\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlsgdFahpf_b"
      },
      "source": [
        "## Batch Normalization:\n",
        "\n",
        "Although using He initialization along with ELU (or any variant of ReLU) can significantly\n",
        "reduce the vanishing/exploding gradients problems at the beginning of training,\n",
        "it doesn’t guarantee that they won’t come back during training.\n",
        "\n",
        "Batch Normalization consists of adding an operation in the model just before or after the\n",
        "activation function of each hidden layer, simply zero-centering and normalizing each\n",
        "input, then scaling and shifting the result using two new parameter vectors per layer:\n",
        "one for scaling, the other for shifting. This operation lets the model\n",
        "learn the optimal scale and mean of each of the layer’s inputs. \n",
        "\n",
        "In many cases, if you\n",
        "add a BN layer as the very first layer of your neural network, you do not need to\n",
        "standardize your training set (e.g., using a StandardScaler): the BN layer will do it\n",
        "for you (well, approximately, since it only looks at one batch at a time, and it can also\n",
        "rescale and shift each input feature).\n",
        "\n",
        "In order to zero-center and normalize the inputs, the algorithm needs to estimate\n",
        "each input’s mean and standard deviation. It does so by evaluating the mean and standard\n",
        "deviation of each input over the current mini-batch (hence the name “Batch\n",
        "Normalization”).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiYIP93Wvms0"
      },
      "source": [
        "So during training, BN just standardizes its inputs then rescales and offsets them.\n",
        "What about at test time?\n",
        "\n",
        "It is often preferred to estimate these final statistics\n",
        "during training using a moving average of the layer’s input means and standard\n",
        "deviations. To sum up, four parameter vectors are learned in each batch-normalized\n",
        "layer: γ (the ouput scale vector) and β (the output offset vector) are learned through\n",
        "regular backpropagation, and μ (the final input mean vector), and σ (the final input\n",
        "standard deviation vector) are estimated using an exponential moving average. Note\n",
        "that μ and σ are estimated during training, but they are not used at all during training,\n",
        "only after training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C332W2YWvmyn"
      },
      "source": [
        "Batch Normalization\n",
        "also acts like a regularizer, reducing the need for other regularization techniques\n",
        "(such as dropout, described later in this chapter).\n",
        "Batch Normalization does, however, add some complexity to the model (although it\n",
        "can remove the need for normalizing the input data, as we discussed earlier). Moreover,\n",
        "there is a runtime penalty: the neural network makes slower predictions due to\n",
        "the extra computations required at each layer. So if you need predictions to be\n",
        "lightning-fast, you may want to check how well plain ELU + He initialization perform\n",
        "before playing with Batch Normalization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6HmG3k5vm9F"
      },
      "source": [
        "You may find that training is rather slow, because each epoch takes\n",
        "much more time when you use batch normalization. However, this\n",
        "is usually counterbalanced by the fact that convergence is much\n",
        "faster with BN, so it will take fewer epochs to reach the same performance.\n",
        "All in all, wall time will usually be smaller (this is the\n",
        "time measured by the clock on your wall)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIyh93WXvnC5"
      },
      "source": [
        "### Implementing Batch Normalization with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hca-FrZcvnI5"
      },
      "source": [
        "Just add a BatchNormalization layer before or after each hidden layer’s activation\n",
        "function, and optionally add a BN layer as well as the first layer in your model. For\n",
        "example, this model applies BN after every hidden layer and as the first layer in the\n",
        "model (after flattening the input images):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zk5rMNcawjAm"
      },
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxFY-LR-wy5G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eeb8e426-4697-40fe-91b3-13a8a75702e6"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 784)               3136      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 300)               235500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 300)               1200      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100)               30100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 271,346\n",
            "Trainable params: 268,978\n",
            "Non-trainable params: 2,368\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpKPcjwKwyIn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c5035d8-aa1b-4e4d-86c3-035bf43a09c0"
      },
      "source": [
        "[(var.name, var.trainable) for var in model.layers[1].variables]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('batch_normalization/gamma:0', True),\n",
              " ('batch_normalization/beta:0', True),\n",
              " ('batch_normalization/moving_mean:0', False),\n",
              " ('batch_normalization/moving_variance:0', False)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jOOXoFvnFv"
      },
      "source": [
        "The authors of the BN paper argued in favor of adding the BN layers before the activation\n",
        "functions, rather than after (as we just did). There is some debate about this, as\n",
        "it seems to depend on the task. So that’s one more thing you can experiment with to\n",
        "see which option works best on your dataset. To add the BN layers before the activation\n",
        "functions, we must remove the activation function from the hidden layers, and\n",
        "add them as separate layers after the BN layers. Moreover, since a Batch Normalization\n",
        "layer includes one offset parameter per input, you can remove the bias term from\n",
        "the previous layer (just pass use_bias=False when creating it):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDixSTNwx6Yv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8a6053a-2283-43a4-966f-a5973d534896"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(300, kernel_initializer=\"he_normal\", use_bias=False),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Activation(\"elu\"),\n",
        "    keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
        "    keras.layers.Activation(\"elu\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_2 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 784)               3136      \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 300)               235200    \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 300)               1200      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 100)               30000     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 270,946\n",
            "Trainable params: 268,578\n",
            "Non-trainable params: 2,368\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJs51wqzvnAK"
      },
      "source": [
        "#### Hyperparameters of Batch Normalization layer:\n",
        "\n",
        "1. Momentum:  This hyperparameter is used when updating the exponential moving averages: given a\n",
        "new value v.\n",
        "Running average, V{avg} is calculated using eqn:\n",
        "V{avg} = V{avg}*momentum + v{new_batch}*(1-momentum)\n",
        "\n",
        "A good momentum value is typically close to 1—for example, 0.9, 0.99, or 0.999 (you\n",
        "want more 9s for larger datasets and smaller mini-batches).\n",
        "\n",
        "2. Axis: it determines which axis should be normalized.\n",
        "It defaults to –1, meaning that by default it will normalize the last axis (using\n",
        "the means and standard deviations computed across the other axes). For example,\n",
        "when the input batch is 2D (i.e., the batch shape is [batch size, features]), this means\n",
        "that each input feature will be normalized based on the mean and standard deviation\n",
        "computed across all the instances in the batch. For example, the first BN layer in the\n",
        "previous code example will independently normalize (and rescale and shift) each of\n",
        "the 784 input features. However, if we move the first BN layer before the Flatten\n",
        "layer, then the input batches will be 3D, with shape [batch size, height, width], therefore\n",
        "the BN layer will compute 28 means and 28 standard deviations.\n",
        "\n",
        "\n",
        "it will normalize all pixels in a given column using the same mean and standard deviation.\n",
        "There will also be just 28 scale parameters and 28 shift parameters. If instead\n",
        "you still want to treat each of the 784 pixels independently, then you should set\n",
        "axis=[1, 2]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1i7v4jxvm5y"
      },
      "source": [
        "Notice that the BN layer does not perform the same computation during training and\n",
        "after training: it uses batch statistics during training, and the “final” statistics after\n",
        "training (i.e., the final value of the moving averages)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVGd1wwVvmvm"
      },
      "source": [
        "Batch Normalization has become one of the most used layers in deep neural networks,\n",
        "to the point that it is often omitted in the diagrams, as it is assumed that BN is\n",
        "added after every layer. \n",
        "\n",
        "However, a very recent paper10 by Hongyi Zhang et al. may\n",
        "well change this: the authors show that by using a novel fixed-update (fixup) weight\n",
        "initialization technique, they manage to train a very deep neural network (10,000 layers!)\n",
        "without BN, achieving state-of-the-art performance on complex image classification\n",
        "tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2_2csjD3E8S"
      },
      "source": [
        "## Gradient Clipping\n",
        "\n",
        "Another popular technique to lessen the exploding gradients problem is to simply\n",
        "clip the gradients during backpropagation so that they never exceed some threshold.\n",
        "This is called Gradient Clipping.\n",
        "\n",
        "This technique is most often used in recurrent neural networks, as Batch Normalization is tricky to use in RNNs.\n",
        "For other types of networks, BN is usually sufficient.\n",
        "\n",
        "In Keras, implementing Gradient Clipping is just a matter of setting the clipvalue or\n",
        "clipnorm argument when creating an optimizer. For example:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysviAWPi4I93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "846ed05a-0a10-4575-8ab8-962abdd2bf03"
      },
      "source": [
        "optimizer = keras.optimizers.SGD(clipvalue=1.0)\n",
        "model.compile(loss=\"mse\", optimizer=optimizer)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_2 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 784)               3136      \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 300)               235200    \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 300)               1200      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 100)               30000     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 270,946\n",
            "Trainable params: 268,578\n",
            "Non-trainable params: 2,368\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RCd2pDe3FDT"
      },
      "source": [
        "This will clip every component of the gradient vector to a value between –1.0 and 1.0.\n",
        "This means that all the partial derivatives of the loss (with regards to each and every\n",
        "trainable parameter) will be clipped between –1.0 and 1.0. The threshold is a hyperparameter\n",
        "you can tune. Note that it may change the orientation of the gradient vector:\n",
        "for example, if the original gradient vector is [0.9, 100.0], it points mostly in the\n",
        "direction of the second axis, but once you clip it by value, you get [0.9, 1.0], which\n",
        "points roughly in the diagonal between the two axes. In practice however, this\n",
        "approach works well. \n",
        "\n",
        "If you want to ensure that Gradient Clipping does not change\n",
        "the direction of the gradient vector, you should clip by norm by setting clipnorm\n",
        "instead of clipvalue. This will clip the whole gradient if its ℓ2 norm is greater than\n",
        "the threshold you picked. For example, if you set clipnorm=1.0, then the vector [0.9,\n",
        "100.0] will be clipped to [0.00899964, 0.9999595], preserving its orientation, but\n",
        "almost eliminating the first component. If you observe that the gradients explode\n",
        "during training (you can track the size of the gradients using TensorBoard), you may\n",
        "want to try both clipping by value and clipping by norm, with different threshold,\n",
        "and see which option performs best on the validation set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYvxV4273FMw"
      },
      "source": [
        "# Transfer Learning\n",
        "\n",
        "## Reusing pretrained layers\n",
        "\n",
        "It is generally not a good idea to train a very large DNN from scratch: instead, you\n",
        "should always try to find an existing neural network that accomplishes a similar task\n",
        "to the one you are trying to tackle, then just reuse the lower layers of this network: this is called transfer learning. It will\n",
        "not only **speed up training considerably, but will also require much less training data.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyBd9My23FS7"
      },
      "source": [
        "For example, suppose that you have access to a DNN that was trained to classify pictures\n",
        "into 100 different categories, including animals, plants, vehicles, and everyday\n",
        "objects. You now want to train a DNN to classify specific types of vehicles. These\n",
        "tasks are very similar, even partly overlapping, so you should try to reuse parts of the\n",
        "first network.\n",
        "\n",
        "You\n",
        "want to find the right number of layers to reuse.\n",
        "The more similar the tasks are, the more layers you want to reuse\n",
        "(starting with the lower layers). For very similar tasks, you can try\n",
        "keeping all the hidden layers and just replace the output layer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZ5dwmQ53FZJ"
      },
      "source": [
        "Try freezing all the reused layers first (i.e., make their weights non-trainable, so gradient\n",
        "descent won’t modify them), then train your model and see how it performs.\n",
        "Then try unfreezing one or two of the top hidden layers to let backpropagation tweak\n",
        "them and see if performance improves. The more training data you have, the more layers you can unfreeze. It is also useful to reduce the learning rate when you unfreeze\n",
        "reused layers: this will avoid wrecking their fine-tuned weights.\n",
        "If you still cannot get good performance, and you have little training data, try dropping\n",
        "the top hidden layer(s) and freeze all remaining hidden layers again. You can\n",
        "iterate until you find the right number of layers to reuse. If you have plenty of training\n",
        "data, you may try replacing the top hidden layers instead of dropping them, and\n",
        "even add more hidden layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tsam77x3Ffi"
      },
      "source": [
        "## Transfer Learning With Keras\n",
        "\n",
        "Let’s look at an example. Suppose the fashion MNIST dataset only contained 8 classes,\n",
        "for example all classes except for sandals and shirts. Someone built and trained a\n",
        "Keras model on that set and got reasonably good performance (>90% accuracy). Let’s\n",
        "call this model A. You now want to tackle a different task: you have images of sandals\n",
        "and shirts, and you want to train a binary classifier (positive=shirts, negative=sandals).\n",
        "However, your dataset is quite small, you only have 200 labeled images. When\n",
        "you train a new model for this task (let’s call it model B), with the same architecture\n",
        "as model A, it performs reasonably well (97.2% accuracy), but since it’s a much easier\n",
        "task (there are just 2 classes), you were hoping for more. You realize that your task is quite similar to task A, so perhaps transfer\n",
        "learning can help? \n",
        "\n",
        "Let’s find out!\n",
        "First, you need to load model A, and create a new model based on the model A’s layers.\n",
        "Let’s reuse all layers except for the output layer:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKMv0A9NBI1p"
      },
      "source": [
        "model_A = keras.models.load_model(\"my_model_A.h5\")\n",
        "\n",
        "model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
        "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2bDBgKrBdV6"
      },
      "source": [
        "model_A and model_B_on_A now share some layers. When you train\n",
        "model_B_on_A, it will also affect model_A. If you want to avoid that, you need to clone\n",
        "model_A before you reuse its layers. To do this, you must clone model A’s architecture,\n",
        "then copy its weights (since clone_model() does not clone the weights):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bQ6WYNtBcP9"
      },
      "source": [
        "model_A_clone = keras.models.clone_model(model_A)\n",
        "model_A_clone.set_weights(model_A.get_weights())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsmhlswI3Fi8"
      },
      "source": [
        "Now we could just train model_B_on_A for task B, but since the new output layer was\n",
        "initialized randomly, it will make large errors, at least during the first few epochs, so\n",
        "there will be large error gradients that may wreck the reused weights. To avoid this,\n",
        "one approach is to freeze the reused layers during the first few epochs, giving the new\n",
        "layer some time to learn reasonable weights. To do this, simply set every layer’s train\n",
        "able attribute to False and compile the model:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJ5Dr3wfBwWb"
      },
      "source": [
        "for layer in model_B_on_A.layers[:-1]:\n",
        "    layer.trainable = False\n",
        "\n",
        "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\",metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHwtkLJn3FcY"
      },
      "source": [
        "**You must always compile your model after you freeze or unfreeze\n",
        "layers.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puOmWafZ3FV7"
      },
      "source": [
        "Next, we can train the model for a few epochs, then unfreeze the reused layers (which\n",
        "requires compiling the model again) and continue training to fine-tune the reused\n",
        "layers for task B. After unfreezing the reused layers, it is usually a good idea to reduce\n",
        "the learning rate, once again to avoid damaging the reused weights:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYMEA9z3CEBj"
      },
      "source": [
        "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4, validation_data=(X_valid_B, y_valid_B))\n",
        "\n",
        "for layer in model_B_on_A.layers[:-1]:\n",
        "    layer.trainable = True\n",
        "optimizer = keras.optimizers.SGD(lr=1e-4) # the default lr is 1e-3\n",
        "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16, validation_data=(X_valid_B, y_valid_B))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewsGHbCK3FP6"
      },
      "source": [
        "**It turns out that transfer learning does not work very well\n",
        "with small dense networks: it works best with deep convolutional neural networks**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4he4GH33FJ2"
      },
      "source": [
        "## Unsupervised Pretraining\n",
        "\n",
        "**good option\n",
        "when you have a complex task to solve, no similar model you can reuse, and little\n",
        "labeled training data but plenty of unlabeled training data.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ucz0ftV3E_0"
      },
      "source": [
        "Suppose you want to tackle a complex task for which you don’t have much labeled\n",
        "training data, but unfortunately you cannot find a model trained on a similar task.\n",
        "Don’t lose all hope! First, you should of course try to gather more labeled training\n",
        "data, but if this is too hard or too expensive, you may still be able to perform unsupervised\n",
        "pretraining. It is often rather cheap to gather unlabeled training\n",
        "examples, but quite expensive to label them. If you can gather plenty of unlabeled\n",
        "training data, you can try to train the layers one by one, starting with the lowest layer\n",
        "and then going up, using an unsupervised feature detector algorithm such as Restricted\n",
        "Boltzmann Machines or autoencoders. Each layer is\n",
        "trained on the output of the previously trained layers (all layers except the one being\n",
        "trained are frozen). Once all layers have been trained this way, you can add the output\n",
        "layer for your task, and fine-tune the final network using supervised learning (i.e.,\n",
        "with the labeled training examples). At this point, you can unfreeze all the pretrained\n",
        "layers, or just some of the upper ones.\n",
        "\n",
        "This is a rather long and tedious process, but it often works well; in fact, it is this\n",
        "technique that Geoffrey Hinton and his team used in 2006 and which led to the\n",
        "revival of neural networks and the success of Deep Learning. Until 2010, unsupervised\n",
        "pretraining (typically using RBMs) was the norm for deep nets, and it was only\n",
        "after the vanishing gradients problem was alleviated that it became much more com‐mon to train DNNs purely using supervised learning. However, unsupervised pretraining\n",
        "(today typically using autoencoders rather than RBMs) is still a good option\n",
        "when you have a complex task to solve, no similar model you can reuse, and little\n",
        "labeled training data but plenty of unlabeled training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flcNh7rd3EyS"
      },
      "source": [
        "## Pretraining on an Auxiliary Task\n",
        "\n",
        "If you do not have much labeled training data, one last option is to train a first neural\n",
        "network on an auxiliary task for which you can easily obtain or generate labeled\n",
        "training data, then reuse the lower layers of that network for your actual task. The\n",
        "first neural network’s lower layers will learn feature detectors that will likely be reusable\n",
        "by the second neural network.\n",
        "\n",
        "\n",
        "For example, if you want to build a system to recognize faces, you may only have a\n",
        "few pictures of each individual—clearly not enough to train a good classifier. Gathering\n",
        "hundreds of pictures of each person would not be practical. However, you could\n",
        "gather a lot of pictures of random people on the web and train a first neural network\n",
        "to detect whether or not two different pictures feature the same person. Such a network\n",
        "would learn good feature detectors for faces, so reusing its lower layers would\n",
        "allow you to train a good face classifier using little training data.\n",
        "\n",
        "\n",
        "For natural language processing (NLP) applications, you can easily download millions\n",
        "of text documents and automatically generate labeled data from it. For example, you\n",
        "could randomly mask out some words and train a model to predict what the missing\n",
        "words are (e.g., it should predict that the missing word in the sentence “What ___\n",
        "you saying?” is probably “are” or “were”). If you can train a model to reach good performance\n",
        "on this task, then it will already know quite a lot about language, and you\n",
        "can certainly reuse it for your actual task, and fine-tune it on your labeled data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDjb2-eaXEvd"
      },
      "source": [
        "## Self-supervised learning \n",
        "Is when you automatically generate the\n",
        "labels from the data itself, then you train a model on the resulting\n",
        "“labeled” dataset using supervised learning techniques. Since this\n",
        "approach requires no human labeling whatsoever, it is best classified\n",
        "as a form of unsupervised learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNFcb09NXE2J"
      },
      "source": [
        "# Faster Optimizers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIwlwkeLXFEz"
      },
      "source": [
        "Another huge speed boost comes from\n",
        "using a faster optimizer than the regular Gradient Descent optimizer.\n",
        "\n",
        "The most popular optimizers are: \n",
        "1. Momentum optimization\n",
        "2. Nesterov Accelerated Gradient\n",
        "3. AdaGrad\n",
        "4. RMSProp\n",
        "5. Adam and Nadam optimization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9SDvlQPXFUZ"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLfsOWGgXFLq"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTHmiXmtXE5A"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuaA5X4aXEy5"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ozEsp0MXElh"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_JVrTocvljq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XF-clykvvlmi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbXo3XYBvlug"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDPgOqLCvlxU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW4jP1qkvlzq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ked0JFQpvl2H"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJEuHkMevl4i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpVNpW32vl69"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ha3z5KPvl9f"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyTyR1n5vl_x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBzl3-tlvmC3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}