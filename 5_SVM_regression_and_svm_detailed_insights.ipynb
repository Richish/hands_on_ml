{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_SVM_regression_and_svm_detailed_insights.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "P71VIoSHoFmQ",
        "t1okizKuzrx9",
        "i-_LDy5R0OmY",
        "0Knj3YIa4CNQ",
        "-Nu5amS84MWv"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNprGGDxyr7WOAux8LkK4S2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Richish/hands_on_ml/blob/master/5_SVM_regression_and_svm_detailed_insights.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0pFFfU_0X3E",
        "colab_type": "text"
      },
      "source": [
        "# SVM Regression\n",
        "Tries to fit as many instances as possible on the street while limiting margin violations(instances off the street).\n",
        "\n",
        "The width of the street is controlled by a hyperparameter\n",
        "ϵ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zlx-LoZR0Taj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import LinearSVR\n",
        "\n",
        "svm_reg = LinearSVR(epsilon=1.5)\n",
        "svm_reg.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n51PoZUK1u-e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVR\n",
        "\n",
        "svm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\n",
        "svm_poly_reg.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P71VIoSHoFmQ",
        "colab_type": "text"
      },
      "source": [
        "## hyperparameter tuning\n",
        "\n",
        "epsilon defines the width of street. However epsilon doesn't affect the predictions made by SVM regression, hence SVM Regresssion is called epsilon insensitive.\n",
        "\n",
        "'C' defines regularization in SVR class. Greates 'C' corresponds to lower regularization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbmacMLXo9Lo",
        "colab_type": "text"
      },
      "source": [
        "# Under the hood - SVC and SVR\n",
        "\n",
        "Decision function: wT x + b = w1 x1 + ⋯ + wn xn + b\n",
        "\n",
        "Classification:\n",
        "ÿ = 1 if (decision function >0) else 0.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ra1K2h3Pq3y_",
        "colab_type": "text"
      },
      "source": [
        "## decision function for 2-feature dataset.\n",
        "\n",
        "decision function that corresponds to the model is a two-dimensional plane if the dataset has two features. The decision boundary is the set of points where the decision function is equal to 0: it is the intersection of two planes, which is a straight line (represented by the thick solid line).\n",
        "\n",
        "The dashed lines represent the points where the decision function is equal to 1 or –1: they are parallel and at equal distance to the decision boundary, forming a margin around it. Training a linear SVM classifier means finding the value of w and b that make this margin as wide as possible while avoiding margin violations (hard margin) or limiting them (soft margin).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1okizKuzrx9",
        "colab_type": "text"
      },
      "source": [
        "### My intution of how to get dotted lines:\n",
        "Consider a line on the decision function plane, with z = 1 and similarly consider another line on the decision function plane, with z = -1.\n",
        "Now project these lines onto the original feature plane(x-y plane)(projected by dropping along z-axis).\n",
        "These 2 projected lines represent the dotted lines. These are the ends of the street and these happen to be parallel of the hard line in centre.\n",
        "\n",
        "Now consider the slope of decision plane wrt to feature plane. If the slope is 90 degrees, then the dotted lines(i.e, the projection of z=1 and z=-1 lines from decision function plane onto x-y plane) will be on the same hard line hence street width=0.\n",
        "If you start decreasing the slope, the width of the street starts increasing and is max when decision function is parallet to (and hence coincides with) feature plane."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-_LDy5R0OmY",
        "colab_type": "text"
      },
      "source": [
        "## Training objective\n",
        "\n",
        "Consider the slope of the decision function: it is equal to the norm of the weight vector,\n",
        "∥ w ∥. If we divide this slope by 2, the points where the decision function is equal\n",
        "to ±1 are going to be twice as far away from the decision boundary. In other words,\n",
        "dividing the slope by 2 will multiply the margin by 2.The smaller the weight vector w, the larger the margin.\n",
        "(see my intution above).\n",
        "\n",
        "So we want to minimize ∥ w ∥ to get a large margin. However, if we also want to avoid\n",
        "any margin violation (hard margin), then we need the decision function to be greater\n",
        "than 1 for all positive training instances, and lower than –1 for negative training\n",
        "instances.\n",
        "\n",
        "If we define t(i) = –1 for negative instances (if y(i) = 0) and t(i) = 1 for positive\n",
        "instances (if y(i) = 1), then we can express this constraint as t(i)(wT x(i) + b) ≥ 1 for all\n",
        "instances.\n",
        "\n",
        "###  Hard margin linear SVM classifier objective\n",
        "minimize(w, b) for 1/2(w†w)\n",
        "subject to t^(i)(w†x^i + b) ≥ 1 for i = 1, 2, ⋯,m\n",
        "\n",
        "We are minimizing 1/2(wT w), which is equal to 1/2(∥ w ∥)^2, rather than\n",
        "minimizing ∥ w ∥. Since ||w||^2 is differentiable at w=0, whereas ||w|| is not differentiable at w=0.\n",
        "\n",
        "### To get the soft margin objective:\n",
        "we need to introduce a slack variable ζ(i) ≥ 0 for each instance: ζ(i) measures how much the ith instance is allowed to violate the margin. We now have two conflicting objectives: making the slack variables as small as possible to reduce the margin violations, and making 1/2(wT w) as small as possible to increase the\n",
        "margin.\n",
        "\n",
        "This is where the C hyperparameter comes in: it allows us to define the trade‐off between these two objectives. This gives us the constrained optimization problem\n",
        "\n",
        "Soft margin linear SVM classifier objective:\n",
        "minimize(w, b, ζ), for {1/2(w†w) + C((from i=1 to m)Σ(ζ^i))};\n",
        "subject to t^(i)(w†x^i + b) ≥ 1-ζ^i, for i = 1, 2, ⋯,m\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97dkOqNW3wRA",
        "colab_type": "text"
      },
      "source": [
        "## How to solve hard and soft margin problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Knj3YIa4CNQ",
        "colab_type": "text"
      },
      "source": [
        "### Quadratic Programming\n",
        "The hard margin and soft margin problems are both convex quadratic optimization\n",
        "problems with linear constraints. Such problems are known as Quadratic Programming\n",
        "(QP) problems. Many off-the-shelf solvers are available to solve QP problems\n",
        "using a variety of techniques. \n",
        "\n",
        "So one way to train a hard margin linear SVM classifier is just to use an off-the-shelf\n",
        "QP solver by passing it the preceding parameters. The resulting vector p will contain\n",
        "the bias term b = p0 and the feature weights wi = pi for i = 1, 2, ⋯, n. Similarly, you\n",
        "can use a QP solver to solve the soft margin problem (see the exercises at the end of\n",
        "the chapter).\n",
        "However, to use the kernel trick we are going to look at a different constrained optimization\n",
        "problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Nu5amS84MWv",
        "colab_type": "text"
      },
      "source": [
        "### The Dual Problem\n",
        "Given a constrained optimization problem, known as the primal problem, it is possible\n",
        "to express a different but closely related problem, called its dual problem. The solution\n",
        "to the dual problem typically gives a lower bound to the solution of the primal\n",
        "problem, but under some conditions it can even have the same solutions as the primal\n",
        "problem. Luckily, the SVM problem happens to meet these conditions,6 so you\n",
        "can choose to solve the primal problem or the dual problem; both will have the same\n",
        "solution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6i8MBMyO4lo9",
        "colab_type": "text"
      },
      "source": [
        "### Kernelized SVM(Kernel Trick)\n",
        "Suppose you want to apply a 2nd-degree polynomial transformation to a two-dimensional training set (such as the moons training set), then train a linear SVM classifier on the transformed training set.\n",
        "\n",
        "Transformed x, ϕ(x) = ϕ[x1 , x2] = [x1^2, x1.x2, x2^2]\n",
        "\n",
        "Dot product of 2 transformed instances: ϕ(a).ϕ(b)\n",
        "= [a1^2, a1.a2, a2^2].[b1^2, b1.b2, b2^2] # this is dot product of 2 vectors.\n",
        "= (a1.b1)^2 + a1.b1.a2.b2 + (a2.b2)^2\n",
        "= (a1.b1 + a2.b2)^2\n",
        "= ([a1, a2]†.[b1, b2])^2\n",
        "= (a†.b)^2\n",
        "\n",
        "Hence in the end, we can just use dot product of 2-dimensional vectors and square that resultant scalar.\n",
        "We don't need to use polynomial features at all even though we get the effect of calculationg polynomial features.\n",
        "\n",
        "In the end we don't even need to to know about the actual ϕ to get the dot product of ϕ(a).ϕ(b). We just need to know the vectors a and b. This is the essence of kernel trick."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OLu1zJX7mbZ",
        "colab_type": "text"
      },
      "source": [
        "The function K(a, b) = (a† b)2 is called a 2nd-degree polynomial kernel. In Machine Learning, a kernel is a function capable of computing the dot product ϕ(a)T.ϕ(b) based only on the original vectors a and b, without having to compute (or even to know about) the transformation ϕ.\n",
        "\n",
        "Common kernels\n",
        "Linear: K(a, b) = a†b\n",
        "Polynomial: K(a, b) = (γa†b + r)^d\n",
        "Gaussian RBF: K(a, b) = exp(−γ||a − b||^2)\n",
        "Sigmoid: K(a, b) = tanh(γa†b + r)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJrJ_Je99Gev",
        "colab_type": "text"
      },
      "source": [
        "### Mercer’s theorem:\n",
        "According to Mercer’s theorem, if a function K(a, b) respects a few mathematical conditions\n",
        "called Mercer’s conditions (K must be continuous, symmetric in its arguments\n",
        "so K(a, b) = K(b, a), etc.), then there exists a function ϕ that maps a and b into\n",
        "another space (possibly with much higher dimensions) such that K(a, b) = ϕ(a)T ϕ(b).\n",
        "So you can use K as a kernel since you know ϕ exists, even if you don’t know what ϕ\n",
        "is. In the case of the Gaussian RBF kernel, it can be shown that ϕ actually maps each\n",
        "training instance to an infinite-dimensional space, so it’s a good thing you don’t need\n",
        "to actually perform the mapping!\n",
        "Note that some frequently used kernels (such as the Sigmoid kernel) don’t respect all\n",
        "of Mercer’s conditions, yet they generally work well in practice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y26XsqeV9GiU",
        "colab_type": "text"
      },
      "source": [
        "# Online SVMs\n",
        "\n",
        "For linear SVM classifiers, one method is to use Gradient Descent (e.g., using\n",
        "SGDClassifier) to minimize the cost function in Equation 5-13, which is derived\n",
        "from the primal problem. Unfortunately it converges much more slowly than the\n",
        "methods based on QP.\n",
        "\n",
        "Linear SVM classifier cost function:\n",
        "J(w, b) = 1/2(wTw) + C(from i=1 to m)Σmax(0, 1 − t^i(wTx^i + b))\n",
        "\n",
        "The first sum in the cost function will push the model to have a small weight vector\n",
        "w, leading to a larger margin. The second sum computes the total of all margin violations.\n",
        "An instance’s margin violation is equal to 0 if it is located off the street and on\n",
        "the correct side, or else it is proportional to the distance to the correct side of the\n",
        "street. Minimizing this term ensures that the model makes the margin violations as\n",
        "small and as few as possible"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmPGGZ2V9GlN",
        "colab_type": "text"
      },
      "source": [
        "## hinge loss:\n",
        "The function max(0, 1 – t) is called the hinge loss function (represented below). It is\n",
        "equal to 0 when t ≥ 1. Its derivative (slope) is equal to –1 if t < 1 and 0 if t > 1. It is not\n",
        "differentiable at t = 1, but just like for Lasso Regression you can still use Gradient Descent using any subderivative at t = 1 (i.e., any\n",
        "value between –1 and 0)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4jTHYXI9GoA",
        "colab_type": "text"
      },
      "source": [
        "# Large scale non-linear problems.\n",
        "These must almost always be solved with neural networks rather than SVM :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPPq-vG81v_k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}