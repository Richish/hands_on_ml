{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6_decision_trees.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMJBdcXl0lKWCfFC488rrzd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Richish/hands_on_ml/blob/master/6_decision_trees.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMKwgU-DU6BL",
        "colab_type": "text"
      },
      "source": [
        "# Training and visualizing a decision tree\n",
        "\n",
        "## Training for iris dataset classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p28OuP7mUlfZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "d4a5f934-6620-44aa-e20c-b4d23c2c6225"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "iris = load_iris()\n",
        "X= iris.data[:,2:] # petal length and width\n",
        "y = iris.target\n",
        "\n",
        "tree_clf = DecisionTreeClassifier(max_depth=2)\n",
        "tree_clf.fit(X, y)\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
              "                       max_depth=2, max_features=None, max_leaf_nodes=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                       random_state=None, splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLwO20NwU7v5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# visualizing\n",
        "from sklearn.tree import export_graphviz\n",
        "export_graphviz(\n",
        "    decision_tree=tree_clf, out_file=\"tree_visual.dot\", feature_names=iris.feature_names[2:], class_names=iris.target_names, rounded=True, filled=True\n",
        ")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWKk3QbjU7y8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e2607afb-e8d5-4fc5-a94d-2de6ab231d9f"
      },
      "source": [
        "!ls\n",
        "#!dot -Tpng tree_visual.dot -o iris_tree.png"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data  somefile.png  tree_visual.dot\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-gfR2uAU71-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!dot -Tpng tree_visual.dot -o iris_tree.png\n",
        "import pydot\n",
        "\n",
        "(graph,) = pydot.graph_from_dot_file('tree_visual.dot')\n",
        "graph.write_png('tree_visual.png')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yy1Yl_7MU6Mo",
        "colab_type": "text"
      },
      "source": [
        "## Note - Training for decision tree doesn't require data scaling or centering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wC3vwSeDU6Pi",
        "colab_type": "text"
      },
      "source": [
        "## Attributes of a node:\n",
        "\n",
        "\n",
        "1.   Samples: How many sample does the current node have/process.\n",
        "2.   Values: List of fixed length(=no. of classes). ith item in list = how many samples in current node belong to ith class.\n",
        "3.   Gini (only for leaf node): Measure of 'impurity' of a node. Lower(=0) means more pure. If current leaf node has samples that all belong to a single class then that node is called pure. More distributed the samples more impure the node becomes.\n",
        "\n",
        "If a node has values = [10,20,30], impurity score = 1 - (10/60)^2 - (20/60)^2 -(30/60)^2\n",
        "\n",
        "If a node has values = [60,0,0] or [0,60,0] or [0,0,60], then purity = 1 - (60/60)^2 - (0/60)^2 - (0/60)^2 = 0\n",
        " \n",
        "Gini impurity of ith node = G(i) = 1 − (k= 1 to n)Σ (p(i, k))^2\n",
        "Here p(i,k) = The ratio of class k instances among the training instances in the ith node\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXIY_WrTU6Sj",
        "colab_type": "text"
      },
      "source": [
        "## Sklearn only has 2 children of their nodes.\n",
        "Scikit-Learn uses the CART algorithm, which produces only binary\n",
        "trees: nonleaf nodes always have two children (i.e., questions only\n",
        "have yes/no answers). However, other algorithms such as ID3 can\n",
        "produce Decision Trees with nodes that have more than two children."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3mFmkMAU6Va",
        "colab_type": "text"
      },
      "source": [
        "# Estimating Class Probabilities\n",
        "\n",
        "A Decision Tree can also estimate the probability that an instance belongs to a particular\n",
        "class k: first it traverses the tree to find the leaf node for this instance, and then it\n",
        "returns the ratio of training instances of class k in this node. For example, suppose\n",
        "you have found a flower whose petals are 5 cm long and 1.5 cm wide. The corresponding\n",
        "leaf node is the depth-2 left node, so the Decision Tree should output the\n",
        "following probabilities: 0% for Iris-Setosa (0/54), 90.7% for Iris-Versicolor (49/54),\n",
        "and 9.3% for Iris-Virginica (5/54). And of course if you ask it to predict the class, it\n",
        "should output Iris-Versicolor (class 1) since it has the highest probability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhkXH3nqU74z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dabc1024-978a-46b1-ea65-707bfa421f28"
      },
      "source": [
        "tree_clf.predict_proba([[5, 1.5]]) # predict probabilities"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.90740741, 0.09259259]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqOge-NvU77d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3ba0b98a-8a29-4d22-cba0-a4f04d7d9638"
      },
      "source": [
        "tree_clf.predict([[5, 1.5]]) # classification only"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLGfyi5KU6YZ",
        "colab_type": "text"
      },
      "source": [
        "## Notice that the estimated probabilities would be exactly same for all the samples that end up inside same leaf node."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FafMtzgCU6b-",
        "colab_type": "text"
      },
      "source": [
        "#CART(Classification and Regression Tree) Training Algorithm\n",
        "CART- Classification and Regression Tree\n",
        "\n",
        "First splits the training set in two subsets using a single feature k and a threshold t(subscript k) (e.g., “petal length ≤ 2.45 cm”). How does it choose k and t(subscript k)? It searches for the\n",
        "pair (k, t(subscript k)) that produces the purest subsets (weighted by their size). The cost function that the algorithm tries to minimize is given as:\n",
        "\n",
        "J(k, tk) = (m(left)/m)*G(left) + (m(right)/m)*G(right)\n",
        "\n",
        "Here, \n",
        "\n",
        "m = total no. of samples in this node.\n",
        "m(left/right) = no. of samples that go to left/right child.\n",
        "G(left/right) = impurity of left/right child node.\n",
        "\n",
        "Once it has successfully split the training set in two, it splits the subsets using the same logic, then the sub-subsets and so on, recursively. It stops recursing once it reaches the maximum depth (defined by the max_depth hyperparameter), or if it cannot find a split that will reduce impurity. A few other hyperparameters control additional stopping conditions (min_samples_split, min_sam\n",
        "ples_leaf, min_weight_fraction_leaf, and max_leaf_nodes)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5hr76FmU6e1",
        "colab_type": "text"
      },
      "source": [
        "## Cart is a greedy algorithm\n",
        "CART algorithm is a greedy algorithm: it greedily\n",
        "searches for an optimum split at the top level, then repeats the\n",
        "process at each level. It does not check whether or not the split will\n",
        "lead to the lowest possible impurity several levels down. \n",
        "A greedy\n",
        "algorithm often produces a reasonably good solution, but it is not\n",
        "guaranteed to be the optimal solution.\n",
        "\n",
        "1. Finding the optimal tree is known to be an NPComplete\n",
        "problem.\n",
        "2. It requires O(exp(m)) time, making the problem\n",
        "intractable even for fairly small training sets. This is why we\n",
        "must settle for a “reasonably good” solution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxEpS9PEU6hl",
        "colab_type": "text"
      },
      "source": [
        "## Computational complexity of CART\n",
        "\n",
        "1. Prediction(not training): Decision Trees are generally approximately balanced, so traversing the Decision Tree requires going through roughly O(log2(m)) nodes. Since each node only requires checking the value of one feature, the overall prediction complexity is just O(log2(m)), independent of the number of features. So predictions are very fast, even when dealing with large training sets.\n",
        "\n",
        "2. Training: The training algorithm compares all features (or less if max_features is set)\n",
        "on all samples at each node. This results in a training complexity of O(n × m log(m)). For small training sets (less than a few thousand instances), Scikit-Learn can speed up training by presorting the data (set presort=True), but this slows down training considerably for larger training sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bpVpImXU6kV",
        "colab_type": "text"
      },
      "source": [
        "# Entropy instead of Gini impurity can be used\n",
        "\n",
        "By default, the Gini impurity measure is used, but you can select the entropy impurity measure instead by setting the criterion hyperparameter to \"entropy\".\n",
        "\n",
        "A set’s entropy is zero when it contains instances of only one class. \n",
        "\n",
        "Entropy, H{i} = − (for (k = 1 to n) and p{i,k}!=0)Σ(p{i,k}).log(p{i,k})\n",
        "\n",
        "note- values in {} represent subscript here.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxE-phcw3ZeD",
        "colab_type": "text"
      },
      "source": [
        "## To use Entropy or Impurity ?\n",
        "\n",
        "Most of the time it does not make a big difference: they lead to similar trees. \n",
        "1. Gini impurity is slightly faster to compute, so it is a good default. \n",
        "2. Entropy tends to produce slightly more balanced trees. Gini impurity tends to\n",
        "isolate the most frequent class in its own branch of the tree(But this happens only rarely so impurity can be the default)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-ycPPQ43Zg0",
        "colab_type": "text"
      },
      "source": [
        "# Regularization hyperparameters\n",
        "\n",
        "Decision Trees make very few assumptions about the training data (as opposed to linear\n",
        "models, which obviously assume that the data is linear, for example). If left\n",
        "unconstrained, the tree structure will adapt itself to the training data, fitting it very\n",
        "closely, and most likely overfitting it. Such a model is often called a nonparametric\n",
        "model, not because it does not have any parameters (it often has a lot) but because the\n",
        "number of parameters is not determined prior to training, so the model structure is\n",
        "free to stick closely to the data. In contrast, a parametric model such as a linear model\n",
        "has a predetermined number of parameters, so its degree of freedom is limited,\n",
        "reducing the risk of overfitting (but increasing the risk of underfitting).\n",
        "\n",
        "In Scikit-Learn, regularization is done by\n",
        "max_depth hyperparameter (the default value is None, which means unlimited).\n",
        "Reducing max_depth will regularize the model and thus reduce the risk of overfitting.\n",
        "\n",
        "Other parameters: \n",
        "1. min_samples_split (the minimum number of samples\n",
        "a node must have before it can be split)\n",
        "2. min_samples_leaf (the minimum number\n",
        "of samples a leaf node must have)\n",
        "3. min_weight_fraction_leaf (same as\n",
        "min_samples_leaf but expressed as a fraction of the total number of weighted instances)\n",
        "4. max_leaf_nodes (maximum number of leaf nodes)\n",
        "5. max_features (maximum number of features that are evaluated for splitting at each node).\n",
        "\n",
        "Increasing\n",
        "min_* hyperparameters or reducing max_* hyperparameters will regularize the\n",
        "model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGmknX4m3Zjk",
        "colab_type": "text"
      },
      "source": [
        "## Other not so common way:\n",
        "First training the Decision Tree without\n",
        "restrictions, then pruning (deleting) unnecessary nodes. \n",
        "A node whose children are all leaf nodes is considered unnecessary if the\n",
        "purity improvement it provides is not statistically significant. \n",
        "Standard statistical tests, such as the χ2 test, are used to estimate the\n",
        "probability that the improvement is purely the result of chance. If this probability, called the pvalue,\n",
        "is higher than a given threshold (typically 5%, controlled by\n",
        "a hyperparameter), then the node is considered unnecessary and its\n",
        "children are deleted. The pruning continues until all unnecessary\n",
        "nodes have been pruned."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_ZTU45W3ZmU",
        "colab_type": "text"
      },
      "source": [
        "# Regression\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOdIbOFw3IgP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "tree_reg = DecisionTreeRegressor(max_depth=2)\n",
        "tree_reg.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKny9Cv43ZpJ",
        "colab_type": "text"
      },
      "source": [
        "This tree looks very similar to the classification tree you built earlier. The main differencesa are:\n",
        "1. Instead of predicting a class in each node, it predicts a value(and not a values list).\n",
        "2. This prediction is simply the average target value of training\n",
        "instances associated to this node. \n",
        "3. This prediction results in a Mean Squared Error associated to that node, this is MSE for the samples in that node.\n",
        "\n",
        "The algorithm splits sample in each node(to be passed on to child nodes) in a way that makes most training instances as close as possible to that predicted value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOVa8C9S3ZbC",
        "colab_type": "text"
      },
      "source": [
        "## CART cost function for regression\n",
        "Uses MSE instead of gini to calculate cost. Otherwise, it is same as that for classification.\n",
        "\n",
        "{} represent subscript in following eqns.\n",
        "\n",
        "J(k, t{k}) = (m{left}/m)*MSE(left) + (m{right}/m)*MSE{right}\n",
        "\n",
        "here, MSE{node} = (for i in all nodes) Σ ( ÿ{node} - y^(i}) )^2\n",
        "and, ÿ{node} = (1/m{node}) * (for i in node) Σ y^(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evvZ7gG7FfQ2",
        "colab_type": "text"
      },
      "source": [
        "# Instability\n",
        "## Decision Trees love orthogonal decision boundaries (all splits are perpendicular to an axis).\n",
        "\n",
        "So even in case original dataset is easily linearly seperable, just rotating the axis of features(say 45 degree)- results in very complex decision tree since decision boundaries are orthogonal and rotated fatures will require multiple of orthogonal boundaries to work.\n",
        "\n",
        "Hence training set that may be easily linearly seperable may require uneccesarily complex models that may not perform well.\n",
        "\n",
        "One way to limit this problem is to use PCA, which often results in a better orientation of the training data.\n",
        "\n",
        "More generally, the main issue with Decision Trees is that they are \n",
        "## very sensitive to small variations in the training data. \n",
        "For example, if you just remove the widest Iris-\n",
        "Versicolor from the iris training set (the one with petals 4.8 cm long and 1.8 cm wide) and train a new Decision Tree, you may get the model that  looks very different from the previous Decision Tree.\n",
        "\n",
        "Also, since the training algorithm used by Scikit-Learn is stochastic you may\n",
        "get very different models even on the same training data (unless you set the\n",
        "random_state hyperparameter).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqilwpGYHIuF",
        "colab_type": "text"
      },
      "source": [
        "### Random forests are a solution to instability.\n",
        "Random Forests can limit this instability by averaging predictions over many trees."
      ]
    }
  ]
}