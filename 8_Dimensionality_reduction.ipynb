{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "8_Dimensionality reduction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM7NQb6Ken+1IUrU1RGWi7o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Richish/hands_on_ml/blob/master/8_Dimensionality_reduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1nakEDyj3Hp",
        "colab_type": "text"
      },
      "source": [
        "# Curse of dimensionality\n",
        "\n",
        "Many Machine Learning problems involve thousands or even millions of features for each training instance. \n",
        "Not only does this make training extremely slow, it can also make it much harder to find a good solution. This problem is often referred to as the curse of dimensionality.\n",
        "\n",
        "It turns out that many things behave very differently in high-dimensional space. For example, if you pick a random point in a unit square (a 1 × 1 square), it will have only about a 0.4% chance of being located less than 0.001 from a border (in other words, it is very unlikely that a random point will be “extreme” along any dimension). But in a 10,000-dimensional unit hypercube (a 1 × 1 × ⋯ × 1 cube, with ten thousand 1s), this probability is greater than 99.999999%. Most points in a high-dimensional hypercube are very close to the border.\n",
        "\n",
        "More importantly:\n",
        "If you pick two points randomly in a unit square, the distance between these two points will be, on average, roughly 0.52. If you pick two random points in a unit 3D cube, the average distance will be roughly 0.66. But what about two points picked randomly in a 1,000,000-dimensional hypercube? the average distance, will be about 408.25 (roughly sqrt(1,000,000/6!)) . This fact implies that high- dimensional datasets are at risk of being very sparse: most training instances are likely to be far away from each other. Of course, this also means that a new instance will likely be far away from any training instance, making predictions much less reliable than in lower dimensions, since they will be based on much larger extrapolations. In short, the more dimensions the training set has, the greater the risk of overfitting it.\n",
        "\n",
        "With just 100 features (much less than in the MNIST problem), you would need more training instances than atoms in the observable universe in order for training instances to be within 0.1 of each other on average, assuming they were spread out uniformly across all dimensions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Zc3V4sfj3Kt",
        "colab_type": "text"
      },
      "source": [
        "Reducing dimensionality does lose some information (just like compressing an image to JPEG can degrade its quality), so even though it will speed up training, it may also make your system perform slightly worse. It also makes your pipelines a bit more complex and thus harder to maintain. So you should first try to train your system with the original data before considering using dimensionality reduction if training is too slow. In some cases, however, reducing the dimensionality of the training data may filter out some noise and unnecessary details and thus result in higher performance (but in general it won’t; it will just speed up training).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwpoulsTj3N2",
        "colab_type": "text"
      },
      "source": [
        "Apart from speeding up training, dimensionality reduction is also extremely useful for data visualization (or DataViz). Reducing the number of dimensions down to two (or three) makes it possible to plot a condensed view of a high-dimensional training set on a graph and often gain some important insights by visually detecting patterns, such as clusters. Moreover, DataViz is essential to communicate your conclusions to people who are not data scientists, in particular decision makers who will use your results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNdsb_dvj3Qy",
        "colab_type": "text"
      },
      "source": [
        "# 2 main approaches for dimensionality reduction:\n",
        "\n",
        "\n",
        "1.   Projection.\n",
        "2.   Manifold learning.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBtPzlf-j3Tm",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEs4Je-Yj3W-",
        "colab_type": "text"
      },
      "source": [
        "## Projection\n",
        "\n",
        "In most real-world problems, training instances are not spread out uniformly across all dimensions. Many features are almost constant, while others are highly correlated (as for MNIST). As a result, all training instances actually lie within (or close to) a much lower-dimensional subspace of the high-dimensional space. \n",
        "\n",
        "Example a 2-d subspace(plane) inside a 3-d space. obtained by projecting all points in 3-d to a 2-d space (may or maynot be parallel to any axes).\n",
        "\n",
        "### Where projection cannot be used:\n",
        " In many cases the subspace may twist and turn, such as in the famous Swiss roll toy data‐set.\n",
        "Simply projecting onto a plane (e.g., by dropping x3) would squash different layers of the Swiss roll together. However, what you really want is to unroll the Swiss roll to obtain the 2D dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YC6hMeCoj3iE",
        "colab_type": "text"
      },
      "source": [
        "## Manifold learning\n",
        "\n",
        "The Swiss roll is an example of a 2D manifold. Put simply, a 2D manifold is a 2D shape that can be bent and twisted in a higher-dimensional space. More generally, a d-dimensional manifold is a part of an n-dimensional space (where d < n) that locally resembles a d-dimensional hyperplane. In the case of the Swiss roll, d = 2 and n = 3: it locally resembles a 2D plane, but it is rolled in the third dimension.\n",
        "\n",
        "Many dimensionality reduction algorithms work by modeling the manifold on which the training instances lie; this is called Manifold Learning. It relies on the manifold assumption, also called the manifold hypothesis, which holds that most real-world high-dimensional datasets lie close to a much lower-dimensional manifold. This assumption is very often empirically observed.\n",
        "\n",
        "Once again, think about the MNIST dataset: all handwritten digit images have some similarities. They are made of connected lines, the borders are white, they are more or less centered, and so on. If you randomly generated images, only a ridiculously tiny fraction of them would look like handwritten digits. In other words, the degrees of freedom available to you if you try to create a digit image are dramatically lower than the degrees of freedom you would have if you were allowed to generate any image you wanted. These constraints tend to squeeze the dataset into a lower- dimensional manifold\n",
        "\n",
        "The manifold assumption is often accompanied by another implicit assumption: that the task at hand (e.g., classification or regression) will be simpler if expressed in the lower-dimensional space of the manifold. However, this assumption does not always hold. \n",
        "\n",
        "In short, if you reduce the dimensionality of your training set before training a model, it will usually speed up training, but it may not always lead to a better or sim‐ pler solution; it all depends on the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvL2tj5ELDme",
        "colab_type": "text"
      },
      "source": [
        "# Dimensionality reduction algorithms:\n",
        "1. PCA\n",
        "2. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6geir7rLDpa",
        "colab_type": "text"
      },
      "source": [
        "## PCA\n",
        "Principal Component Analysis (PCA) is the most popular dimensionality reduction algorithm. First it identifies the hyperplane that lies closest to the data, and then it projects the data onto it.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIfpEhscNSNI",
        "colab_type": "text"
      },
      "source": [
        "### Preserving the variance:\n",
        "Before you can project the training set onto a lower-dimensional hyperplane, you first need to choose the right hyperplane.\n",
        "\n",
        "Choose a hyperplane so that there is minimal reduction in variance (in data).\n",
        "In other words, select the axis that preserves the maximum amount of var‐ iance, as it will most likely lose less information than the other projections. Another way to justify this choice is that it is the axis that minimizes the mean squared dis‐ tance between the original dataset and its projection onto that axis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40BrHPe0LDsg",
        "colab_type": "text"
      },
      "source": [
        "### Principal Components\n",
        "\n",
        "PCA identifies the axis that accounts for the largest amount of variance in the training set. For 2-d data, It also finds a second axis, **orthogonal to the first one, that accounts for the largest amount of remaining variance**. If it were a higher-dimensional data‐ set, PCA would also find a third axis, orthogonal to both previous axes, and a fourth, a fifth, and so on—as many axes as the number of dimensions in the dataset.\n",
        "\n",
        "The unit vector that defines the ith axis is called the ith principal component (PC). \n",
        "\n",
        "The direction of the principal components is not stable: if you perturb the training set slightly and run PCA again, some of the new PCs may point in the opposite direction of the original PCs. However, they will generally still lie on the same axes. In some cases, a pair of PCs may even rotate or swap, but the plane they define will generally remain the same.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esoBCxDWLDvk",
        "colab_type": "text"
      },
      "source": [
        "### How to find principal components of a training set (Singular value decomposition)\n",
        "\n",
        "There is a standard matrix factorization technique called Singular Value Decomposition (SVD) that can decompose the training set matrix X into the matrix multiplication of three matrices U Σ VT, where V contains all the principal components that we are looking for.\n",
        "\n",
        "PCA assumes that the dataset is centered around the origin. As we will see, Scikit-Learn’s PCA classes take care of centering the data for you. However, if you implement PCA yourself (say using numpy), or if you use other libraries, don’t forget to center the data first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMmYyKUjPcwN",
        "colab_type": "text"
      },
      "source": [
        "#### svd using numpy only"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvsLAa56PPBv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "220e671c-efde-4134-98c8-a9d24cc0e4ea"
      },
      "source": [
        "# Numpy provides a svd() method.\n",
        "import numpy as np\n",
        "X_centered = X - X.mean(axis=0)\n",
        "U, s, Vt = np.linalg.svd(X_centered)\n",
        "c1 = Vt.T[:, 0]\n",
        "c2 = Vt.T[:, 1]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-6eef83b7173d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Numpy provides a svd() method.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX_centered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_centered\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZtZu3E1LDyv",
        "colab_type": "text"
      },
      "source": [
        "### Projecting down to d dimensions(from n dimensions):\n",
        "\n",
        "Once you have identified all the principal components, you can reduce the dimensionality of the dataset down to d dimensions by projecting it onto the hyperplane defined by the first d principal components. Selecting this hyperplane ensures that the projection will preserve as much variance as possible. For example,the 3D dataset is projected down to the 2D plane defined by the first two principal components, preserving a large part of the dataset’s variance. As a result, the 2D projection looks very much like the original 3D dataset.\n",
        "\n",
        "To project the training set onto the hyperplane, you can simply compute the matrix multiplication of the training set matrix X by the matrix Wd, defined as the matrix containing the first d principal components (i.e., the matrix composed of the first d columns of V). Projecting the training set down to d dimensions, Xd‐proj = X.Wd\n",
        "\n",
        "The Python code in next cell projects the training set onto the plane defined by the first two principal components. You now know how to reduce the dimensionality of any dataset down to any number of dimensions, while preserving as much variance as possible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBK7OznPS8zJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "a4317979-95f2-45b5-cb7a-af22b29d9ed1"
      },
      "source": [
        "W2 = Vt.T[:, :2] \n",
        "X2D = X_centered.dot(W2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-46ccd1e2941d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mW2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX2D\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_centered\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Vt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luGOj_PwLD1X",
        "colab_type": "text"
      },
      "source": [
        "### PCA with sk-learn\n",
        "\n",
        "Scikit-Learn’s PCA class implements PCA using SVD decomposition just like we did before. The code in next cell applies PCA to reduce the dimensionality of the dataset down to two dimensions (note that it automatically takes care of centering the data):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXMdEO5qTaRS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "dbef35b1-d2c1-4fef-8b84-e353160affe0"
      },
      "source": [
        "from sklearn.decomposition import PCA \n",
        "pca = PCA(n_components = 2)\n",
        "X2D = pca.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-d06b3461dff6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX2D\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvgA5XVCLD4w",
        "colab_type": "text"
      },
      "source": [
        "After fitting the PCA transformer to the dataset, you can access the principal components using the **components_** variable (note that it contains the PCs as horizontal vectors, so, for example, the first principal component is equal to pca.components_.T[:, 0])."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLgUf_lUWlXY",
        "colab_type": "text"
      },
      "source": [
        "## Explained Variance Ratio\n",
        "Very useful piece of information is the explained variance ratio of each principal component, available via the **explained_variance_ratio_** variable. It **indicates the proportion of the dataset’s variance that lies along the axis of each principal component**. \n",
        "For example, if the explained variance ratios of the first two components of the 3D dataset:\n",
        "pca.explained_variance_ratio_ is array([0.84248607, 0.14631839])\n",
        "\n",
        "This tells you that 84.2% of the dataset’s variance lies along the first axis, and 14.6% lies along the second axis. This leaves less than 1.2% for the third axis, so it is reason‐ able to assume that it probably carries little information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaYtv5URXCP9",
        "colab_type": "text"
      },
      "source": [
        "## Choosing the Right Number of Dimensions\n",
        "Instead of arbitrarily choosing the number of dimensions to reduce down to, it is generally preferable to choose the number of dimensions that add up to a sufficiently large portion of the variance (e.g., 95%). Unless, you are reducing dimensionality for data visualization—in that case you will generally want to reduce the dimensionality down to 2 or 3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWBln528WiMl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "67c7a389-a34c-4ebd-dea5-fd44f561c0d1"
      },
      "source": [
        "# The following code computes PCA without reducing dimensionality, \n",
        "# then computes the minimum number of dimensions required to preserve 95% of the training set’s variance:\n",
        "pca = PCA()\n",
        "pca.fit(X_train)\n",
        "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
        "d = np.argmax(cumsum >= 0.95) + 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-4a4707a33c79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# then computes the minimum number of dimensions required to preserve 95% of the training set’s variance:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mcumsum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplained_variance_ratio_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcumsum\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.95\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6Oc76k6XySW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "fef67ca3-9b81-48ca-835b-0c65ecf96034"
      },
      "source": [
        "# You could then set n_components=d and run PCA again. \n",
        "# However, there is a much better option: \n",
        "# instead of specifying the number of principal components you want to preserve, \n",
        "# you can set n_components to be a float between 0.0 and 1.0, indicating the ratio of variance you wish to preserve:\n",
        "\n",
        "pca = PCA(n_components=0.95)\n",
        "X_reduced = pca.fit_transform(X_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-be9fe95127cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mX_reduced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHvoUhjOXCE7",
        "colab_type": "text"
      },
      "source": [
        "Yet another option is to plot the explained variance as a function of the number of dimensions (simply plot cumsum). There will usually be an elbow in the curve, where the explained variance stops growing fast. You can think of this as the intrinsic dimensionality of the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsAazakKYfyf",
        "colab_type": "text"
      },
      "source": [
        "## PCA for compression\n",
        "\n",
        "After dimensionality reduction, the training set takes up much less space. For example, try applying PCA to the MNIST dataset while preserving 95% of its variance. You should find that each instance will have just over 150 features, instead of the original 784 features. So while most of the variance is preserved, the dataset is now less than 20% of its original size! This is a reasonable compression ratio, and you can see how this can speed up a classification algorithm (such as an SVM classifier) tremendously.\n",
        "\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4SnY609bEOn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3be09592-9236-4327-ecad-cd2bbdca443b"
      },
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "mnist=fetch_openml(name=\"mnist_784\", version=1)\n",
        "mnist.keys()\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['data', 'target', 'frame', 'feature_names', 'target_names', 'DESCR', 'details', 'categories', 'url'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvEhZekBcTAQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b10cfec3-4ab0-4359-f562-743ccf30a7bf"
      },
      "source": [
        "# without PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "mnist['data'].shape\n",
        "X,y=mnist['data'], mnist['target']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=60_000)\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 784), (10000, 784), (60000,), (10000,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDTNeHSYesqT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "5a9f9cfa-441f-49c7-9b7f-62332c6bc8bc"
      },
      "source": [
        "# train on whole data - without pca\n",
        "from sklearn.svm import SVC\n",
        "svc = SVC()\n",
        "svc.fit(X=X_train, y=y_train)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
              "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
              "    tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSkZuwLHcgj3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8fc1d4a7-25d4-4258-e014-802a68efc777"
      },
      "source": [
        "# applying pca\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=0.95)\n",
        "X_reduced = pca.fit_transform(X)\n",
        "X_reduced.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(70000, 154)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoCrEdnhe__O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, train_size=60_000)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HZ8sxFpdwrN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "baaf8e19-aba5-4729-b073-b02cb8babb6c"
      },
      "source": [
        "# train using pca\n",
        "from sklearn.svm import SVC\n",
        "svc = SVC()\n",
        "svc.fit(X=X_train, y=y_train)\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
              "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
              "    tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hO91vXFdwt5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "67c5bb51-df5a-47a1-b984-c4fb8f4aed6d"
      },
      "source": [
        "y_pred=svc.predict(X_test)\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_pred=y_pred, y_true=y_test)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9844"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HyiwyLwdw2w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-NjTXmcYgQ5",
        "colab_type": "text"
      },
      "source": [
        "### Decompression:\n",
        "It is also possible to decompress the reduced dataset back to 784 dimensions by applying the inverse transformation of the PCA projection. Of course this won’t give you back the original data, since the projection lost a bit of information (within the 5% variance that was dropped), but it will likely be quite close to the original data. The mean squared distance between the original data and the reconstructed data (compressed and then decompressed) is called the reconstruction error. For example, the following code compresses the MNIST dataset down to 154 dimensions, then uses the inverse_transform() method to decompress it back to 784 dimensions. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9Ibswa5a7ol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca = PCA(n_components = 154)\n",
        "X_reduced = pca.fit_transform(X_train)\n",
        "X_recovered = pca.inverse_transform(X_reduced)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtiL8LHJYgVC",
        "colab_type": "text"
      },
      "source": [
        "Xrecovered = X{d‐proj}.W{dT}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_ZYHpXMYfns",
        "colab_type": "text"
      },
      "source": [
        "## Randomized PCA.\n",
        "\n",
        "Time complexity of PCA is O(m × n^2) + O(n^3) for the full SVD approach.\n",
        "\n",
        "This time complexity can be reduced to O(m × d^2) + O(d^3) by using randomized PCA.\n",
        "\n",
        "If you set the svd_solver hyperparameter to \"randomized\", Scikit-Learn uses a stochastic algorithm called Randomized PCA that quickly finds an approximation of the first d principal components.\n",
        "\n",
        "It is dramatically faster than full SVD when d is much smaller than n.\n",
        "\n",
        "By default, svd_solver is actually set to \"auto\": Scikit-Learn automatically uses the randomized PCA algorithm if m or n is greater than 500 and d is less than 80% of m or n, or else it uses the full SVD approach. If you want to force Scikit-Learn to use full SVD, you can set the svd_solver hyperparameter to \"full\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjsGoe7L91N0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnd_pca = PCA(n_components=154, svd_solver=\"randomized\")\n",
        "X_reduced = rnd_pca.fit_transform(X_train)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_6_UQI3Qeth",
        "colab_type": "text"
      },
      "source": [
        "## Incremental PCA\n",
        "\n",
        "One problem with the normal implementations of PCA is that they require the whole training set to fit in memory in order for the algorithm to run. Fortunately, Incremental PCA (IPCA) algorithms have been developed: you can split the training set into mini-batches and feed an I-PCA algorithm one mini-batch at a time. This is useful for large training sets, and also to apply PCA online.\n",
        "\n",
        "The following code splits the MNIST dataset into 100 mini-batches (using NumPy’s array_split() function) and feeds them to Scikit-Learn’s IncrementalPCA class to reduce the dimensionality of the MNIST dataset down to 154 dimensions (just like before). Note that you must call the partial_fit() method with each mini-batch rather than the fit() method with the whole training set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6vCcKPCQfTB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "81d40cc0-98e7-418c-fe1b-44232801bbbf"
      },
      "source": [
        "from sklearn.decomposition import IncrementalPCA\n",
        "import numpy as np\n",
        "n_batches=100\n",
        "inc_pca = IncrementalPCA(n_components=154)\n",
        "for X_batch in np.array_split(ary=X_train, indices_or_sections=n_batches):\n",
        "    inc_pca.fit(X=X_batch)\n",
        "X_reduced = inc_pca.transform(X_train)\n",
        "X_reduced.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 154)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOyofDVmQhmV",
        "colab_type": "text"
      },
      "source": [
        "### Using NumPy’s memmap\n",
        "\n",
        "Alternatively, you can use NumPy’s memmap class, which allows you to manipulate a large array stored in a binary file on disk as if it were entirely in memory; the class loads only the data it needs in memory, when it needs it. \n",
        "\n",
        "Since the IncrementalPCA class uses only a small part of the array at any given time, the memory usage remains under control. This makes it possible to call the usual fit() method, as you can see in the following code:\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlMpWfNKQfbM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "73bd1485-e1e8-4cab-815c-80af1f3239e9"
      },
      "source": [
        "X_mm = np.memmap(filename, dtype=\"float32\", mode=\"readonly\", shape=(m, n))\n",
        "batch_size = m // n_batches\n",
        "inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)\n",
        "inc_pca.fit(X_mm)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-af0c1de52db5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"readonly\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mn_batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minc_pca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIncrementalPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m154\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minc_pca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'filename' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uX9dls_eQeMr",
        "colab_type": "text"
      },
      "source": [
        "## Kernel PCA\n",
        "\n",
        "The kernel trick(same as in SVC) is a mathematical technique that implicitly maps instances into a very high-dimensional space (called the feature space), enabling nonlinear classification and regression with Support Vector Machines. Recall that a linear decision boundary in the high-dimensional feature space corresponds to a complex nonlinear decision boundary in the original space.\n",
        "\n",
        "It turns out that the same trick can be applied to PCA, making it possible to perform complex nonlinear projections for dimensionality reduction. This is called Kernel PCA (kPCA).\n",
        "\n",
        "It is often good at preserving clusters of instances after projection, or sometimes even unrolling datasets that lie close to a twisted manifold.\n",
        "For example, the following code uses Scikit-Learn’s KernelPCA class to perform kPCA with an RBF kernel:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNL3wSnMWjIL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import KernelPCA\n",
        "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.04)\n",
        "X_reduced = rbf_pca.fit_transform(X)\n",
        "X_reduced.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51XQTCNBXJ49",
        "colab_type": "text"
      },
      "source": [
        "### Selecting a kernel and hyperparameter tuning\n",
        "\n",
        "As kPCA is an unsupervised learning algorithm, there is no obvious performance measure to help you select the best kernel and hyperparameter values. However, dimensionality reduction is often a preparation step for a supervised learning task (e.g., classification), so you can simply use grid search to select the kernel and hyperparameters that lead to the best performance on that task.\n",
        "\n",
        "Example: the following code creates a two-step pipeline, first reducing dimensionality to two dimensions using kPCA, then applying Logistic Regression for classification. Then it uses Grid SearchCV to find the best kernel and gamma value for kPCA in order to get the best classification accuracy at the end of the pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZT1oOD_4Yx0C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUcVVzctWJsy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import KernelPCA\n",
        "import numpy as np\n",
        "\n",
        "clf = Pipeline([\n",
        "                (\"kpca\", KernelPCA(n_components=2)),\n",
        "                (\"log_svc\", LogisticRegression())\n",
        "])\n",
        "\n",
        "param_grid = [{\n",
        "    \"kpca__gamma\": np.linspace(0.03, 0.05, 10),\n",
        "    \"kpca__kernel\": [\"rbf\", \"sigmoid\"]\n",
        "}]\n",
        "\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=3)\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "print(grid_search.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6Vu7l1DZbWE",
        "colab_type": "text"
      },
      "source": [
        "Another approach, this time entirely unsupervised, is to select the kernel and hyper‐ parameters that yield the lowest reconstruction error. \n",
        "\n",
        "However, reconstruction is not as easy as with linear PCA. Becuase, the reconstructed point would lie in feature space, not in the original space. Since the feature space is infinite-dimensional, we cannot compute the reconstructed point, and therefore we cannot compute the true reconstruction error. Fortunately, it is possible to find a point in the original space that would map close to the reconstructed point. This is called the reconstruction pre-image. Once you have this pre-image, you can measure its squared distance to the original instance. You can then select the kernel and hyperparameters that minimize this reconstruction pre-image error.\n",
        "\n",
        "It is done as follows in sklearn:\n",
        "Notice: set fit_inverse_transform=True. \n",
        "By default, fit_inverse_transform=False and KernelPCA has no inverse_transform() method. This method only gets created when you set fit_inverse_transform=True."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fA-hllqFaMJL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.0433, \n",
        "                    fit_inverse_transform=True)\n",
        "X_reduced = rbf_pca.fit_transform(X)\n",
        "X_preimage = rbf_pca.inverse_transform(X_reduced)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4G_u51PbbLa3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compute the reconstruction pre-image error:\n",
        "from sklearn.metrics import mean_squared_error\n",
        "mean_squared_error(X, X_preimage)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3zCGBwXZbg3",
        "colab_type": "text"
      },
      "source": [
        "# LLE (Local linear embedding) - one of manifold learning technoques.\n",
        " \n",
        "It is a very powerful nonlinear dimensionality reduction (NLDR) technique. It is a Manifold Learning technique that does not rely on projections.\n",
        "\n",
        "LLE works by first measur‐ ing how each training instance linearly relates to its closest neighbors (c.n.), and then looking for a low-dimensional representation of the training set where these local relationships are best preserved.\n",
        "This makes it particularly good at unrolling twisted manifolds, especially when there is not too much noise.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gYjDFffciUz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.manifold import LocallyLinearEmbedding\n",
        "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)\n",
        "X_reduced = lle.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSK31QGnZbpi",
        "colab_type": "text"
      },
      "source": [
        "## How LLE works internally:\n",
        "\n",
        "first, for each training instance x(i), the algorithm identifies its\n",
        "k closest neighbors (in the preceding code k = 10), then tries to reconstruct x(i) as a linear function of these neighbors. More specifically, it finds the weights wi,j such that\n",
        "the squared distance between x(i) and ∑m w x j is as small as possible, assuming w j=1 i,j i,j\n",
        "= 0 if x(j) is not one of the k closest neighbors of x(i). \n",
        "\n",
        "Thus the first step of LLE is the constrained optimization problem, where W is the weight matrix containing all the weights wi,j. The second constraint simply normalizes the weights for each training instance x(i).\n",
        "\n",
        "After this step, the weight matrix W (containing the weights wi, j) encodes the local linear relationships between the training instances. Now the second step is to map the\n",
        "training instances into a d-dimensional space (where d < n) while preserving these\n",
        "local relationships as much as possible. If z(i) is the image of x(i) in this d-dimensional\n",
        "space, then we want the squared distance between z(i) and ∑m w z j   to be as small j=1 i,j\n",
        "as possible. This idea leads to the unconstrained optimization problem described. It looks very similar to the first step, but instead of keeping the instances fixed and finding the optimal weights, we are doing the reverse: keeping the weights fixed and finding the optimal position of the instances’ images in the low- dimensional space. Note that Z is the matrix containing all z(i).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWdLJC0Vj3pd",
        "colab_type": "text"
      },
      "source": [
        "## Running time of LLE\n",
        "\n",
        "O(m log(m)n log(k)) for finding the k nearest neighbors, O(mnk^3) for optimizing the weights, and O(dm^2) for constructing the low-dimensional representations. **The m^2 in the last term makes this algorithm scale poorly to very large datasets**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjPJWVp8kM6o",
        "colab_type": "text"
      },
      "source": [
        "# Other Dimensionality Reduction Techniques\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLlRQdfgkVg3",
        "colab_type": "text"
      },
      "source": [
        "##Multidimensional Scaling (MDS) \n",
        "reduces dimensionality while trying to preserve the distances between the instances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqB4xoeHkVjr",
        "colab_type": "text"
      },
      "source": [
        "## Isomap \n",
        "creates a graph by connecting each instance to its nearest neighbors, then reduces dimensionality while trying to preserve the geodesic distances between the instances.\n",
        "\n",
        "The geodesic distance between two nodes in a graph is the number of nodes on the shortest path between these nodes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zzeTYt3kVmz",
        "colab_type": "text"
      },
      "source": [
        "## t-Distributed Stochastic Neighbor Embedding (t-SNE) \n",
        "reduces dimensionality while trying to keep similar instances close and dissimilar instances apart. It is mostly used for visualization, in particular to visualize clusters of instances in high-dimensional space (e.g., to visualize the MNIST images in 2D)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vg9BYgOdkVpm",
        "colab_type": "text"
      },
      "source": [
        "## Linear Discriminant Analysis (LDA) \n",
        "It is actually a classification algorithm, but during training it learns the most discriminative axes between the classes, and these axes can then be used to define a hyperplane onto which to project the data. The benefit is that the projection will keep classes as far apart as possible, so LDA is a good technique to reduce dimensionality before running another classification algorithm such as an SVM classifier."
      ]
    }
  ]
}