{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch11_training_deep_neural_networks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPI6MFF5DEDp+M2pZ0J8ESR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Richish/hands_on_ml/blob/master/ch11_training_deep_neural_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZld5fT5Ci-W"
      },
      "source": [
        "# Challenges\n",
        "when traiining a much deeper DNN, perhaps with 10 layers or much more, each containing hundreds of neurons, connected by hundreds of thousands of connections.\n",
        "\n",
        "1. Vanishing gradients problem (or the related exploding gradients problem) that affects deep neural networks and makes lower layers very hard to train. \n",
        "2. You might not have enough training data for such a large network, or it might be too costly to label - Solved by transfer learning.\n",
        "3. Training may be extremely slow - solved by various optimizers.\n",
        "4. A model with millions of parameters would severely risk overfitting the training set, especially if there are not enough training instances, or they are too noisy. - solved by regularization techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMVvrhnCCjKg"
      },
      "source": [
        "# Vanishing/Exploding Gradients Problems\n",
        "\n",
        "During backpropagation: gradients often get smaller and smaller as the algorithm progresses down to the lower layers. As a result, the Gradient Descent update leaves the lower layer connection weights virtually unchanged, and training never converges to a good\n",
        "solution. This is called the vanishing gradients problem. In some cases, the opposite\n",
        "can happen: the gradients can grow bigger and bigger, so many layers get insanely\n",
        "large weight updates and the algorithm diverges. This is the exploding gradients problem,\n",
        "which is mostly encountered in recurrent neural networks. More generally,\n",
        "deep neural networks suffer from unstable gradients; different layers may learn at\n",
        "widely different speeds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8Oy4C1BCjNR"
      },
      "source": [
        "This behavior was one of the reasons why deep neural networks were mostly abandoned for a\n",
        "long time, it is only around 2010 that significant progress was made in understanding\n",
        "it. \n",
        "\n",
        "A paper titled “Understanding the Difficulty of Training Deep Feedforward\n",
        "Neural Networks” by Xavier Glorot and Yoshua Bengio found a few suspects, including\n",
        "the combination of the popular logistic sigmoid activation function and the\n",
        "weight initialization technique that was most popular at the time, namely random initialization\n",
        "using a normal distribution with a mean of 0 and a standard deviation of 1.\n",
        "In short, they showed that with this activation function and this initialization scheme,\n",
        "the variance of the outputs of each layer is much greater than the variance of its\n",
        "inputs. Going forward in the network, the variance keeps increasing after each layer\n",
        "until the activation function saturates at the top layers. This is actually made worse by\n",
        "the fact that the logistic function has a mean of 0.5, not 0 (the hyperbolic tangent\n",
        "function has a mean of 0 and behaves slightly better than the logistic function in deep\n",
        "networks).\n",
        "\n",
        "When\n",
        "inputs become large (negative or positive), the function saturates at 0 or 1, with a\n",
        "derivative extremely close to 0. Thus when backpropagation kicks in, it has virtually\n",
        "no gradient to propagate back through the network, and what little gradient exists\n",
        "keeps getting diluted as backpropagation progresses down through the top layers, so\n",
        "there is really nothing left for the lower layers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVacXQYvCjSd"
      },
      "source": [
        "## Initializers- Glorot, LeCunn and He Initializations\n",
        "\n",
        "### Glorot\n",
        "In their paper, Glorot and Bengio propose a way to significantly alleviate this problem.\n",
        "We need the signal to flow properly in both directions: in the forward direction\n",
        "when making predictions, and in the reverse direction when backpropagating gradients.\n",
        "We don’t want the signal to die out, nor do we want it to explode and saturate.\n",
        "For the signal to flow properly, the authors argue that we need the variance of the\n",
        "outputs of each layer to be equal to the variance of its inputs,2 and we also need the\n",
        "gradients to have equal variance before and after flowing through a layer in the\n",
        "reverse direction. \n",
        "\n",
        "It is actually not possible to guarantee both unless the layer has an equal\n",
        "number of inputs and neurons (these numbers are called the fan-in and fan-out of the\n",
        "layer), but they proposed a good compromise that has proven to work very well in\n",
        "practice: the connection weights of each layer must be initialized randomly as described in Equation below, where fan{avg} = (fan{in} + fan{out})/2. This initialization strategy is called Xavier initialization (after the author’s first name) or Glorot initialization (after his last name).\n",
        "\n",
        "Normal distribution with mean 0 and variance: σ^2 = 1/fan{avg}\n",
        "\n",
        "Or a uniform distribution between −r and + r, with r = root(3/fan{avg})\n",
        "\n",
        "### LeCunn\n",
        "If you just replace fan{avg} with fan{in} in above eqn, you get an initialization strategy\n",
        "that was actually already proposed by Yann LeCun in the 1990s, called LeCun initialization. It is equivalent to Glorot initialization when fan{in} = fan{out}. It took over a decade for researchers to realize\n",
        "just how important this trick really is. Using Glorot initialization can speed up training considerably, and it is one of the tricks that led to the current success of Deep Learning.\n",
        "\n",
        "### He\n",
        "Some papers have provided similar strategies for different activation functions.\n",
        "These strategies differ only by the scale of the variance and whether they use fan{avg} or fan{in}, as shown in Table below - for the uniform distribution, just compute r = root(3.σ^2). \n",
        "\n",
        "The initialization strategy for the ReLU activation function (and its variants, including the ELU activation described shortly) is sometimes called He initialization (after the last name of its author). \n",
        "\n",
        "The SELU activation function will be explained . It should be used with LeCun initialization (preferably with a normal distribution, as we will see).\n",
        "\n",
        "### Table of initializers:\n",
        "| Initialization      | Activation functions | σ^2 (Normal)    |\n",
        "| :---        |    :----   |          :--- |\n",
        "| Glorot      | None, Logistic, tanh, Softmax      | 1/fan{avg}  |\n",
        "| LeCunn   | SELU        | 1/fan{in}      |\n",
        "| He   | RELU        | 2/fan{in}      |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQ7d6CgwCjVI"
      },
      "source": [
        "By default, Keras uses Glorot initialization with a uniform distribution. You can\n",
        "change this to He initialization by setting kernel_initializer=\"he_uniform\" or kernel_initializer=\"he_normal\" when creating a layer, like this:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rM8OkWOS_zF",
        "outputId": "37c52392-433c-46ca-dc87-86bf9e609b31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import keras\n",
        "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.layers.core.Dense at 0x7ff6d47a33c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9R0_ZR-TXbN",
        "outputId": "0492833d-52d9-4f3f-d273-45a2381d5e9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# If you want He initialization with a uniform distribution, but based on fanavg rather\n",
        "# than fanin, you can use the VarianceScaling initializer like this:\n",
        "he_avg_init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg', distribution='uniform') # basically a custom initializer\n",
        "keras.layers.Dense(10, activation=\"sigmoid\", kernel_initializer=he_avg_init)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.layers.core.Dense at 0x7ff6d5128358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Y4kaf_BXnUN"
      },
      "source": [
        "## Activation Functions- Nonsaturating Activation Functions\n",
        "\n",
        "### Relu\n",
        "One of the insights in the 2010 paper by Glorot and Bengio was that the vanishing/\n",
        "exploding gradients problems were in part due to a poor choice of activation function.\n",
        "Until then most people had assumed that if Mother Nature had chosen to use\n",
        "roughly sigmoid activation functions in biological neurons, they must be an excellent\n",
        "choice. But it turns out that other activation functions behave much better in deep\n",
        "neural networks, in particular the ReLU activation function, mostly because it does\n",
        "not saturate for positive values (and also because it is quite fast to compute).\n",
        "\n",
        "#### Problem of dying relus:\n",
        "During training in relu, some neurons effectively die, meaning\n",
        "they stop outputting anything other than 0. In some cases, you may find that half of your network’s neurons are dead, especially if you used a large learning rate. A neuron dies when its weights get tweaked in such a way that the weighted sum of its inputs are negative for all instances in the training set. When this happens, it just keeps outputting 0s, and gradient descent does not affect it anymore since the gradient\n",
        "of the ReLU function is 0 when its input is negative.\n",
        "\n",
        "### Leaky relu(solves the problem of dying relu):\n",
        "This function is defined as LeakyReLUα(z) = max(αz, z). The hyperparameter α defines how much the function “leaks”: it is the\n",
        "slope of the function for z < 0, and is typically set to 0.01. This small slope ensures that leaky ReLUs never die; they can go into a long coma, but they have a chance to eventually wake up. \n",
        "\n",
        "A 2015 paper compared several variants of the ReLU activation function and one of its conclusions was that the leaky variants always outperformed the strict ReLU activation function. In fact, setting α = 0.2 (huge leak) seemed to result in better performance than α = 0.01 (small leak). \n",
        "\n",
        "They also evaluated the **randomized leaky ReLU (RReLU)***, where α is picked randomly in a given range during training, and it is fixed to an average value during testing. It also performed fairly well and seemed to act as a regularizer (reducing the risk of overfitting the training set).\n",
        "\n",
        "Finally, they also evaluated the **parametric leaky ReLU (PReLU)**, where α is authorized to be learned during training (instead of being a hyperparameter, it becomes a parameter that can be modified by backpropagation like any other parameter). This was reported to strongly outperform ReLU on large image datasets, but on smaller\n",
        "datasets it runs the risk of overfitting the training set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E12NRlMFCjX5"
      },
      "source": [
        "### ELU\n",
        "A 2015 paper by Djork-Arné Clevert et al.6 proposed a new activation\n",
        "function called the exponential linear unit (ELU) that outperformed all the ReLU variants in their experiments: training time was reduced and the neural network performed better on the test set. \n",
        "ELU activation function:\n",
        "ELU{α} (z) = α (exp(z) − 1) if z < 0 else z\n",
        "\n",
        "It looks like relu for +ve values of z.\n",
        "\n",
        " 3 differences from relu:\n",
        "1. It takes on negative values when z < 0, which allows the unit to have an average output closer to 0. This helps alleviate the vanishing gradients problem, as discussed earlier. The hyperparameter α defines the value that the ELU function approaches when z is a large negative number. It is usually set to 1, but you can tweak it like any other hyperparameter if you want.\n",
        "2. It has a nonzero gradient for z < 0, which avoids the dead neurons problem.\n",
        "3. If α is equal to 1 then the function is smooth everywhere, including\n",
        "around z = 0, which helps speed up Gradient Descent, since it does not bounce as much left and right of z = 0.\n",
        "\n",
        "Drawbacks of ELU:\n",
        "The main drawback of the ELU activation function is that it is slower to compute than the ReLU and its variants (due to the use of the exponential function), but during training this is compensated by the faster convergence rate. However, at test time an ELU network will be slower than a ReLU network.\n",
        "\n",
        "#### SELU (Scaled ELU)\n",
        "In a 2017 paper7 by Günter Klambauer et al., called “Self-Normalizing\n",
        "Neural Networks”, the authors showed that if you build a neural network composed exclusively of a stack of dense layers, and if all hidden layers use the SELU activation function (which is just a scaled version of the ELU activation function, as its name\n",
        "suggests), then the network will self-normalize: the output of each layer will tend to preserve mean 0 and standard deviation 1 during training, which solves the vanishing/ exploding gradients problem. As a result, this activation function often outperforms other activation functions very significantly for such neural nets (especially\n",
        "deep ones). **However, there are a few conditions for self-normalization to happen:**\n",
        "\n",
        "1. The input features must be standardized (mean 0 and standard deviation 1).\n",
        "2. Every hidden layer’s weights must also be initialized using LeCun normal initialization. In Keras, this means setting kernel_initializer=\"lecun_normal\".\n",
        "3. **The network’s architecture must be sequential.** Unfortunately, if you try to use SELU in non-sequential architectures, such as recurrent networks or networks with skip connections (i.e., connections that skip layers, such as in wide & deep nets), self-normalization will not be guaranteed, so SELU will not necessarily outperform other activation functions.\n",
        "4. The paper only guarantees self-normalization if all layers are dense. However, in practice the SELU activation function seems to work great with convolutional neural nets as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNBkW6G1ouQ_"
      },
      "source": [
        "### Which optimization function to use:\n",
        "\n",
        "For the hidden layers of your deep neural networks- Although your mileage will vary, in\n",
        "general:\n",
        "\n",
        "SELU > ELU > leaky ReLU (and its variants) > ReLU > tanh\n",
        "> logistic. \n",
        "\n",
        "If the network’s architecture prevents it from self-normalizing,\n",
        "then ELU may perform better than SELU (since SELU\n",
        "is not smooth at z = 0). \n",
        "\n",
        "If you care a lot about runtime latency, then you may prefer leaky ReLU. \n",
        "\n",
        "If you don’t want to tweak yet another hyperparameter, you may just use the default α values used by Keras (e.g., 0.3 for the leaky ReLU). \n",
        "If you have spare time and computing power, you can use cross-validation to evaluate other activation functions, in particular RReLU if your network is overfitting, or PReLU if you have a huge training set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xiv6Wn46pk06"
      },
      "source": [
        "# To use the leaky ReLU activation function, you must create a LeakyReLU instance like this:\n",
        "from tensorflow import keras\n",
        "leaky_relu = keras.layers.LeakyReLU(alpha=0.2)\n",
        "layer = keras.layers.Dense(10, activation=leaky_relu, kernel_initializer=\"he_normal\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNS4Yx8Cpk38"
      },
      "source": [
        "# For PReLU, just replace LeakyRelu(alpha=0.2) with PReLU(). There is currently no\n",
        "# official implementation of RReLU in Keras, but you can fairly easily implement your own.\n",
        "p_relu = keras.layers.PReLU()\n",
        "layer = keras.layers.Dense(10, activation=p_relu, kernel_initializer=\"he_normal\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTwy3PFspk68"
      },
      "source": [
        "# For SELU activation, just set activation=\"selu\" and kernel_initializer=\"lecun_normal\" when creating a layer:\n",
        "layer = keras.layers.Dense(10, activation=\"selu\", kernel_initializer=\"lecun_normal\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlsgdFahpf_b"
      },
      "source": [
        "## Batch Normalization:\n",
        "\n",
        "Although using He initialization along with ELU (or any variant of ReLU) can significantly\n",
        "reduce the vanishing/exploding gradients problems at the beginning of training,\n",
        "it doesn’t guarantee that they won’t come back during training.\n",
        "\n",
        "Batch Normalization consists of adding an operation in the model just before or after the\n",
        "activation function of each hidden layer, simply zero-centering and normalizing each\n",
        "input, then scaling and shifting the result using two new parameter vectors per layer:\n",
        "one for scaling, the other for shifting. This operation lets the model\n",
        "learn the optimal scale and mean of each of the layer’s inputs. \n",
        "\n",
        "In many cases, if you\n",
        "add a BN layer as the very first layer of your neural network, you do not need to\n",
        "standardize your training set (e.g., using a StandardScaler): the BN layer will do it\n",
        "for you (well, approximately, since it only looks at one batch at a time, and it can also\n",
        "rescale and shift each input feature).\n",
        "\n",
        "In order to zero-center and normalize the inputs, the algorithm needs to estimate\n",
        "each input’s mean and standard deviation. It does so by evaluating the mean and standard\n",
        "deviation of each input over the current mini-batch (hence the name “Batch\n",
        "Normalization”).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiYIP93Wvms0"
      },
      "source": [
        "So during training, BN just standardizes its inputs then rescales and offsets them.\n",
        "What about at test time?\n",
        "\n",
        "It is often preferred to estimate these final statistics\n",
        "during training using a moving average of the layer’s input means and standard\n",
        "deviations. To sum up, four parameter vectors are learned in each batch-normalized\n",
        "layer: γ (the ouput scale vector) and β (the output offset vector) are learned through\n",
        "regular backpropagation, and μ (the final input mean vector), and σ (the final input\n",
        "standard deviation vector) are estimated using an exponential moving average. Note\n",
        "that μ and σ are estimated during training, but they are not used at all during training,\n",
        "only after training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C332W2YWvmyn"
      },
      "source": [
        "Batch Normalization\n",
        "also acts like a regularizer, reducing the need for other regularization techniques\n",
        "(such as dropout, described later in this chapter).\n",
        "Batch Normalization does, however, add some complexity to the model (although it\n",
        "can remove the need for normalizing the input data, as we discussed earlier). Moreover,\n",
        "there is a runtime penalty: the neural network makes slower predictions due to\n",
        "the extra computations required at each layer. So if you need predictions to be\n",
        "lightning-fast, you may want to check how well plain ELU + He initialization perform\n",
        "before playing with Batch Normalization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6HmG3k5vm9F"
      },
      "source": [
        "You may find that training is rather slow, because each epoch takes\n",
        "much more time when you use batch normalization. However, this\n",
        "is usually counterbalanced by the fact that convergence is much\n",
        "faster with BN, so it will take fewer epochs to reach the same performance.\n",
        "All in all, wall time will usually be smaller (this is the\n",
        "time measured by the clock on your wall)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIyh93WXvnC5"
      },
      "source": [
        "### Implementing Batch Normalization with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hca-FrZcvnI5"
      },
      "source": [
        "Just add a BatchNormalization layer before or after each hidden layer’s activation\n",
        "function, and optionally add a BN layer as well as the first layer in your model. For\n",
        "example, this model applies BN after every hidden layer and as the first layer in the\n",
        "model (after flattening the input images):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zk5rMNcawjAm"
      },
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxFY-LR-wy5G",
        "outputId": "eeb8e426-4697-40fe-91b3-13a8a75702e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 784)               3136      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 300)               235500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 300)               1200      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100)               30100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 271,346\n",
            "Trainable params: 268,978\n",
            "Non-trainable params: 2,368\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpKPcjwKwyIn",
        "outputId": "5c5035d8-aa1b-4e4d-86c3-035bf43a09c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "[(var.name, var.trainable) for var in model.layers[1].variables]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('batch_normalization/gamma:0', True),\n",
              " ('batch_normalization/beta:0', True),\n",
              " ('batch_normalization/moving_mean:0', False),\n",
              " ('batch_normalization/moving_variance:0', False)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jOOXoFvnFv"
      },
      "source": [
        "The authors of the BN paper argued in favor of adding the BN layers before the activation\n",
        "functions, rather than after (as we just did). There is some debate about this, as\n",
        "it seems to depend on the task. So that’s one more thing you can experiment with to\n",
        "see which option works best on your dataset. To add the BN layers before the activation\n",
        "functions, we must remove the activation function from the hidden layers, and\n",
        "add them as separate layers after the BN layers. Moreover, since a Batch Normalization\n",
        "layer includes one offset parameter per input, you can remove the bias term from\n",
        "the previous layer (just pass use_bias=False when creating it):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDixSTNwx6Yv",
        "outputId": "c8a6053a-2283-43a4-966f-a5973d534896",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(300, kernel_initializer=\"he_normal\", use_bias=False),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Activation(\"elu\"),\n",
        "    keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
        "    keras.layers.Activation(\"elu\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model.summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_2 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 784)               3136      \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 300)               235200    \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 300)               1200      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 100)               30000     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 270,946\n",
            "Trainable params: 268,578\n",
            "Non-trainable params: 2,368\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJs51wqzvnAK"
      },
      "source": [
        "#### Hyperparameters of Batch Normalization layer:\n",
        "\n",
        "1. Momentum:  This hyperparameter is used when updating the exponential moving averages: given a\n",
        "new value v.\n",
        "Running average, V{avg} is calculated using eqn:\n",
        "V{avg} = V{avg}*momentum + v{new_batch}*(1-momentum)\n",
        "\n",
        "A good momentum value is typically close to 1—for example, 0.9, 0.99, or 0.999 (you\n",
        "want more 9s for larger datasets and smaller mini-batches).\n",
        "\n",
        "2. Axis: it determines which axis should be normalized.\n",
        "It defaults to –1, meaning that by default it will normalize the last axis (using\n",
        "the means and standard deviations computed across the other axes). For example,\n",
        "when the input batch is 2D (i.e., the batch shape is [batch size, features]), this means\n",
        "that each input feature will be normalized based on the mean and standard deviation\n",
        "computed across all the instances in the batch. For example, the first BN layer in the\n",
        "previous code example will independently normalize (and rescale and shift) each of\n",
        "the 784 input features. However, if we move the first BN layer before the Flatten\n",
        "layer, then the input batches will be 3D, with shape [batch size, height, width], therefore\n",
        "the BN layer will compute 28 means and 28 standard deviations.\n",
        "\n",
        "\n",
        "it will normalize all pixels in a given column using the same mean and standard deviation.\n",
        "There will also be just 28 scale parameters and 28 shift parameters. If instead\n",
        "you still want to treat each of the 784 pixels independently, then you should set\n",
        "axis=[1, 2]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1i7v4jxvm5y"
      },
      "source": [
        "Notice that the BN layer does not perform the same computation during training and\n",
        "after training: it uses batch statistics during training, and the “final” statistics after\n",
        "training (i.e., the final value of the moving averages)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVGd1wwVvmvm"
      },
      "source": [
        "Batch Normalization has become one of the most used layers in deep neural networks,\n",
        "to the point that it is often omitted in the diagrams, as it is assumed that BN is\n",
        "added after every layer. \n",
        "\n",
        "However, a very recent paper10 by Hongyi Zhang et al. may\n",
        "well change this: the authors show that by using a novel fixed-update (fixup) weight\n",
        "initialization technique, they manage to train a very deep neural network (10,000 layers!)\n",
        "without BN, achieving state-of-the-art performance on complex image classification\n",
        "tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2_2csjD3E8S"
      },
      "source": [
        "## Gradient Clipping\n",
        "\n",
        "Another popular technique to lessen the exploding gradients problem is to simply\n",
        "clip the gradients during backpropagation so that they never exceed some threshold.\n",
        "This is called Gradient Clipping.\n",
        "\n",
        "This technique is most often used in recurrent neural networks, as Batch Normalization is tricky to use in RNNs.\n",
        "For other types of networks, BN is usually sufficient.\n",
        "\n",
        "In Keras, implementing Gradient Clipping is just a matter of setting the clipvalue or\n",
        "clipnorm argument when creating an optimizer. For example:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysviAWPi4I93",
        "outputId": "846ed05a-0a10-4575-8ab8-962abdd2bf03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "optimizer = keras.optimizers.SGD(clipvalue=1.0)\n",
        "model.compile(loss=\"mse\", optimizer=optimizer)\n",
        "model.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_2 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 784)               3136      \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 300)               235200    \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 300)               1200      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 100)               30000     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 270,946\n",
            "Trainable params: 268,578\n",
            "Non-trainable params: 2,368\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RCd2pDe3FDT"
      },
      "source": [
        "This will clip every component of the gradient vector to a value between –1.0 and 1.0.\n",
        "This means that all the partial derivatives of the loss (with regards to each and every\n",
        "trainable parameter) will be clipped between –1.0 and 1.0. The threshold is a hyperparameter\n",
        "you can tune. Note that it may change the orientation of the gradient vector:\n",
        "for example, if the original gradient vector is [0.9, 100.0], it points mostly in the\n",
        "direction of the second axis, but once you clip it by value, you get [0.9, 1.0], which\n",
        "points roughly in the diagonal between the two axes. In practice however, this\n",
        "approach works well. \n",
        "\n",
        "If you want to ensure that Gradient Clipping does not change\n",
        "the direction of the gradient vector, you should clip by norm by setting clipnorm\n",
        "instead of clipvalue. This will clip the whole gradient if its ℓ2 norm is greater than\n",
        "the threshold you picked. For example, if you set clipnorm=1.0, then the vector [0.9,\n",
        "100.0] will be clipped to [0.00899964, 0.9999595], preserving its orientation, but\n",
        "almost eliminating the first component. If you observe that the gradients explode\n",
        "during training (you can track the size of the gradients using TensorBoard), you may\n",
        "want to try both clipping by value and clipping by norm, with different threshold,\n",
        "and see which option performs best on the validation set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYvxV4273FMw"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyBd9My23FS7"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZ5dwmQ53FZJ"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tsam77x3Ffi"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsmhlswI3Fi8"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHwtkLJn3FcY"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puOmWafZ3FV7"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewsGHbCK3FP6"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4he4GH33FJ2"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ucz0ftV3E_0"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flcNh7rd3EyS"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_JVrTocvljq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XF-clykvvlmi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbXo3XYBvlug"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDPgOqLCvlxU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW4jP1qkvlzq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ked0JFQpvl2H"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJEuHkMevl4i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpVNpW32vl69"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ha3z5KPvl9f"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyTyR1n5vl_x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBzl3-tlvmC3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}