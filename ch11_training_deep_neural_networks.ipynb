{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch11_training_deep_neural_networks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNw7IPX4KWlrsTS8Lpt/Ws+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Richish/hands_on_ml/blob/master/ch11_training_deep_neural_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZld5fT5Ci-W"
      },
      "source": [
        "# Challenges\n",
        "when traiining a much deeper DNN, perhaps with 10 layers or much more, each containing hundreds of neurons, connected by hundreds of thousands of connections.\n",
        "\n",
        "1. Vanishing gradients problem (or the related exploding gradients problem) that affects deep neural networks and makes lower layers very hard to train. \n",
        "2. You might not have enough training data for such a large network, or it might be too costly to label - Solved by transfer learning.\n",
        "3. Training may be extremely slow - solved by various optimizers.\n",
        "4. A model with millions of parameters would severely risk overfitting the training set, especially if there are not enough training instances, or they are too noisy. - solved by regularization techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOg3xtpLCjHu"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMVvrhnCCjKg"
      },
      "source": [
        "# Vanishing/Exploding Gradients Problems\n",
        "\n",
        "During backpropagation: gradients often get smaller and smaller as the algorithm progresses down to the lower layers. As a result, the Gradient Descent update leaves the lower layer connection weights virtually unchanged, and training never converges to a good\n",
        "solution. This is called the vanishing gradients problem. In some cases, the opposite\n",
        "can happen: the gradients can grow bigger and bigger, so many layers get insanely\n",
        "large weight updates and the algorithm diverges. This is the exploding gradients problem,\n",
        "which is mostly encountered in recurrent neural networks. More generally,\n",
        "deep neural networks suffer from unstable gradients; different layers may learn at\n",
        "widely different speeds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8Oy4C1BCjNR"
      },
      "source": [
        "This behavior was one of the reasons why deep neural networks were mostly abandoned for a\n",
        "long time, it is only around 2010 that significant progress was made in understanding\n",
        "it. \n",
        "\n",
        "A paper titled “Understanding the Difficulty of Training Deep Feedforward\n",
        "Neural Networks” by Xavier Glorot and Yoshua Bengio found a few suspects, including\n",
        "the combination of the popular logistic sigmoid activation function and the\n",
        "weight initialization technique that was most popular at the time, namely random initialization\n",
        "using a normal distribution with a mean of 0 and a standard deviation of 1.\n",
        "In short, they showed that with this activation function and this initialization scheme,\n",
        "the variance of the outputs of each layer is much greater than the variance of its\n",
        "inputs. Going forward in the network, the variance keeps increasing after each layer\n",
        "until the activation function saturates at the top layers. This is actually made worse by\n",
        "the fact that the logistic function has a mean of 0.5, not 0 (the hyperbolic tangent\n",
        "function has a mean of 0 and behaves slightly better than the logistic function in deep\n",
        "networks).\n",
        "\n",
        "When\n",
        "inputs become large (negative or positive), the function saturates at 0 or 1, with a\n",
        "derivative extremely close to 0. Thus when backpropagation kicks in, it has virtually\n",
        "no gradient to propagate back through the network, and what little gradient exists\n",
        "keeps getting diluted as backpropagation progresses down through the top layers, so\n",
        "there is really nothing left for the lower layers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVacXQYvCjSd"
      },
      "source": [
        "## Glorot, LeCunn and He Initialization\n",
        "\n",
        "### Glorot\n",
        "In their paper, Glorot and Bengio propose a way to significantly alleviate this problem.\n",
        "We need the signal to flow properly in both directions: in the forward direction\n",
        "when making predictions, and in the reverse direction when backpropagating gradients.\n",
        "We don’t want the signal to die out, nor do we want it to explode and saturate.\n",
        "For the signal to flow properly, the authors argue that we need the variance of the\n",
        "outputs of each layer to be equal to the variance of its inputs,2 and we also need the\n",
        "gradients to have equal variance before and after flowing through a layer in the\n",
        "reverse direction. \n",
        "\n",
        "It is actually not possible to guarantee both unless the layer has an equal\n",
        "number of inputs and neurons (these numbers are called the fan-in and fan-out of the\n",
        "layer), but they proposed a good compromise that has proven to work very well in\n",
        "practice: the connection weights of each layer must be initialized randomly as described in Equation below, where fan{avg} = (fan{in} + fan{out})/2. This initialization strategy is called Xavier initialization (after the author’s first name) or Glorot initialization (after his last name).\n",
        "\n",
        "Normal distribution with mean 0 and variance: σ^2 = 1/fan{avg}\n",
        "\n",
        "Or a uniform distribution between −r and + r, with r = root(3/fan{avg})\n",
        "\n",
        "### LeCunn\n",
        "If you just replace fan{avg} with fan{in} in above eqn, you get an initialization strategy\n",
        "that was actually already proposed by Yann LeCun in the 1990s, called LeCun initialization. It is equivalent to Glorot initialization when fan{in} = fan{out}. It took over a decade for researchers to realize\n",
        "just how important this trick really is. Using Glorot initialization can speed up training considerably, and it is one of the tricks that led to the current success of Deep Learning.\n",
        "\n",
        "### He\n",
        "Some papers have provided similar strategies for different activation functions.\n",
        "These strategies differ only by the scale of the variance and whether they use fan{avg} or fan{in}, as shown in Table below - for the uniform distribution, just compute r = root(3.σ^2). \n",
        "\n",
        "The initialization strategy for the ReLU activation function (and its variants, including the ELU activation described shortly) is sometimes called He initialization (after the last name of its author). \n",
        "\n",
        "The SELU activation function will be explained . It should be used with LeCun initialization (preferably with a normal distribution, as we will see).\n",
        "\n",
        "### Table of initializers:\n",
        "| Initialization      | Activation functions | σ^2 (Normal)    |\n",
        "| :---        |    :----   |          :--- |\n",
        "| Glorot      | None, Logistic, tanh, Softmax      | 1/fan{avg}  |\n",
        "| LeCunn   | SELU        | 1/fan{in}      |\n",
        "| He   | RELU        | 2/fan{in}      |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQ7d6CgwCjVI"
      },
      "source": [
        "By default, Keras uses Glorot initialization with a uniform distribution. You can\n",
        "change this to He initialization by setting kernel_initializer=\"he_uniform\" or kernel_initializer=\"he_normal\" when creating a layer, like this:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rM8OkWOS_zF",
        "outputId": "37c52392-433c-46ca-dc87-86bf9e609b31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import keras\n",
        "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.layers.core.Dense at 0x7ff6d47a33c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9R0_ZR-TXbN",
        "outputId": "0492833d-52d9-4f3f-d273-45a2381d5e9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# If you want He initialization with a uniform distribution, but based on fanavg rather\n",
        "# than fanin, you can use the VarianceScaling initializer like this:\n",
        "he_avg_init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg', distribution='uniform') # basically a custom initializer\n",
        "keras.layers.Dense(10, activation=\"sigmoid\", kernel_initializer=he_avg_init)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.layers.core.Dense at 0x7ff6d5128358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Y4kaf_BXnUN"
      },
      "source": [
        "## Nonsaturating Activation Functions\n",
        "\n",
        "### Relu\n",
        "One of the insights in the 2010 paper by Glorot and Bengio was that the vanishing/\n",
        "exploding gradients problems were in part due to a poor choice of activation function.\n",
        "Until then most people had assumed that if Mother Nature had chosen to use\n",
        "roughly sigmoid activation functions in biological neurons, they must be an excellent\n",
        "choice. But it turns out that other activation functions behave much better in deep\n",
        "neural networks, in particular the ReLU activation function, mostly because it does\n",
        "not saturate for positive values (and also because it is quite fast to compute).\n",
        "\n",
        "#### Problem of dying relus:\n",
        "During training in relu, some neurons effectively die, meaning\n",
        "they stop outputting anything other than 0. In some cases, you may find that half of your network’s neurons are dead, especially if you used a large learning rate. A neuron dies when its weights get tweaked in such a way that the weighted sum of its inputs are negative for all instances in the training set. When this happens, it just keeps outputting 0s, and gradient descent does not affect it anymore since the gradient\n",
        "of the ReLU function is 0 when its input is negative.\n",
        "\n",
        "### Leaky relu(solves the problem of dying relu):\n",
        "This function is defined as LeakyReLUα(z) = max(αz, z). The hyperparameter α defines how much the function “leaks”: it is the\n",
        "slope of the function for z < 0, and is typically set to 0.01. This small slope ensures that leaky ReLUs never die; they can go into a long coma, but they have a chance to eventually wake up. \n",
        "\n",
        "A 2015 paper compared several variants of the ReLU activation function and one of its conclusions was that the leaky variants always outperformed the strict ReLU activation function. In fact, setting α = 0.2 (huge leak) seemed to result in better performance than α = 0.01 (small leak). \n",
        "\n",
        "They also evaluated the **randomized leaky ReLU (RReLU)***, where α is picked randomly in a given range during training, and it is fixed to an average value during testing. It also performed fairly well and seemed to act as a regularizer (reducing the risk of overfitting the training set).\n",
        "\n",
        "Finally, they also evaluated the **parametric leaky ReLU (PReLU)**, where α is authorized to be learned during training (instead of being a hyperparameter, it becomes a parameter that can be modified by backpropagation like any other parameter). This was reported to strongly outperform ReLU on large image datasets, but on smaller\n",
        "datasets it runs the risk of overfitting the training set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E12NRlMFCjX5"
      },
      "source": [
        "### ELU\n",
        "A 2015 paper by Djork-Arné Clevert et al.6 proposed a new activation\n",
        "function called the exponential linear unit (ELU) that outperformed all the ReLU variants in their experiments: training time was reduced and the neural network performed better on the test set. \n",
        "ELU activation function:\n",
        "ELU{α} (z) = α (exp(z) − 1) if z < 0 else z\n",
        "\n",
        "It looks like relu for +ve values of z.\n",
        "\n",
        " 3 differences from relu:\n",
        "1. It takes on negative values when z < 0, which allows the unit to have an average output closer to 0. This helps alleviate the vanishing gradients problem, as discussed earlier. The hyperparameter α defines the value that the ELU function approaches when z is a large negative number. It is usually set to 1, but you can tweak it like any other hyperparameter if you want.\n",
        "2. It has a nonzero gradient for z < 0, which avoids the dead neurons problem.\n",
        "3. If α is equal to 1 then the function is smooth everywhere, including\n",
        "around z = 0, which helps speed up Gradient Descent, since it does not bounce as much left and right of z = 0.\n",
        "\n",
        "Drawbacks of ELU:\n",
        "The main drawback of the ELU activation function is that it is slower to compute than the ReLU and its variants (due to the use of the exponential function), but during training this is compensated by the faster convergence rate. However, at test time an ELU network will be slower than a ReLU network.\n",
        "\n",
        "#### SELU (Scaled ELU)\n",
        "In a 2017 paper7 by Günter Klambauer et al., called “Self-Normalizing\n",
        "Neural Networks”, the authors showed that if you build a neural network composed exclusively of a stack of dense layers, and if all hidden layers use the SELU activation function (which is just a scaled version of the ELU activation function, as its name\n",
        "suggests), then the network will self-normalize: the output of each layer will tend to preserve mean 0 and standard deviation 1 during training, which solves the vanishing/ exploding gradients problem. As a result, this activation function often outperforms other activation functions very significantly for such neural nets (especially\n",
        "deep ones). **However, there are a few conditions for self-normalization to happen:**\n",
        "\n",
        "1. The input features must be standardized (mean 0 and standard deviation 1).\n",
        "2. Every hidden layer’s weights must also be initialized using LeCun normal initialization. In Keras, this means setting kernel_initializer=\"lecun_normal\".\n",
        "3. **The network’s architecture must be sequential.** Unfortunately, if you try to use SELU in non-sequential architectures, such as recurrent networks or networks with skip connections (i.e., connections that skip layers, such as in wide & deep nets), self-normalization will not be guaranteed, so SELU will not necessarily outperform other activation functions.\n",
        "4. The paper only guarantees self-normalization if all layers are dense. However, in practice the SELU activation function seems to work great with convolutional neural nets as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNBkW6G1ouQ_"
      },
      "source": [
        "### Which optimization function to use:\n",
        "\n",
        "For the hidden layers of your deep neural networks- Although your mileage will vary, in\n",
        "general:\n",
        "\n",
        "SELU > ELU > leaky ReLU (and its variants) > ReLU > tanh\n",
        "> logistic. \n",
        "\n",
        "If the network’s architecture prevents it from self-normalizing,\n",
        "then ELU may perform better than SELU (since SELU\n",
        "is not smooth at z = 0). \n",
        "\n",
        "If you care a lot about runtime latency, then you may prefer leaky ReLU. \n",
        "\n",
        "If you don’t want to tweak yet another hyperparameter, you may just use the default α values used by Keras (e.g., 0.3 for the leaky ReLU). \n",
        "If you have spare time and computing power, you can use cross-validation to evaluate other activation functions, in particular RReLU if your network is overfitting, or PReLU if you have a huge training set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xiv6Wn46pk06"
      },
      "source": [
        "# To use the leaky ReLU activation function, you must create a LeakyReLU instance like this:\n",
        "from tensorflow import keras\n",
        "leaky_relu = keras.layers.LeakyReLU(alpha=0.2)\n",
        "layer = keras.layers.Dense(10, activation=leaky_relu, kernel_initializer=\"he_normal\")\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNS4Yx8Cpk38"
      },
      "source": [
        "# For PReLU, just replace LeakyRelu(alpha=0.2) with PReLU(). There is currently no\n",
        "# official implementation of RReLU in Keras, but you can fairly easily implement your own.\n",
        "p_relu = keras.layers.PReLU()\n",
        "layer = keras.layers.Dense(10, activation=p_relu, kernel_initializer=\"he_normal\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTwy3PFspk68"
      },
      "source": [
        "# For SELU activation, just set activation=\"selu\" and kernel_initializer=\"lecun_normal\" when creating a layer:\n",
        "layer = keras.layers.Dense(10, activation=\"selu\", kernel_initializer=\"lecun_normal\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlsgdFahpf_b"
      },
      "source": [
        "## Batch Normalization:\n"
      ]
    }
  ]
}